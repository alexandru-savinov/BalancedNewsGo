
<!DOCTYPE html>
<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
		<title>llm: Go Coverage Report</title>
		<style>
			body {
				background: black;
				color: rgb(80, 80, 80);
			}
			body, pre, #legend span {
				font-family: Menlo, monospace;
				font-weight: bold;
			}
			#topbar {
				background: black;
				position: fixed;
				top: 0; left: 0; right: 0;
				height: 42px;
				border-bottom: 1px solid rgb(80, 80, 80);
			}
			#content {
				margin-top: 50px;
			}
			#nav, #legend {
				float: left;
				margin-left: 10px;
			}
			#legend {
				margin-top: 12px;
			}
			#nav {
				margin-top: 10px;
			}
			#legend span {
				margin: 0 5px;
			}
			.cov0 { color: rgb(192, 0, 0) }
.cov1 { color: rgb(128, 128, 128) }
.cov2 { color: rgb(116, 140, 131) }
.cov3 { color: rgb(104, 152, 134) }
.cov4 { color: rgb(92, 164, 137) }
.cov5 { color: rgb(80, 176, 140) }
.cov6 { color: rgb(68, 188, 143) }
.cov7 { color: rgb(56, 200, 146) }
.cov8 { color: rgb(44, 212, 149) }
.cov9 { color: rgb(32, 224, 152) }
.cov10 { color: rgb(20, 236, 155) }

		</style>
	</head>
	<body>
		<div id="topbar">
			<div id="nav">
				<select id="files">
				
				<option value="file0">github.com/alexandru-savinov/BalancedNewsGo/internal/llm/cache.go (86.7%)</option>
				
				<option value="file1">github.com/alexandru-savinov/BalancedNewsGo/internal/llm/composite_score_fix.go (81.2%)</option>
				
				<option value="file2">github.com/alexandru-savinov/BalancedNewsGo/internal/llm/composite_score_utils.go (82.9%)</option>
				
				<option value="file3">github.com/alexandru-savinov/BalancedNewsGo/internal/llm/ensemble.go (18.6%)</option>
				
				<option value="file4">github.com/alexandru-savinov/BalancedNewsGo/internal/llm/llm.go (43.8%)</option>
				
				<option value="file5">github.com/alexandru-savinov/BalancedNewsGo/internal/llm/progress_manager.go (95.5%)</option>
				
				<option value="file6">github.com/alexandru-savinov/BalancedNewsGo/internal/llm/score_calculator.go (91.1%)</option>
				
				<option value="file7">github.com/alexandru-savinov/BalancedNewsGo/internal/llm/score_manager.go (70.5%)</option>
				
				<option value="file8">github.com/alexandru-savinov/BalancedNewsGo/internal/llm/service_http.go (91.3%)</option>
				
				</select>
			</div>
			<div id="legend">
				<span>not tracked</span>
			
				<span class="cov0">not covered</span>
				<span class="cov8">covered</span>
			
			</div>
		</div>
		<div id="content">
		
		<pre class="file" id="file0" style="display: none">package llm

import (
        "encoding/json"
        "fmt"
        "sync"

        "github.com/alexandru-savinov/BalancedNewsGo/internal/db"
)

// Cache provides a thread-safe in-memory cache
type Cache struct {
        m sync.Map
}

// NewCache creates a new empty cache instance
func NewCache() *Cache <span class="cov8" title="1">{
        return &amp;Cache{}
}</span>

// makeKey creates a composite key from content hash and model
func makeKey(contentHash, model string) string <span class="cov8" title="1">{
        return fmt.Sprintf("%s:%s", contentHash, model)
}</span>

// Get retrieves a value from the cache
func (c *Cache) Get(contentHash, model string) (*db.LLMScore, bool) <span class="cov8" title="1">{
        v, ok := c.m.Load(makeKey(contentHash, model))
        if !ok </span><span class="cov8" title="1">{
                return nil, false
        }</span>

        // Convert stored JSON string back to LLMScore
        <span class="cov8" title="1">var score db.LLMScore
        if err := json.Unmarshal([]byte(v.(string)), &amp;score); err != nil </span><span class="cov0" title="0">{
                return nil, false
        }</span>
        <span class="cov8" title="1">return &amp;score, true</span>
}

// Set stores a value in the cache
func (c *Cache) Set(contentHash, model string, score *db.LLMScore) <span class="cov8" title="1">{
        // Convert LLMScore to JSON string for storage
        data, err := json.Marshal(score)
        if err != nil </span><span class="cov0" title="0">{
                return
        }</span>
        <span class="cov8" title="1">c.m.Store(makeKey(contentHash, model), string(data))</span>
}

// Delete removes a value from the cache
func (c *Cache) Delete(key string) <span class="cov8" title="1">{
        c.m.Delete(key)
}</span>

// Remove removes a value from the cache by content hash and model
func (c *Cache) Remove(contentHash, model string) <span class="cov8" title="1">{
        c.m.Delete(makeKey(contentHash, model))
}</span>
</pre>
		
		<pre class="file" id="file1" style="display: none">package llm

import (
        "encoding/json"
        "fmt"
        "log"
        "strings"

        "github.com/alexandru-savinov/BalancedNewsGo/internal/db"
)

// MapModelToPerspective maps a model name to its perspective (left, center, right)
// based on the provided composite score configuration
func MapModelToPerspective(modelName string, cfg *CompositeScoreConfig) string <span class="cov8" title="1">{
        if cfg == nil </span><span class="cov8" title="1">{
                log.Printf("Error: CompositeScoreConfig is nil in MapModelToPerspective")
                return ""
        }</span>

        <span class="cov8" title="1">normalizedModelName := strings.ToLower(strings.TrimSpace(modelName))

        // Look up the model in the configuration
        for _, model := range cfg.Models </span><span class="cov8" title="1">{
                if strings.ToLower(strings.TrimSpace(model.ModelName)) == normalizedModelName </span><span class="cov8" title="1">{
                        return strings.ToLower(model.Perspective)
                }</span>
        }

        // Fallback to legacy names
        <span class="cov8" title="1">if normalizedModelName == "left" </span><span class="cov8" title="1">{
                return "left"
        }</span> else<span class="cov8" title="1"> if normalizedModelName == "center" </span><span class="cov8" title="1">{
                return "center"
        }</span> else<span class="cov8" title="1"> if normalizedModelName == "right" </span><span class="cov8" title="1">{
                return "right"
        }</span>

        <span class="cov8" title="1">log.Printf("Warning: Model '%s' not found in composite score configuration", modelName)
        return ""</span>
}

// checkForAllZeroResponses detects if all LLM responses have zero scores and zero confidence
func checkForAllZeroResponses(scores []db.LLMScore) (bool, error) <span class="cov8" title="1">{
        if len(scores) == 0 </span><span class="cov0" title="0">{
                return false, fmt.Errorf("no LLM scores provided")
        }</span>

        <span class="cov8" title="1">allZeros := true
        for _, score := range scores </span><span class="cov8" title="1">{
                // Check if we have a non-zero score
                if score.Score != 0.0 </span><span class="cov8" title="1">{
                        allZeros = false
                        break</span>
                }

                // Extract confidence from metadata
                <span class="cov8" title="1">var metadata map[string]interface{}
                if err := json.Unmarshal([]byte(score.Metadata), &amp;metadata); err == nil </span><span class="cov8" title="1">{
                        if confidence, ok := metadata["confidence"].(float64); ok &amp;&amp; confidence &gt; 0.0 </span><span class="cov0" title="0">{
                                allZeros = false
                                break</span>
                        }
                }
        }

        <span class="cov8" title="1">if allZeros </span><span class="cov8" title="1">{
                log.Printf("Critical warning: All %d LLM models returned empty responses or zero values", len(scores))
                return true, fmt.Errorf("all LLMs returned empty or zero-confidence responses (count: %d)", len(scores))
        }</span>

        <span class="cov8" title="1">return false, nil</span>
}

// ComputeCompositeScoreWithConfidenceFixed is an improved version of ComputeCompositeScoreWithConfidence
// that properly maps model names to their perspectives based on the configuration
func ComputeCompositeScoreWithConfidenceFixed(scores []db.LLMScore) (float64, float64, error) <span class="cov8" title="1">{
        // First check if we have all zero responses
        if allZeros, err := checkForAllZeroResponses(scores); allZeros </span><span class="cov8" title="1">{
                return 0, 0, err
        }</span>

        // Use the global test config if available (for tests), otherwise load from file
        <span class="cov8" title="1">var cfg *CompositeScoreConfig
        var err error

        if testModelConfig != nil </span><span class="cov8" title="1">{
                cfg = testModelConfig
        }</span> else<span class="cov8" title="1"> {
                cfg, err = LoadCompositeScoreConfig()
                if err != nil </span><span class="cov0" title="0">{
                        return 0, 0, fmt.Errorf("loading composite score config: %w", err)
                }</span>
        }

        // Map for left/center/right
        <span class="cov8" title="1">scoreMap := map[string]float64{
                "left":   cfg.DefaultMissing,
                "center": cfg.DefaultMissing,
                "right":  cfg.DefaultMissing,
        }

        validCount := 0
        sum := 0.0
        weightedSum := 0.0
        weightTotal := 0.0
        validModels := make(map[string]bool)

        // Log the scores we're processing
        log.Printf("ComputeCompositeScoreWithConfidenceFixed: Processing %d scores", len(scores))
        for i, s := range scores </span><span class="cov8" title="1">{
                log.Printf("Score[%d]: Model=%s, Score=%.2f", i, s.Model, s.Score)
        }</span>

        // Process scores by perspective
        <span class="cov8" title="1">perspectiveModels := mapModelsToPerspectives(scores, cfg)
        processScoresByPerspective(perspectiveModels, cfg, scoreMap, &amp;validCount, &amp;validModels)

        if validCount == 0 </span><span class="cov8" title="1">{
                return 0, 0, fmt.Errorf("no valid model scores to compute composite score (input count: %d)", len(scores))
        }</span>

        // Calculate weighted sums
        <span class="cov8" title="1">for k, v := range scoreMap </span><span class="cov8" title="1">{
                score := v
                w := 1.0
                if cfg.Formula == "weighted" </span><span class="cov0" title="0">{
                        if weight, ok := cfg.Weights[k]; ok </span><span class="cov0" title="0">{
                                w = weight
                        }</span>
                }
                <span class="cov8" title="1">weightedSum += score * w
                weightTotal += w
                sum += score</span>
        }

        // Calculate composite score
        <span class="cov8" title="1">composite := calculateCompositeScore(cfg, scoreMap, sum, weightedSum, weightTotal)

        // Calculate confidence using the proper calculation function
        confidence := calculateConfidence(cfg, &amp;validModels, scoreMap)

        log.Printf("ComputeCompositeScoreWithConfidenceFixed: Final composite=%.2f, confidence=%.2f", composite, confidence)
        return composite, confidence, nil</span>
}

// processScoresByPerspective handles selecting the best score for each perspective
func processScoresByPerspective(
        perspectiveModels map[string][]db.LLMScore,
        cfg *CompositeScoreConfig,
        scoreMap map[string]float64,
        validCount *int,
        validModels *map[string]bool) <span class="cov8" title="1">{

        // Use the best score from each perspective
        for perspective, models := range perspectiveModels </span><span class="cov8" title="1">{
                if len(models) == 0 </span><span class="cov0" title="0">{
                        log.Printf("No models found for perspective %s", perspective)
                        continue</span>
                }

                <span class="cov8" title="1">log.Printf("Candidates for %s: ", perspective)
                for _, m := range models </span><span class="cov8" title="1">{
                        log.Printf("  Model: %s, Score: %.2f, Metadata: %s", m.Model, m.Score, m.Metadata)
                }</span>

                // Find the model with highest confidence
                <span class="cov8" title="1">bestScore := findBestConfidenceScore(models)
                log.Printf("Selected for %s: Model: %s, Score: %.2f", perspective, bestScore.Model, bestScore.Score)

                // Use the best score for this perspective
                val := bestScore.Score

                // Only consider a score invalid if it's truly invalid (NaN, +/-Inf)
                // or outside the configured range when specified
                hasScoreRange := cfg.MinScore &gt; -1e9 || cfg.MaxScore &lt; 1e9
                isOutsideRange := hasScoreRange &amp;&amp; (val &lt; cfg.MinScore || val &gt; cfg.MaxScore)

                if cfg.HandleInvalid == "ignore" &amp;&amp; (isInvalid(val) || isOutsideRange) </span><span class="cov0" title="0">{
                        log.Printf("Ignoring invalid score %.2f for perspective %s", val, perspective)
                        continue</span>
                }

                <span class="cov8" title="1">if isInvalid(val) || isOutsideRange </span><span class="cov0" title="0">{
                        val = cfg.DefaultMissing
                        log.Printf("Using default value %.2f for invalid score from perspective %s", val, perspective)
                }</span> else<span class="cov8" title="1"> {
                        log.Printf("Using actual score %.2f for perspective %s from model %s", val, perspective, bestScore.Model)
                }</span>

                <span class="cov8" title="1">log.Printf("Adding score %.2f for perspective %s from model %s", val, perspective, bestScore.Model)
                scoreMap[perspective] = val
                (*validCount)++
                (*validModels)[perspective] = true</span>
        }
}

// mapModelsToPerspectives groups LLM scores by their corresponding perspectives
func mapModelsToPerspectives(scores []db.LLMScore, cfg *CompositeScoreConfig) map[string][]db.LLMScore <span class="cov8" title="1">{
        perspectiveModels := make(map[string][]db.LLMScore)
        for _, s := range scores </span><span class="cov8" title="1">{
                // Skip ensemble scores
                if strings.ToLower(s.Model) == "ensemble" </span><span class="cov8" title="1">{
                        continue</span>
                }

                // First try to map the model to its perspective
                <span class="cov8" title="1">perspective := MapModelToPerspective(s.Model, cfg)

                // If mapping failed, try the old way (legacy model names)
                if perspective == "" </span><span class="cov8" title="1">{
                        model := strings.ToLower(s.Model)
                        // Direct check for legacy model names - these are the model names themselves
                        if model == "left" </span><span class="cov0" title="0">{
                                perspective = "left"
                        }</span> else<span class="cov8" title="1"> if model == "center" </span><span class="cov0" title="0">{
                                perspective = "center"
                        }</span> else<span class="cov8" title="1"> if model == "right" </span><span class="cov0" title="0">{
                                perspective = "right"
                        }</span> else<span class="cov8" title="1"> if model == LabelLeft </span><span class="cov0" title="0">{
                                perspective = "left"
                        }</span> else<span class="cov8" title="1"> if model == LabelRight </span><span class="cov0" title="0">{
                                perspective = "right"
                        }</span> else<span class="cov8" title="1"> {
                                // Skip unknown models
                                log.Printf("Skipping unknown model: %s", s.Model)
                                continue</span>
                        }
                }

                // Ensure perspective is one of the expected values
                <span class="cov8" title="1">if perspective != "left" &amp;&amp; perspective != "center" &amp;&amp; perspective != "right" </span><span class="cov0" title="0">{
                        log.Printf("Skipping model with invalid perspective: %s -&gt; %s", s.Model, perspective)
                        continue</span>
                }

                // Add to perspective models map
                <span class="cov8" title="1">log.Printf("Mapping model '%s' (score %.2f) to perspective '%s'", s.Model, s.Score, perspective)
                perspectiveModels[perspective] = append(perspectiveModels[perspective], s)</span>
        }

        // Log the perspective mapping results
        <span class="cov8" title="1">for perspective, models := range perspectiveModels </span><span class="cov8" title="1">{
                log.Printf("Perspective %s has %d models", perspective, len(models))
        }</span>

        <span class="cov8" title="1">return perspectiveModels</span>
}

// findBestConfidenceScore selects the score with highest confidence from a group of models
func findBestConfidenceScore(models []db.LLMScore) db.LLMScore <span class="cov8" title="1">{
        if len(models) == 1 </span><span class="cov8" title="1">{
                return models[0]
        }</span>

        <span class="cov8" title="1">bestScore := models[0]
        bestConfidence := extractConfidence(bestScore.Metadata)
        allSameConfidence := true

        for _, model := range models[1:] </span><span class="cov8" title="1">{
                modelConfidence := extractConfidence(model.Metadata)
                if modelConfidence != bestConfidence </span><span class="cov8" title="1">{
                        allSameConfidence = false
                }</span>
                <span class="cov8" title="1">if modelConfidence &gt; bestConfidence </span><span class="cov8" title="1">{
                        bestScore = model
                        bestConfidence = modelConfidence
                }</span> else<span class="cov8" title="1"> if modelConfidence == bestConfidence </span><span class="cov8" title="1">{
                        if model.Score &gt; bestScore.Score </span><span class="cov0" title="0">{
                                bestScore = model
                        }</span>
                }
        }

        // If all confidences are equal, pick the highest score
        <span class="cov8" title="1">if allSameConfidence </span><span class="cov8" title="1">{
                maxScore := bestScore.Score
                for _, model := range models </span><span class="cov8" title="1">{
                        if model.Score &gt; maxScore </span><span class="cov0" title="0">{
                                bestScore = model
                                maxScore = model.Score
                        }</span>
                }
        }

        <span class="cov8" title="1">return bestScore</span>
}

// extractConfidence gets the confidence value from model metadata, defaulting to 0.5
func extractConfidence(metadata string) float64 <span class="cov8" title="1">{
        defaultConfidence := 0.5

        var metaMap map[string]interface{}
        if err := json.Unmarshal([]byte(metadata), &amp;metaMap); err != nil </span><span class="cov8" title="1">{
                return defaultConfidence
        }</span>

        <span class="cov8" title="1">if conf, ok := metaMap["confidence"].(float64); ok </span><span class="cov8" title="1">{
                return conf
        }</span>

        <span class="cov0" title="0">return defaultConfidence</span>
}

// calculateCompositeScore computes the final score based on the configured formula
func calculateCompositeScore(cfg *CompositeScoreConfig, scoreMap map[string]float64,
        sum, weightedSum, weightTotal float64) float64 <span class="cov8" title="1">{

        var composite float64
        switch cfg.Formula </span>{
        case "average":<span class="cov8" title="1">
                composite = sum / 3.0</span>
        case "weighted":<span class="cov0" title="0">
                if weightTotal &gt; 0 </span><span class="cov0" title="0">{
                        composite = weightedSum / weightTotal
                }</span> else<span class="cov0" title="0"> {
                        composite = 0.0
                }</span>
        case "min":<span class="cov0" title="0">
                composite = minNonNil(scoreMap, cfg.DefaultMissing)</span>
        case "max":<span class="cov0" title="0">
                composite = maxNonNil(scoreMap, cfg.DefaultMissing)</span>
        default:<span class="cov8" title="1">
                composite = sum / 3.0</span>
        }

        <span class="cov8" title="1">return composite</span>
}

// calculateConfidence determines the confidence level based on the configured method
func calculateConfidence(cfg *CompositeScoreConfig, validModels *map[string]bool,
        scoreMap map[string]float64) float64 <span class="cov8" title="1">{

        var confidence float64
        switch cfg.ConfidenceMethod </span>{
        case "count_valid":<span class="cov8" title="1">
                confidence = float64(len(*validModels)) / 3.0</span>
        case "spread":<span class="cov0" title="0">
                confidence = 1.0 - scoreSpread(scoreMap)</span>
        default:<span class="cov0" title="0">
                confidence = float64(len(*validModels)) / 3.0</span>
        }

        <span class="cov8" title="1">if confidence &lt; cfg.MinConfidence </span><span class="cov0" title="0">{
                confidence = cfg.MinConfidence
        }</span>
        <span class="cov8" title="1">if confidence &gt; cfg.MaxConfidence </span><span class="cov8" title="1">{
                confidence = cfg.MaxConfidence
        }</span>

        <span class="cov8" title="1">return confidence</span>
}
</pre>
		
		<pre class="file" id="file2" style="display: none">package llm

import (
        "encoding/json"
        "fmt"
        "os"
        "sort"
        "sync"
)

var (
        // fileCompositeScoreConfig caches the config loaded from file
        fileCompositeScoreConfig     *CompositeScoreConfig
        fileCompositeScoreConfigOnce sync.Once
)

// LoadCompositeScoreConfig loads the composite score configuration from file
func LoadCompositeScoreConfig() (*CompositeScoreConfig, error) <span class="cov8" title="1">{
        var err error
        fileCompositeScoreConfigOnce.Do(func() </span><span class="cov8" title="1">{
                const configPath = "configs/composite_score_config.json"
                f, e := os.Open(configPath)
                if e != nil </span><span class="cov0" title="0">{
                        err = fmt.Errorf("opening composite score config %q: %w", configPath, e)
                        return
                }</span>
                <span class="cov8" title="1">defer f.Close()
                decoder := json.NewDecoder(f)
                var cfg CompositeScoreConfig
                if e := decoder.Decode(&amp;cfg); e != nil </span><span class="cov0" title="0">{
                        err = fmt.Errorf("decoding composite score config %q: %w", configPath, e)
                        return
                }</span>
                <span class="cov8" title="1">if len(cfg.Models) == 0 </span><span class="cov0" title="0">{
                        err = fmt.Errorf("composite score config %q loaded but contains no models", configPath)
                        return
                }</span>
                <span class="cov8" title="1">fileCompositeScoreConfig = &amp;cfg</span>
        })
        <span class="cov8" title="1">if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov8" title="1">return fileCompositeScoreConfig, nil</span>
}

// minNonNil returns the minimum value from a map of float64s
func minNonNil(m map[string]float64, def float64) float64 <span class="cov8" title="1">{
        min := def
        first := true
        for _, v := range m </span><span class="cov8" title="1">{
                if first || v &lt; min </span><span class="cov8" title="1">{
                        min = v
                        first = false
                }</span>
        }
        <span class="cov8" title="1">return min</span>
}

// maxNonNil returns the maximum value from a map of float64s
func maxNonNil(m map[string]float64, def float64) float64 <span class="cov8" title="1">{
        max := def
        first := true
        for _, v := range m </span><span class="cov8" title="1">{
                if first || v &gt; max </span><span class="cov8" title="1">{
                        max = v
                        first = false
                }</span>
        }
        <span class="cov8" title="1">return max</span>
}

// scoreSpread calculates the difference between the maximum and minimum score
func scoreSpread(m map[string]float64) float64 <span class="cov8" title="1">{
        vals := []float64{}
        for _, v := range m </span><span class="cov8" title="1">{
                vals = append(vals, v)
        }</span>
        <span class="cov8" title="1">if len(vals) &lt; 2 </span><span class="cov8" title="1">{
                return 0.0
        }</span>
        <span class="cov8" title="1">sort.Float64s(vals)
        return vals[len(vals)-1] - vals[0]</span>
}
</pre>
		
		<pre class="file" id="file3" style="display: none">package llm

import (
        "crypto/sha256"
        "encoding/json"
        "fmt"
        "log"
        "math"
        "regexp"
        "strconv"
        "strings"
        "time"

        "github.com/alexandru-savinov/BalancedNewsGo/internal/db"
)

// callLLM queries a specific LLM with a prompt variant
func (c *LLMClient) callLLM(articleID int64, modelName string, promptVariant PromptVariant, content string) (float64, string, float64, string, error) <span class="cov0" title="0">{
        maxRetries := 2
        var lastErr error
        var rawResp string
        var score, confidence float64
        var explanation string

        for attempt := 0; attempt &lt;= maxRetries; attempt++ </span><span class="cov0" title="0">{
                prompt := promptVariant.FormatPrompt(content)

                // Compute prompt hash for logging
                h := sha256.Sum256([]byte(prompt))
                promptHash := fmt.Sprintf("%x", h[:8]) // first 8 bytes as hex string
                promptSnippet := prompt
                if len(promptSnippet) &gt; 80 </span><span class="cov0" title="0">{
                        promptSnippet = promptSnippet[:80] + "..."
                }</span>
                <span class="cov0" title="0">log.Printf("Prompt snippet [%s] (attempt %d): %s", promptHash, attempt+1, promptSnippet)

                var err error
                // Use the generic LLM service stored in the client
                if c.llmService == nil </span><span class="cov0" title="0">{
                        log.Printf("[LLM] ArticleID %d | Model %s | PromptHash %s | LLM service not initialized", articleID, modelName, promptHash)
                        return 0, "", 0, "", fmt.Errorf("LLM service not initialized")
                }</span>

                // We need the raw response string and the parsed score/confidence/explanation.
                // The current HTTPLLMService.AnalyzeWithPrompt returns a *db.LLMScore which contains metadata,
                // but not necessarily the raw response string needed for parseLLMResponse here.
                // Let's adapt by calling the lower-level API call method directly.
                // Assuming c.llmService is *HTTPLLMService (might need type assertion)
                <span class="cov0" title="0">httpService, ok := c.llmService.(*HTTPLLMService)
                if !ok </span><span class="cov0" title="0">{
                        log.Printf("[LLM] ArticleID %d | Model %s | PromptHash %s | LLM service is not *HTTPLLMService", articleID, modelName, promptHash)
                        return 0, "", 0, "", fmt.Errorf("LLM service is not *HTTPLLMService")
                }</span>

                // Call the underlying API method
                <span class="cov0" title="0">apiResp, err := httpService.callLLMAPIWithKey(modelName, prompt, httpService.apiKey) // Use renamed function and pass primary key
                if err != nil </span><span class="cov0" title="0">{
                        // Error is already logged within callLLMAPI
                        lastErr = err
                        // Try to get raw response body even on error for logging/parsing attempts
                        if apiResp != nil </span><span class="cov0" title="0">{
                                rawResp = apiResp.String()
                        }</span>
                        <span class="cov0" title="0">continue</span> // Retry
                }
                <span class="cov0" title="0">rawResp = apiResp.String() // Store raw response on success

                // --- BEGIN INSERTED: Check for embedded error structure ---
                var genericResponse map[string]interface{}
                if errUnmarshal := json.Unmarshal([]byte(rawResp), &amp;genericResponse); errUnmarshal == nil </span><span class="cov0" title="0">{
                        if errorField, ok := genericResponse["error"].(map[string]interface{}); ok </span><span class="cov0" title="0">{
                                if message, msgOK := errorField["message"].(string); msgOK &amp;&amp; message != "" </span><span class="cov0" title="0">{
                                        errType, _ := errorField["type"].(string)
                                        codeVal := errorField["code"]
                                        isRateLimit := strings.Contains(strings.ToLower(message), "rate limit exceeded") || fmt.Sprintf("%v", codeVal) == "429"

                                        if isRateLimit </span><span class="cov0" title="0">{
                                                log.Printf("[LLM] ArticleID %d | Model %s | PromptHash %s | Detected embedded rate limit: %s", articleID, modelName, promptHash, message)
                                                lastErr = ErrBothLLMKeysRateLimited // Use sentinel error
                                        }</span> else<span class="cov0" title="0"> {
                                                log.Printf("[LLM] ArticleID %d | Model %s | PromptHash %s | Detected embedded API error: %s", articleID, modelName, promptHash, message)
                                                lastErr = fmt.Errorf("API error: %s (type: %s, code: %v)", message, errType, codeVal)
                                        }</span>
                                        <span class="cov0" title="0">continue</span> // Skip parsing, retry loop
                                }
                        }
                }
                // --- END INSERTED: Check for embedded error structure ---

                <span class="cov0" title="0">var parseErr error
                // Use the renamed parser for nested JSON expected in this ensemble context
                score, explanation, confidence, parseErr = parseNestedLLMJSONResponse(rawResp)
                if parseErr != nil </span><span class="cov0" title="0">{
                        rawSnippet := rawResp
                        if len(rawSnippet) &gt; 200 </span><span class="cov0" title="0">{
                                rawSnippet = rawSnippet[:200] + "..."
                        }</span>
                        <span class="cov0" title="0">log.Printf("[LLM] ArticleID %d | Model %s | PromptHash %s | Parse error: %v | Raw response: %s", articleID, modelName, promptHash, parseErr, rawSnippet)
                        if articleID == 133 </span><span class="cov0" title="0">{
                                log.Printf("[DEBUG][Article 133] Parse error: %v", parseErr)
                                log.Printf("[DEBUG][Article 133] FULL raw response:\n%s", rawResp)
                        }</span>
                        <span class="cov0" title="0">lastErr = parseErr
                        continue</span>
                }

                // Validate parsed values
                <span class="cov0" title="0">if confidence == 0 </span><span class="cov0" title="0">{
                        log.Printf("[LLM] ArticleID %d | Model %s | PromptHash %s | Invalid zero confidence, retrying...", articleID, modelName, promptHash)
                        lastErr = fmt.Errorf("invalid zero confidence")
                        continue</span>
                }

                <span class="cov0" title="0">log.Printf("[LLM] ArticleID %d | Model %s | PromptHash %s | Success | Score: %.3f | Confidence: %.3f", articleID, modelName, promptHash, score, confidence)
                return score, explanation, confidence, rawResp, nil</span>
        }

        <span class="cov0" title="0">log.Printf("[LLM] ArticleID %d | Model %s | Final failure after retries. Last error: %v", articleID, modelName, lastErr)
        return 0, "", 0, rawResp, lastErr</span>
}

// Removed callOpenAIAPI as it's replaced by direct use of httpService.callLLMAPI

// parseNestedLLMJSONResponse extracts score, explanation, confidence from a raw response
// where the LLM is expected to return a JSON string *within* the main content field
// (e.g., {"choices":[{"message":{"content":"{\"score\":...}"}}]}), or in text format
// with patterns like "Score: X.X" and "Confidence: X.X".
func parseNestedLLMJSONResponse(rawResp string) (float64, string, float64, error) <span class="cov8" title="1">{
        // Step 1: Parse the OpenAI API response JSON
        var apiResp struct {
                Choices []struct {
                        Message struct {
                                Content string `json:"content"`
                        } `json:"message"`
                } `json:"choices"`
        }
        err := json.Unmarshal([]byte(rawResp), &amp;apiResp)
        if err != nil </span><span class="cov8" title="1">{
                return 0, "", 0, fmt.Errorf("error parsing outer LLM API response JSON: %w", err)
        }</span>
        <span class="cov8" title="1">if len(apiResp.Choices) == 0 </span><span class="cov8" title="1">{
                return 0, "", 0, fmt.Errorf("no choices in outer LLM API response")
        }</span>

        // Step 2: Extract the content string
        <span class="cov8" title="1">contentStr := apiResp.Choices[0].Message.Content

        // Step 3: First try to parse as JSON
        // Try JSON format first (with or without backticks)
        var innerResp struct {
                Score       float64 `json:"score"`
                Explanation string  `json:"explanation"`
                Confidence  float64 `json:"confidence"`
        }

        // Add robust backtick stripping
        re := regexp.MustCompile("(?s)```(?:json)?\\s*(.*?)\\s*```") // Matches ```json ... ``` or ``` ... ```
        matches := re.FindStringSubmatch(contentStr)
        if len(matches) &gt;= 2 </span><span class="cov8" title="1">{
                contentStr = strings.TrimSpace(matches[1]) // Use the captured group
        }</span>

        // Try to parse as JSON
        <span class="cov8" title="1">if err = json.Unmarshal([]byte(contentStr), &amp;innerResp); err == nil </span><span class="cov8" title="1">{
                // Successfully parsed as JSON
                return innerResp.Score, innerResp.Explanation, innerResp.Confidence, nil
        }</span>

        // Step 4: If JSON parsing fails, try to extract values using regex patterns
        // Extract score with regex
        <span class="cov8" title="1">scoreRegex := regexp.MustCompile(`Score: (-?\d+\.?\d*)`)
        scoreMatches := scoreRegex.FindStringSubmatch(contentStr)
        if len(scoreMatches) &lt; 2 </span><span class="cov8" title="1">{
                return 0, "", 0, fmt.Errorf("error parsing inner content JSON: %w", err)
        }</span>
        <span class="cov8" title="1">score, err := strconv.ParseFloat(scoreMatches[1], 64)
        if err != nil </span><span class="cov0" title="0">{
                return 0, "", 0, fmt.Errorf("invalid score format: %w", err)
        }</span>

        // Extract confidence with regex
        <span class="cov8" title="1">confidenceRegex := regexp.MustCompile(`Confidence: (\d+\.?\d*)`)
        confidenceMatches := confidenceRegex.FindStringSubmatch(contentStr)
        if len(confidenceMatches) &lt; 2 </span><span class="cov0" title="0">{
                // Default confidence if not found
                return score, "Extracted from text response", 0.5, nil
        }</span>
        <span class="cov8" title="1">confidence, err := strconv.ParseFloat(confidenceMatches[1], 64)
        if err != nil </span><span class="cov0" title="0">{
                return 0, "", 0, fmt.Errorf("invalid confidence format: %w", err)
        }</span>

        // Extract reasoning or explanation
        <span class="cov8" title="1">explanation := "Extracted from text response"
        reasoningRegex := regexp.MustCompile(`Reasoning: (.+)`)
        reasoningMatches := reasoningRegex.FindStringSubmatch(contentStr)
        if len(reasoningMatches) &gt;= 2 </span><span class="cov8" title="1">{
                explanation = reasoningMatches[1]
        }</span>

        <span class="cov8" title="1">return score, explanation, confidence, nil</span>
}

// EnsembleAnalyze performs multi-model, multi-prompt ensemble analysis
func (c *LLMClient) EnsembleAnalyze(articleID int64, content string) (*db.LLMScore, error) <span class="cov0" title="0">{
        // Use models defined in the loaded configuration
        if c.config == nil || len(c.config.Models) == 0 </span><span class="cov0" title="0">{
                log.Printf("[Ensemble] ArticleID %d | Error: LLMClient config is nil or has no models defined.", articleID)
                return nil, fmt.Errorf("LLMClient config is nil or has no models defined")
        }</span>
        // Extract model names from the config
        <span class="cov0" title="0">models := make([]string, 0, len(c.config.Models))
        for _, modelCfg := range c.config.Models </span><span class="cov0" title="0">{
                if modelCfg.ModelName == "" </span><span class="cov0" title="0">{
                        log.Printf("[Ensemble] Warning: Skipping model config with empty name (Perspective: %s)", modelCfg.Perspective)
                        continue</span>
                }
                <span class="cov0" title="0">models = append(models, modelCfg.ModelName)</span>
        }
        <span class="cov0" title="0">if len(models) == 0 </span><span class="cov0" title="0">{
                log.Printf("[Ensemble] ArticleID %d | Error: No valid models found in configuration after filtering.", articleID)
                return nil, fmt.Errorf("no valid models found in configuration")
        }</span>
        <span class="cov0" title="0">log.Printf("[Ensemble] ArticleID %d | Using %d models from config: %v", articleID, len(models), models)
        promptVariants := loadPromptVariants()

        type SubResult struct {
                Model         string  `json:"model"`
                PromptVariant string  `json:"prompt_variant"`
                Score         float64 `json:"score"`
                Explanation   string  `json:"explanation"`
                Confidence    float64 `json:"confidence"`
                RawResponse   string  `json:"raw_response"`
        }

        allSubResults := make([]SubResult, 0)
        perModelResults := make(map[string][]SubResult)
        perModelAgg := make(map[string]map[string]float64)

        const minValid = 1
        const maxAttempts = 6
        const confidenceThreshold = 0.5

        for _, model := range models </span><span class="cov0" title="0">{
                validResponses := make([]SubResult, 0, minValid)
                attempts := 0
        outer:
                for attempts &lt; maxAttempts &amp;&amp; len(validResponses) &lt; minValid </span><span class="cov0" title="0">{
                        for _, pv := range promptVariants </span><span class="cov0" title="0">{
                                for retry := 0; retry &lt; 2 &amp;&amp; attempts &lt; maxAttempts &amp;&amp; len(validResponses) &lt; minValid; retry++ </span><span class="cov0" title="0">{
                                        attempts++
                                        score, explanation, confidence, rawResp, err := c.callLLM(articleID, model, pv, content)
                                        if err != nil </span><span class="cov0" title="0">{
                                                continue</span>
                                        }
                                        <span class="cov0" title="0">sub := SubResult{
                                                Model: model, PromptVariant: pv.ID,
                                                Score: score, Explanation: explanation,
                                                Confidence: confidence, RawResponse: rawResp,
                                        }
                                        allSubResults = append(allSubResults, sub)
                                        if confidence &gt;= confidenceThreshold </span><span class="cov0" title="0">{
                                                validResponses = append(validResponses, sub)
                                        }</span>
                                        <span class="cov0" title="0">if len(validResponses) &gt;= minValid || attempts &gt;= maxAttempts </span><span class="cov0" title="0">{
                                                break outer</span>
                                        }
                                }
                        }
                }

                <span class="cov0" title="0">if len(validResponses) == 0 </span><span class="cov0" title="0">{
                        log.Printf("[Ensemble] Model %s: no valid high-confidence responses. Failing ensemble.", model)
                        return nil, fmt.Errorf("ensemble failed: no valid high-confidence responses from model %s", model)
                }</span>

                <span class="cov0" title="0">var sum, weightedSum, sumWeights float64
                for _, r := range validResponses </span><span class="cov0" title="0">{
                        sum += r.Score
                        weightedSum += r.Score * r.Confidence
                        sumWeights += r.Confidence
                }</span>
                <span class="cov0" title="0">mean := sum / float64(len(validResponses))
                weightedMean := weightedSum / math.Max(sumWeights, 1e-6)

                var varianceSum float64
                for _, r := range validResponses </span><span class="cov0" title="0">{
                        diff := r.Score - mean
                        varianceSum += diff * diff
                }</span>
                <span class="cov0" title="0">variance := varianceSum / float64(len(validResponses))

                perModelResults[model] = validResponses
                perModelAgg[model] = map[string]float64{
                        "mean":          mean,
                        "weighted_mean": weightedMean,
                        "variance":      variance,
                        "count":         float64(len(validResponses)),
                }

                log.Printf("[Ensemble] Model %s: %d valid responses, weighted mean=%.3f, variance=%.3f", model, len(validResponses), weightedMean, variance)</span>
        }

        <span class="cov0" title="0">if len(perModelAgg) == 0 </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("no valid LLM responses from any model")
        }</span>

        // Aggregate across models
        <span class="cov0" title="0">var totalWeightedSum, totalSumWeights float64
        for _, agg := range perModelAgg </span><span class="cov0" title="0">{
                weight := agg["count"] // or customize per model
                totalWeightedSum += agg["weighted_mean"] * weight
                totalSumWeights += weight
        }</span>
        <span class="cov0" title="0">finalScore := totalWeightedSum / math.Max(totalSumWeights, 1e-6)

        // Compute overall variance (average of per-model variances weighted by count)
        var totalVariance float64
        for _, agg := range perModelAgg </span><span class="cov0" title="0">{
                totalVariance += agg["variance"] * agg["count"]
        }</span>
        <span class="cov0" title="0">totalVariance /= math.Max(totalSumWeights, 1e-6)

        uncertaintyFlag := totalVariance &gt; 0.1 || (totalSumWeights/float64(len(perModelAgg)*minValid) &lt; 0.5)

        meta := map[string]interface{}{
                "all_sub_results":       allSubResults,
                "per_model_results":     perModelResults,
                "per_model_aggregation": perModelAgg,
                "final_aggregation": map[string]interface{}{
                        "weighted_mean":    finalScore,
                        "variance":         totalVariance,
                        "uncertainty_flag": uncertaintyFlag,
                },
                "timestamp": time.Now().Format(time.RFC3339),
        }
        metaBytes, _ := json.Marshal(meta)

        return &amp;db.LLMScore{
                Model:     "ensemble",
                Score:     finalScore,
                Metadata:  string(metaBytes),
                CreatedAt: time.Now(),
        }, nil</span>
}

const promptScaleFragment = "on a scale from -1.0 (strongly left) to 1.0 (strongly right). Respond with a JSON object containing 'score', "
const promptJsonFieldsFragment = "'explanation', and 'confidence'."

// loadPromptVariants returns hardcoded prompt variants (replace with config later)
func loadPromptVariants() []PromptVariant <span class="cov0" title="0">{
        return []PromptVariant{
                {
                        ID: "default",
                        Template: "Please analyze the political bias of the following article on a scale from -1.0 (strongly left) " +
                                "to 1.0 (strongly right). Respond ONLY with a valid JSON object containing 'score', 'explanation', and 'confidence'. Do not include any other text or formatting.",
                        Examples: []string{
                                `{"score": -1.0, "explanation": "Strongly left-leaning language", "confidence": 0.9}`,
                                `{"score": 0.0, "explanation": "Neutral reporting", "confidence": 0.95}`,
                                `{"score": 1.0, "explanation": "Strongly right-leaning language", "confidence": 0.9}`,
                        },
                },
                {
                        ID: "left_focus",
                        Template: "From a progressive or left-leaning perspective, analyze the political bias of the following article " +
                                promptScaleFragment +
                                promptJsonFieldsFragment + "\nRespond ONLY with a valid JSON object containing 'score', 'explanation', and 'confidence'. Do not include any other text or formatting.",
                        Examples: []string{
                                `{"score": -1.0, "explanation": "Strongly aligns with progressive viewpoints", "confidence": 0.9}`,
                                `{"score": 0.0, "explanation": "Balanced or neutral reporting", "confidence": 0.95}`,
                                `{"score": 1.0, "explanation": "Strongly opposes progressive viewpoints", "confidence": 0.9}`,
                        },
                },
                {
                        ID: "center_focus",
                        Template: "From a centrist or neutral perspective, analyze the political bias of the following article " +
                                promptScaleFragment +
                                promptJsonFieldsFragment + "\nRespond ONLY with a valid JSON object containing 'score', 'explanation', and 'confidence'. Do not include any other text or formatting.",
                        Examples: []string{
                                `{"score": -1.0, "explanation": "Clearly favors left-leaning positions", "confidence": 0.9}`,
                                `{"score": 0.0, "explanation": "Appears balanced without clear bias", "confidence": 0.95}`,
                                `{"score": 1.0, "explanation": "Clearly favors right-leaning positions", "confidence": 0.9}`,
                        },
                },
                {
                        ID: "right_focus",
                        Template: "From a conservative or right-leaning perspective, analyze the political bias of the following article " +
                                promptScaleFragment +
                                promptJsonFieldsFragment + "\nRespond ONLY with a valid JSON object containing 'score', 'explanation', and 'confidence'. Do not include any other text or formatting.",
                        Examples: []string{
                                `{"score": -1.0, "explanation": "Strongly opposes conservative viewpoints", "confidence": 0.9}`,
                                `{"score": 0.0, "explanation": "Balanced or neutral reporting", "confidence": 0.95}`,
                                `{"score": 1.0, "explanation": "Strongly aligns with conservative viewpoints", "confidence": 0.9}`,
                        },
                },
        }
}</span>
</pre>
		
		<pre class="file" id="file4" style="display: none">package llm

import (
        "context"
        "crypto/sha256"
        "encoding/json"
        "errors"
        "fmt"
        "log"
        "net/http"
        "os"
        "sort"
        "strings"
        "time"

        "github.com/alexandru-savinov/BalancedNewsGo/internal/apperrors"
        "github.com/alexandru-savinov/BalancedNewsGo/internal/db"
        "github.com/go-resty/resty/v2"
        "github.com/jmoiron/sqlx"
)

// ErrInvalidLLMResponse represents an invalid response from LLM service
var ErrInvalidLLMResponse = apperrors.New("Invalid response from LLM service", "llm_service_error")

// HTTP timeout for LLM requests
const defaultLLMTimeout = 30 * time.Second

const (
        LabelUnknown = "unknown"
        LabelLeft    = "left"
        LabelRight   = "right"
        LabelNeutral = "neutral"
)

// compositeScoreConfig holds test override when len(Models)==0
var compositeScoreConfig *CompositeScoreConfig

type ModelConfig struct {
        Perspective string `json:"perspective"`
        ModelName   string `json:"modelName"`
        URL         string `json:"url"`
}

type CompositeScoreConfig struct {
        Formula          string             `json:"formula"`
        Weights          map[string]float64 `json:"weights"`
        MinScore         float64            `json:"min_score"`
        MaxScore         float64            `json:"max_score"`
        DefaultMissing   float64            `json:"default_missing"`
        HandleInvalid    string             `json:"handle_invalid"`
        ConfidenceMethod string             `json:"confidence_method"`
        MinConfidence    float64            `json:"min_confidence"`
        MaxConfidence    float64            `json:"max_confidence"`
        Models           []ModelConfig      `json:"models"`
}

// PromptVariant defines a prompt template with few-shot examples
type PromptVariant struct {
        ID       string
        Template string
        Examples []string
        Model    string // Model name for this variant
        URL      string // API endpoint URL
}

// GeneratePrompt formats the prompt template with content
func (pv *PromptVariant) FormatPrompt(content string) string <span class="cov8" title="1">{
        examplesText := strings.Join(pv.Examples, "\n")
        return fmt.Sprintf("%s\n%s\nArticle:\n%s", pv.Template, examplesText, content)
}</span>

// DefaultPromptVariant is the standard prompt template for analyzing articles
var DefaultPromptVariant = PromptVariant{
        ID: "default",
        Template: "Please analyze the political bias of the following article on a scale from -1.0 (strongly left) " +
                "to 1.0 (strongly right). Respond ONLY with a valid JSON object containing 'score', 'explanation', and 'confidence'. Do not include any other text or formatting.",
        Examples: []string{
                `{"score": -1.0, "explanation": "Strongly left-leaning language", "confidence": 0.9}`,
                `{"score": 0.0, "explanation": "Neutral reporting", "confidence": 0.95}`,
                `{"score": 1.0, "explanation": "Strongly right-leaning language", "confidence": 0.9}`,
        },
}

// Returns (compositeScore, confidence, error)
func ComputeCompositeScoreWithConfidence(scores []db.LLMScore) (float64, float64, error) <span class="cov8" title="1">{
        // Use override config in tests if provided, else use built-in defaults
        var cfg *CompositeScoreConfig
        if compositeScoreConfig != nil </span><span class="cov8" title="1">{
                cfg = compositeScoreConfig
                // clear override for next invocation
                compositeScoreConfig = nil
        }</span> else<span class="cov8" title="1"> {
                cfg = &amp;CompositeScoreConfig{
                        Formula:          "average",
                        Weights:          map[string]float64{"left": 1.0, "center": 1.0, "right": 1.0},
                        MinScore:         -1.0,
                        MaxScore:         1.0,
                        DefaultMissing:   0.0,
                        HandleInvalid:    "default",
                        ConfidenceMethod: "count_valid",
                }
        }</span>

        // Map scores to perspectives
        <span class="cov8" title="1">scoreMap := map[string]float64{"left": cfg.DefaultMissing, "center": cfg.DefaultMissing, "right": cfg.DefaultMissing}
        validModels := make(map[string]bool)
        sumScores := 0.0
        sumWeights := 0.0
        for _, s := range scores </span><span class="cov8" title="1">{
                model := strings.ToLower(s.Model)
                var perspective string
                switch model </span>{
                case "left":<span class="cov8" title="1">
                        perspective = "left"</span>
                case "center":<span class="cov8" title="1">
                        perspective = "center"</span>
                case "right":<span class="cov8" title="1">
                        perspective = "right"</span>
                default:<span class="cov8" title="1">
                        continue</span>
                }
                <span class="cov8" title="1">val := s.Score
                if cfg.HandleInvalid == "ignore" &amp;&amp; (isInvalid(val) || val &lt; cfg.MinScore || val &gt; cfg.MaxScore) </span><span class="cov8" title="1">{
                        continue</span>
                }
                <span class="cov8" title="1">if isInvalid(val) || val &lt; cfg.MinScore || val &gt; cfg.MaxScore </span><span class="cov8" title="1">{
                        val = cfg.DefaultMissing
                }</span>
                <span class="cov8" title="1">w := 1.0
                if cfg.Formula == "weighted" </span><span class="cov8" title="1">{
                        if weight, ok := cfg.Weights[perspective]; ok </span><span class="cov8" title="1">{
                                w = weight
                        }</span>
                }
                <span class="cov8" title="1">scoreMap[perspective] = val
                sumScores += val * w
                sumWeights += w
                validModels[perspective] = true</span>
        }
        // Compute composite
        <span class="cov8" title="1">var composite float64
        switch cfg.Formula </span>{
        case "weighted":<span class="cov8" title="1">
                if sumWeights &gt; 0 </span><span class="cov8" title="1">{
                        composite = sumScores / sumWeights
                }</span>
        default:<span class="cov8" title="1"> // average and others: simple average over three perspectives
                composite = (scoreMap["left"] + scoreMap["center"] + scoreMap["right"]) / 3.0</span>
        }
        // Compute confidence
        <span class="cov8" title="1">var confidence float64
        switch cfg.ConfidenceMethod </span>{
        case "count_valid":<span class="cov8" title="1">
                confidence = float64(len(validModels)) / 3.0</span>
        case "spread":<span class="cov8" title="1">
                // normalize spread across configured score range
                span := cfg.MaxScore - cfg.MinScore
                if span &gt; 0 </span><span class="cov8" title="1">{
                        // difference between max and min perspective scores
                        vals := []float64{scoreMap["left"], scoreMap["center"], scoreMap["right"]}
                        sort.Float64s(vals)
                        confidence = (vals[2] - vals[0]) / span
                }</span>
        default:<span class="cov0" title="0">
                confidence = float64(len(validModels)) / 3.0</span>
        }
        <span class="cov8" title="1">return composite, confidence, nil</span>
}

func ComputeCompositeScore(scores []db.LLMScore) float64 <span class="cov8" title="1">{
        score, _, _ := ComputeCompositeScoreWithConfidence(scores)
        return score
}</span>

func isInvalid(f float64) bool <span class="cov8" title="1">{
        return (f != f) || (f &gt; 1e10) || (f &lt; -1e10)
}</span>

func parseLLMAPIResponse(body []byte) (string, error) <span class="cov8" title="1">{
        var directResponse struct {
                Text   string `json:"text"`
                Result string `json:"result"`
                Output string `json:"output"`
        }

        if err := json.Unmarshal(body, &amp;directResponse); err == nil </span><span class="cov8" title="1">{
                if directResponse.Text != "" </span><span class="cov0" title="0">{
                        return directResponse.Text, nil
                }</span>
                <span class="cov8" title="1">if directResponse.Result != "" </span><span class="cov0" title="0">{
                        return directResponse.Result, nil
                }</span>
                <span class="cov8" title="1">if directResponse.Output != "" </span><span class="cov0" title="0">{
                        return directResponse.Output, nil
                }</span>
        }

        <span class="cov8" title="1">var genericResponse map[string]interface{}
        if err := json.Unmarshal(body, &amp;genericResponse); err == nil </span><span class="cov8" title="1">{
                if errorField, ok := genericResponse["error"].(map[string]interface{}); ok </span><span class="cov8" title="1">{
                        if message, ok := errorField["message"].(string); ok </span><span class="cov8" title="1">{
                                errType := errorField["type"]
                                errCode := errorField["code"]
                                if strings.Contains(strings.ToLower(message), "rate limit") </span><span class="cov0" title="0">{
                                        return "", ErrBothLLMKeysRateLimited
                                }</span>
                                <span class="cov8" title="1">return "", apperrors.HandleError(
                                        fmt.Errorf("API error: %s (type: %v, code: %v)", message, errType, errCode),
                                        "LLM service error response",
                                )</span>
                        }
                }
        }

        <span class="cov0" title="0">var standardResp struct {
                Choices []struct {
                        Message struct {
                                Content string `json:"content"`
                        } `json:"message"`
                } `json:"choices"`
        }

        if err := json.Unmarshal(body, &amp;standardResp); err != nil </span><span class="cov0" title="0">{
                return "", apperrors.HandleError(err, "failed to parse LLM response")
        }</span>

        <span class="cov0" title="0">if len(standardResp.Choices) == 0 || standardResp.Choices[0].Message.Content == "" </span><span class="cov0" title="0">{
                return "", ErrInvalidLLMResponse
        }</span>

        <span class="cov0" title="0">return standardResp.Choices[0].Message.Content, nil</span>
}

// LLMClient provides methods to analyze articles using language models
type LLMClient struct {
        client     *http.Client
        cache      *Cache
        db         *sqlx.DB
        llmService LLMService
        config     *CompositeScoreConfig
}

// ArticleAnalysis represents the full analysis results for an article
type ArticleAnalysis struct {
        ArticleID       int64                  `json:"article_id"`
        Scores          []db.LLMScore          `json:"scores"`
        CompositeScore  float64                `json:"composite_score"`
        Confidence      float64                `json:"confidence"`
        CategoryScores  map[string]float64     `json:"category_scores"`
        DetailedResults map[string]interface{} `json:"detailed_results"`
        CreatedAt       time.Time              `json:"created_at"`
}

func (c *LLMClient) SetHTTPLLMTimeout(timeout time.Duration) <span class="cov8" title="1">{
        httpService, ok := c.llmService.(*HTTPLLMService)
        if ok &amp;&amp; httpService != nil &amp;&amp; httpService.client != nil </span><span class="cov8" title="1">{
                httpService.client.SetTimeout(timeout)
        }</span>
}

func NewLLMClient(dbConn *sqlx.DB) *LLMClient <span class="cov8" title="1">{
        cache := NewCache()

        // Get OpenRouter configuration
        primaryKey := os.Getenv("LLM_API_KEY")
        backupKey := os.Getenv("LLM_API_KEY_SECONDARY")
        baseURL := os.Getenv("LLM_BASE_URL")

        // Replace fatal exit with panic for missing primary key to satisfy tests
        if primaryKey == "" </span><span class="cov8" title="1">{
                panic("ERROR: LLM_API_KEY not set")</span>
        }

        <span class="cov8" title="1">config := &amp;CompositeScoreConfig{
                Formula:          "average",
                Weights:          map[string]float64{"left": 1.0, "center": 1.0, "right": 1.0},
                MinScore:         -1e10,
                MaxScore:         1e10,
                DefaultMissing:   0.0,
                HandleInvalid:    "default",
                ConfidenceMethod: "count_valid",
                MinConfidence:    0.0,
                MaxConfidence:    1.0,
        }

        // Create resty client with timeout
        restyClient := resty.New()
        restyClient.SetTimeout(defaultLLMTimeout)

        // Initialize service with OpenRouter configuration
        service := NewHTTPLLMService(restyClient, primaryKey, backupKey, baseURL)

        return &amp;LLMClient{
                client:     &amp;http.Client{},
                cache:      cache,
                db:         dbConn,
                llmService: service,
                config:     config,
        }</span>
}

func (c *LLMClient) analyzeContent(articleID int64, content string, model string) (*db.LLMScore, error) <span class="cov0" title="0">{
        log.Printf("[analyzeContent] Entry: articleID=%d, model=%s", articleID, model)
        contentHash := hashContent(content)

        if cached, ok := c.cache.Get(contentHash, model); ok </span><span class="cov0" title="0">{
                return cached, nil
        }</span>

        <span class="cov0" title="0">var err error

        generalPrompt := PromptVariant{
                ID: "default",
                Template: "Please analyze the political bias of the following article on a scale from -1.0 (strongly left) " +
                        "to 1.0 (strongly right). Respond ONLY with a valid JSON object containing 'score', 'explanation', and 'confidence'. Do not include any other text or formatting.",
                Examples: []string{
                        `{"score": -1.0, "explanation": "Strongly left-leaning language", "confidence": 0.9}`,
                        `{"score": 0.0, "explanation": "Neutral reporting", "confidence": 0.95}`,
                        `{"score": 1.0, "explanation": "Strongly right-leaning language", "confidence": 0.9}`,
                },
        }

        scoreVal, explanation, confidence, _, err := c.callLLM(articleID, model, generalPrompt, content)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        <span class="cov0" title="0">meta := fmt.Sprintf(`{"explanation": %q, "confidence": %.3f}`, explanation, confidence)

        score := &amp;db.LLMScore{
                ArticleID: articleID,
                Model:     model,
                Score:     scoreVal,
                Metadata:  meta,
                CreatedAt: time.Now(),
        }

        c.cache.Set(contentHash, model, score)

        return score, nil</span>
}

func (c *LLMClient) ProcessUnscoredArticles() error <span class="cov0" title="0">{
        query := `
        SELECT a.* FROM articles a
        WHERE NOT EXISTS (
                SELECT 1 FROM llm_scores s
                WHERE s.article_id = a.id
        )
        `
        var articles []db.Article
        if err := c.db.Select(&amp;articles, query); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov0" title="0">for _, article := range articles </span><span class="cov0" title="0">{
                if err := c.AnalyzeAndStore(&amp;article); err != nil </span><span class="cov0" title="0">{
                        log.Printf("Failed to analyze article ID %d: %v", article.ID, err)
                }</span>
        }

        <span class="cov0" title="0">return nil</span>
}

func (c *LLMClient) AnalyzeAndStore(article *db.Article) error <span class="cov0" title="0">{
        cfg := &amp;CompositeScoreConfig{
                Formula:          "average",
                Weights:          map[string]float64{"left": 1.0, "center": 1.0, "right": 1.0},
                MinScore:         -1e10,
                MaxScore:         1e10,
                DefaultMissing:   0.0,
                HandleInvalid:    "default",
                ConfidenceMethod: "count_valid",
                MinConfidence:    0.0,
                MaxConfidence:    1.0,
        }

        for _, m := range cfg.Models </span><span class="cov0" title="0">{
                log.Printf("[DEBUG][AnalyzeAndStore] Article %d | Perspective: %s | ModelName passed: %s | URL: %s", article.ID, m.Perspective, m.ModelName, m.URL)
                score, err := c.analyzeContent(article.ID, article.Content, m.ModelName)
                if err != nil </span><span class="cov0" title="0">{
                        log.Printf("Error analyzing article %d with model %s: %v", article.ID, m.ModelName, err)

                        continue</span>
                }

                <span class="cov0" title="0">_, err = db.InsertLLMScore(c.db, score)
                if err != nil </span><span class="cov0" title="0">{
                        log.Printf("Error inserting LLM score for article %d model %s: %v", article.ID, m.ModelName, err)
                }</span>
        }

        <span class="cov0" title="0">return nil</span>
}

func (c *LLMClient) ReanalyzeArticle(articleID int64) error <span class="cov0" title="0">{
        log.Printf("[ReanalyzeArticle %d] Starting reanalysis", articleID)
        tx, err := c.db.Beginx()
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov0" title="0">log.Printf("[ReanalyzeArticle %d] Deleting existing scores", articleID)
        _, err = tx.Exec("DELETE FROM llm_scores WHERE article_id = ?", articleID)
        if err != nil </span><span class="cov0" title="0">{
                if rbErr := tx.Rollback(); rbErr != nil </span><span class="cov0" title="0">{
                        log.Printf("tx.Rollback() failed: %v", rbErr)
                }</span>

                <span class="cov0" title="0">return err</span>
        }

        <span class="cov0" title="0">var article db.Article

        log.Printf("[ReanalyzeArticle %d] Fetching article data", articleID)
        err = tx.Get(&amp;article, "SELECT * FROM articles WHERE id = ?", articleID)
        if err != nil </span><span class="cov0" title="0">{
                if rbErr := tx.Rollback(); rbErr != nil </span><span class="cov0" title="0">{
                        log.Printf("tx.Rollback() failed: %v", rbErr)
                }</span>

                <span class="cov0" title="0">return err</span>
        }

        <span class="cov0" title="0">log.Printf("[ReanalyzeArticle %d] Fetched article: Title='%.50s'", articleID, article.Title)
        log.Printf("[ReanalyzeArticle %d] Starting analysis loop for models", articleID)
        for _, m := range compositeScoreConfig.Models </span><span class="cov0" title="0">{
                log.Printf("[ReanalyzeArticle %d] Calling analyzeContent for model: %s", articleID, m.ModelName)
                score, err := c.analyzeContent(article.ID, article.Content, m.ModelName)
                if err != nil </span><span class="cov0" title="0">{
                        log.Printf("[ReanalyzeArticle %d] Error from analyzeContent for %s: %v", articleID, m.ModelName, err)
                        continue</span>
                }
                <span class="cov0" title="0">log.Printf("[ReanalyzeArticle %d] analyzeContent successful for: %s. Score: %.2f", articleID, m.ModelName, score.Score)

                _, err = tx.NamedExec(`INSERT INTO llm_scores (article_id, model, score, metadata)
                        VALUES (:article_id, :model, :score, :metadata)`, score)
                if err != nil </span><span class="cov0" title="0">{
                        log.Printf("[ReanalyzeArticle %d] Error inserting score for %s: %v", articleID, m.ModelName, err)
                }</span> else<span class="cov0" title="0"> {
                        log.Printf("[ReanalyzeArticle %d] Successfully inserted score for: %s", articleID, m.ModelName)
                }</span>
        }

        <span class="cov0" title="0">scores, err := c.FetchScores(article.ID)
        if err != nil </span><span class="cov0" title="0">{
                log.Printf("[ReanalyzeArticle %d] Error fetching scores: %v", articleID, err)
                return fmt.Errorf("failed to fetch scores for ensemble calculation: %w", err)
        }</span>

        <span class="cov0" title="0">finalScore, confidence, err := ComputeCompositeScoreWithConfidence(scores)
        if err != nil </span><span class="cov0" title="0">{
                log.Printf("[ReanalyzeArticle %d] Error computing composite score: %v", articleID, err)
                return fmt.Errorf("failed to compute composite score: %w", err)
        }</span>

        <span class="cov0" title="0">meta := map[string]interface{}{
                "timestamp": time.Now().Format(time.RFC3339),
                "final_aggregation": map[string]interface{}{
                        "weighted_mean": finalScore,
                        "variance":      1.0 - confidence,
                },
        }
        metaBytes, _ := json.Marshal(meta)

        ensembleScore := &amp;db.LLMScore{
                ArticleID: article.ID,
                Model:     "ensemble",
                Score:     finalScore,
                Metadata:  string(metaBytes),
                CreatedAt: time.Now(),
        }

        _, err = tx.NamedExec(`INSERT INTO llm_scores (article_id, model, score, metadata, created_at)
                VALUES (:article_id, :model, :score, :metadata, :created_at)`, ensembleScore)
        if err != nil </span><span class="cov0" title="0">{
                log.Printf("[ReanalyzeArticle %d] Error inserting ensemble score: %v", articleID, err)
                return fmt.Errorf("failed to insert ensemble score: %w", err)
        }</span>

        <span class="cov0" title="0">err = db.UpdateArticleScore(c.db, article.ID, finalScore, confidence)
        if err != nil </span><span class="cov0" title="0">{
                log.Printf("[ReanalyzeArticle %d] Error updating article score: %v", articleID, err)
                return fmt.Errorf("failed to update article score: %w", err)
        }</span>

        <span class="cov0" title="0">log.Printf("[ReanalyzeArticle %d] Successfully completed reanalysis with score: %.2f, confidence: %.2f",
                articleID, finalScore, confidence)
        return nil</span>
}

func (c *LLMClient) AnalyzeContent(articleID int64, content string, model string, url string) (*db.LLMScore, error) <span class="cov0" title="0">{
        return c.analyzeContent(articleID, content, model)
}</span>

func (c *LLMClient) GetArticle(articleID int64) (db.Article, error) <span class="cov0" title="0">{
        var article db.Article
        err := c.db.Get(&amp;article, "SELECT * FROM articles WHERE id = ?", articleID)
        return article, err
}</span>

func (c *LLMClient) DeleteScores(articleID int64) error <span class="cov0" title="0">{
        _, err := c.db.Exec("DELETE FROM llm_scores WHERE article_id = ?", articleID)
        return err
}</span>

func (c *LLMClient) FetchScores(articleID int64) ([]db.LLMScore, error) <span class="cov0" title="0">{
        return db.FetchLLMScores(c.db, articleID)
}</span>

// ScoreWithModel uses a single model to score content
func (c *LLMClient) ScoreWithModel(article *db.Article, modelName string) (float64, error) <span class="cov8" title="1">{
        // Create a prompt variant with the specified model
        promptVariant := DefaultPromptVariant
        promptVariant.Model = modelName

        // Use the LLM service directly to handle rate limiting properly
        score, _, err := c.llmService.ScoreContent(context.Background(), promptVariant, article)

        if err != nil </span><span class="cov8" title="1">{
                // Specifically check for rate limit errors first
                if errors.Is(err, ErrBothLLMKeysRateLimited) </span><span class="cov8" title="1">{
                        return 0, ErrBothLLMKeysRateLimited
                }</span>

                // Check for service unavailable
                <span class="cov8" title="1">if strings.Contains(strings.ToLower(err.Error()), "503") ||
                        strings.Contains(strings.ToLower(err.Error()), "service unavailable") </span><span class="cov8" title="1">{
                        return 0, apperrors.New("LLM service unavailable", "llm_service_error")
                }</span>

                // For any other errors, return a more descriptive error with llm_service_error code
                <span class="cov8" title="1">return 0, apperrors.Wrap(err, "llm_service_error", fmt.Sprintf("scoring with model %s failed", modelName))</span>
        }

        <span class="cov8" title="1">return score, nil</span>
}

func (c *LLMClient) StoreEnsembleScore(article *db.Article) (float64, error) <span class="cov0" title="0">{
        scores, err := c.FetchScores(article.ID)
        if err != nil </span><span class="cov0" title="0">{
                return 0.0, fmt.Errorf("failed to fetch scores for ensemble calculation: %w", err)
        }</span>

        <span class="cov0" title="0">finalScore, confidence, err := ComputeCompositeScoreWithConfidence(scores)
        if err != nil </span><span class="cov0" title="0">{
                return 0.0, fmt.Errorf("failed to compute composite score: %w", err)
        }</span>

        <span class="cov0" title="0">meta := map[string]interface{}{
                "timestamp": time.Now().Format(time.RFC3339),
                "final_aggregation": map[string]interface{}{
                        "weighted_mean": finalScore,
                        "variance":      1.0 - confidence,
                },
        }
        metaBytes, _ := json.Marshal(meta)

        ensembleScore := &amp;db.LLMScore{
                ArticleID: article.ID,
                Model:     "ensemble",
                Score:     finalScore,
                Metadata:  string(metaBytes),
                CreatedAt: time.Now(),
        }

        _, err = c.db.NamedExec(`INSERT INTO llm_scores (article_id, model, score, metadata, created_at)
                VALUES (:article_id, :model, :score, :metadata, :created_at)`, ensembleScore)
        if err != nil </span><span class="cov0" title="0">{
                return finalScore, fmt.Errorf("failed to insert ensemble score: %w", err)
        }</span>

        <span class="cov0" title="0">updateErr := db.UpdateArticleScore(c.db, article.ID, finalScore, confidence)
        if updateErr != nil </span><span class="cov0" title="0">{
                return finalScore, fmt.Errorf("failed to update article score: %w", updateErr)
        }</span>

        <span class="cov0" title="0">return finalScore, nil</span>
}

func hashContent(content string) string <span class="cov8" title="1">{
        hash := sha256.Sum256([]byte(content))
        return fmt.Sprintf("%x", hash)
}</span>

func min(a, b int) int <span class="cov8" title="1">{
        if a &lt; b </span><span class="cov8" title="1">{
                return a
        }</span>
        <span class="cov8" title="1">return b</span>
}
</pre>
		
		<pre class="file" id="file5" style="display: none">package llm

import (
        "sync"
        "time"

        "github.com/alexandru-savinov/BalancedNewsGo/internal/models"
)

// ProgressManager tracks scoring progress with cleanup
type ProgressManager struct {
        progressMap     map[int64]*models.ProgressState
        progressMapLock sync.RWMutex
        cleanupInterval time.Duration
}

// NewProgressManager creates a progress manager with cleanup
func NewProgressManager(cleanupInterval time.Duration) *ProgressManager <span class="cov8" title="1">{
        pm := &amp;ProgressManager{
                progressMap:     make(map[int64]*models.ProgressState),
                cleanupInterval: cleanupInterval,
        }
        go pm.startCleanupRoutine()
        return pm
}</span>

// SetProgress sets the progress state for an article
func (pm *ProgressManager) SetProgress(articleID int64, state *models.ProgressState) <span class="cov8" title="1">{
        pm.progressMapLock.Lock()
        defer pm.progressMapLock.Unlock()
        pm.progressMap[articleID] = state
}</span>

// GetProgress retrieves the progress state for an article
func (pm *ProgressManager) GetProgress(articleID int64) *models.ProgressState <span class="cov8" title="1">{
        pm.progressMapLock.RLock()
        defer pm.progressMapLock.RUnlock()
        return pm.progressMap[articleID]
}</span>

// startCleanupRoutine periodically removes stale entries
func (pm *ProgressManager) startCleanupRoutine() <span class="cov8" title="1">{
        ticker := time.NewTicker(pm.cleanupInterval)
        defer ticker.Stop()
        for range ticker.C </span><span class="cov0" title="0">{
                pm.cleanup()
        }</span>
}

// cleanup removes completed or stale progress entries
func (pm *ProgressManager) cleanup() <span class="cov8" title="1">{
        pm.progressMapLock.Lock()
        defer pm.progressMapLock.Unlock()
        now := time.Now().Unix()
        for id, progress := range pm.progressMap </span><span class="cov8" title="1">{
                if (progress.Status == "Success" || progress.Status == "Error") &amp;&amp; now-progress.LastUpdated &gt; 300 </span><span class="cov8" title="1">{
                        delete(pm.progressMap, id)
                        continue</span>
                }
                <span class="cov8" title="1">if progress.Status == "InProgress" &amp;&amp; now-progress.LastUpdated &gt; 1800 </span><span class="cov8" title="1">{
                        delete(pm.progressMap, id)
                }</span>
        }
}
</pre>
		
		<pre class="file" id="file6" style="display: none">package llm

import (
        "encoding/json"
        "fmt"
        "strings"

        "github.com/alexandru-savinov/BalancedNewsGo/internal/db"
)

var perspectives = []string{"left", "center", "right"}

// ScoreCalculator defines the interface for composite score calculation
// Returns (score, confidence, error)
type ScoreCalculator interface {
        CalculateScore(scores []db.LLMScore) (float64, float64, error)
}

// DefaultScoreCalculator implements ScoreCalculator using the new averaging logic
// It preserves the -1.0 to +1.0 scale and averages confidences from model metadata
// Missing perspectives are treated as 0 for both score and confidence
type DefaultScoreCalculator struct {
        Config *CompositeScoreConfig // Must be provided, not nil
}

// initializeMaps creates and initializes maps for scores and confidence values
func (c *DefaultScoreCalculator) initializeMaps() (map[string]*float64, map[string]*float64) <span class="cov8" title="1">{
        scoreMap := make(map[string]*float64)
        confMap := make(map[string]*float64)
        for _, p := range perspectives </span><span class="cov8" title="1">{
                scoreMap[p] = nil
                confMap[p] = nil
        }</span>
        <span class="cov8" title="1">return scoreMap, confMap</span>
}

// getPerspective determines the perspective (left/center/right) for a given model
func (c *DefaultScoreCalculator) getPerspective(model string) string <span class="cov8" title="1">{
        perspective := MapModelToPerspective(model, c.Config)
        if perspective != "" </span><span class="cov8" title="1">{
                return perspective
        }</span>

        <span class="cov8" title="1">model = strings.ToLower(model)
        switch model </span>{
        case LabelLeft:<span class="cov0" title="0">
                return "left"</span>
        case LabelRight:<span class="cov0" title="0">
                return "right"</span>
        case "center":<span class="cov0" title="0">
                return "center"</span>
        default:<span class="cov8" title="1">
                return ""</span>
        }
}

// extractConfidence extracts confidence value from score metadata
func (c *DefaultScoreCalculator) extractConfidence(metadata string) float64 <span class="cov8" title="1">{
        var meta map[string]interface{}
        if err := json.Unmarshal([]byte(metadata), &amp;meta); err != nil </span><span class="cov8" title="1">{
                return 0.0
        }</span>
        <span class="cov8" title="1">if conf, ok := meta["confidence"]; ok </span><span class="cov8" title="1">{
                // Handle both float64 and integer confidence values
                switch v := conf.(type) </span>{
                case float64:<span class="cov8" title="1">
                        return v</span>
                case int:<span class="cov0" title="0">
                        return float64(v)</span>
                case int64:<span class="cov0" title="0">
                        return float64(v)</span>
                default:<span class="cov8" title="1">
                        return 0.0</span>
                }
        }
        <span class="cov8" title="1">return 0.0</span>
}

func (c *DefaultScoreCalculator) CalculateScore(scores []db.LLMScore) (float64, float64, error) <span class="cov8" title="1">{
        if c.Config == nil </span><span class="cov8" title="1">{
                return 0, 0, fmt.Errorf("DefaultScoreCalculator: Config must not be nil")
        }</span>

        <span class="cov8" title="1">scoreMap, confMap := c.initializeMaps()

        // For each perspective, use the last provided score (and its confidence)
        for _, s := range scores </span><span class="cov8" title="1">{
                perspective := c.getPerspective(s.Model)
                if perspective == "" || (perspective != "left" &amp;&amp; perspective != "center" &amp;&amp; perspective != "right") </span><span class="cov8" title="1">{
                        continue</span>
                }

                <span class="cov8" title="1">val := s.Score
                if isInvalid(val) || val &lt; c.Config.MinScore || val &gt; c.Config.MaxScore </span><span class="cov8" title="1">{
                        // Set out of range scores to 0.0 per test expectations
                        val = 0.0
                }</span>
                <span class="cov8" title="1">scoreMap[perspective] = &amp;val

                conf := c.extractConfidence(s.Metadata)
                confMap[perspective] = &amp;conf</span>
        }

        // Calculate average score and confidence
        <span class="cov8" title="1">validScores := 0
        validConfs := 0
        scoreSum := 0.0
        confSum := 0.0

        for _, p := range perspectives </span><span class="cov8" title="1">{
                if scoreMap[p] != nil </span><span class="cov8" title="1">{
                        scoreSum += *scoreMap[p]
                        validScores++
                }</span>

                <span class="cov8" title="1">if confMap[p] != nil </span><span class="cov8" title="1">{
                        confSum += *confMap[p]
                        validConfs++
                }</span>
        }

        // Calculate averages based on valid values
        <span class="cov8" title="1">var avgScore float64
        var avgConf float64

        if validScores &gt; 0 </span><span class="cov8" title="1">{
                avgScore = scoreSum / float64(validScores)
        }</span>

        <span class="cov8" title="1">if validConfs &gt; 0 </span><span class="cov8" title="1">{
                avgConf = confSum / float64(validConfs)
        }</span>

        <span class="cov8" title="1">return avgScore, avgConf, nil</span>
}
</pre>
		
		<pre class="file" id="file7" style="display: none">package llm

import (
        "context"
        "encoding/json"
        "fmt"
        "time"

        "github.com/alexandru-savinov/BalancedNewsGo/internal/db"
        "github.com/alexandru-savinov/BalancedNewsGo/internal/models"
        "github.com/jmoiron/sqlx"
)

// ScoreManager orchestrates score operations and dependencies
// This is a skeleton for the refactor, to be filled in with logic in later steps
type ScoreManager struct {
        db          *sqlx.DB
        cache       *Cache
        calculator  ScoreCalculator
        progressMgr *ProgressManager
}

// NewScoreManager creates a new score manager with dependencies
func NewScoreManager(db *sqlx.DB, cache *Cache, calculator ScoreCalculator, progressMgr *ProgressManager) *ScoreManager <span class="cov8" title="1">{
        return &amp;ScoreManager{
                db:          db,
                cache:       cache,
                calculator:  calculator,
                progressMgr: progressMgr,
        }
}</span>

// UpdateArticleScore handles atomic update of score and confidence
func (sm *ScoreManager) UpdateArticleScore(articleID int64, scores []db.LLMScore, cfg *CompositeScoreConfig) (float64, float64, error) <span class="cov8" title="1">{
        if sm.calculator == nil </span><span class="cov0" title="0">{
                return 0, 0, fmt.Errorf("ScoreManager: calculator is nil")
        }</span>
        <span class="cov8" title="1">if sm.db == nil </span><span class="cov0" title="0">{
                return 0, 0, fmt.Errorf("ScoreManager: db is nil")
        }</span>
        <span class="cov8" title="1">if cfg == nil </span><span class="cov0" title="0">{
                return 0, 0, fmt.Errorf("ScoreManager: config is nil")
        }</span>

        // Progress: Start
        <span class="cov8" title="1">if sm.progressMgr != nil </span><span class="cov8" title="1">{
                ps := &amp;models.ProgressState{Step: "Start", Message: "Starting scoring", Percent: 0, Status: "InProgress", LastUpdated: time.Now().Unix()}
                sm.progressMgr.SetProgress(articleID, ps)
        }</span>

        <span class="cov8" title="1">tx, err := sm.db.BeginTxx(context.Background(), nil)
        if err != nil </span><span class="cov0" title="0">{
                if sm.progressMgr != nil </span><span class="cov0" title="0">{
                        ps := &amp;models.ProgressState{Step: "DB Transaction", Message: "Failed to start DB transaction", Percent: 0, Status: "Error", Error: err.Error(), LastUpdated: time.Now().Unix()}
                        sm.progressMgr.SetProgress(articleID, ps)
                }</span>
                <span class="cov0" title="0">return 0, 0, fmt.Errorf("failed to begin transaction: %w", err)</span>
        }
        <span class="cov8" title="1">defer func() </span><span class="cov8" title="1">{
                if p := recover(); p != nil </span><span class="cov0" title="0">{
                        tx.Rollback()
                }</span>
        }()

        // Progress: Calculating
        <span class="cov8" title="1">if sm.progressMgr != nil </span><span class="cov8" title="1">{
                ps := &amp;models.ProgressState{Step: "Calculating", Message: "Calculating score", Percent: 20, Status: "InProgress", LastUpdated: time.Now().Unix()}
                sm.progressMgr.SetProgress(articleID, ps)
        }</span>

        <span class="cov8" title="1">score, confidence, calcErr := sm.calculator.CalculateScore(scores)
        if calcErr != nil </span><span class="cov8" title="1">{
                tx.Rollback()
                if sm.progressMgr != nil </span><span class="cov8" title="1">{
                        ps := &amp;models.ProgressState{Step: "Calculation", Message: "Score calculation failed", Percent: 20, Status: "Error", Error: calcErr.Error(), LastUpdated: time.Now().Unix()}
                        sm.progressMgr.SetProgress(articleID, ps)
                }</span>
                <span class="cov8" title="1">return 0, 0, fmt.Errorf("score calculation failed: %w", calcErr)</span>
        }

        // Progress: Storing ensemble score
        <span class="cov8" title="1">if sm.progressMgr != nil </span><span class="cov8" title="1">{
                ps := &amp;models.ProgressState{Step: "Storing", Message: "Storing ensemble score", Percent: 60, Status: "InProgress", LastUpdated: time.Now().Unix()}
                sm.progressMgr.SetProgress(articleID, ps)
        }</span>

        <span class="cov8" title="1">meta := map[string]interface{}{
                "timestamp":   time.Now().Format(time.RFC3339),
                "aggregation": "ensemble",
                "confidence":  confidence,
        }
        metaBytes, _ := json.Marshal(meta)
        ensembleScore := &amp;db.LLMScore{
                ArticleID: articleID,
                Model:     "ensemble",
                Score:     score,
                Metadata:  string(metaBytes),
                CreatedAt: time.Now(),
        }
        _, err = db.InsertLLMScore(tx, ensembleScore)
        if err != nil </span><span class="cov0" title="0">{
                tx.Rollback()
                if sm.progressMgr != nil </span><span class="cov0" title="0">{
                        ps := &amp;models.ProgressState{Step: "DB Insert", Message: "Failed to insert ensemble score", Percent: 70, Status: "Error", Error: err.Error(), LastUpdated: time.Now().Unix()}
                        sm.progressMgr.SetProgress(articleID, ps)
                }</span>
                <span class="cov0" title="0">return 0, 0, fmt.Errorf("failed to insert ensemble score: %w", err)</span>
        }

        // Progress: Updating article
        <span class="cov8" title="1">if sm.progressMgr != nil </span><span class="cov8" title="1">{
                ps := &amp;models.ProgressState{Step: "Updating", Message: "Updating article score", Percent: 80, Status: "InProgress", LastUpdated: time.Now().Unix()}
                sm.progressMgr.SetProgress(articleID, ps)
        }</span>

        <span class="cov8" title="1">err = db.UpdateArticleScoreLLM(tx, articleID, score, confidence)
        if err != nil </span><span class="cov0" title="0">{
                tx.Rollback()
                if sm.progressMgr != nil </span><span class="cov0" title="0">{
                        ps := &amp;models.ProgressState{Step: "DB Update", Message: "Failed to update article", Percent: 90, Status: "Error", Error: err.Error(), LastUpdated: time.Now().Unix()}
                        sm.progressMgr.SetProgress(articleID, ps)
                }</span>
                <span class="cov0" title="0">return 0, 0, fmt.Errorf("failed to update article: %w", err)</span>
        }

        <span class="cov8" title="1">if err := tx.Commit(); err != nil </span><span class="cov0" title="0">{
                if sm.progressMgr != nil </span><span class="cov0" title="0">{
                        ps := &amp;models.ProgressState{Step: "DB Commit", Message: "Failed to commit transaction", Percent: 95, Status: "Error", Error: err.Error(), LastUpdated: time.Now().Unix()}
                        sm.progressMgr.SetProgress(articleID, ps)
                }</span>
                <span class="cov0" title="0">return 0, 0, fmt.Errorf("failed to commit transaction: %w", err)</span>
        }

        // Progress: Invalidate cache
        <span class="cov8" title="1">if sm.cache != nil </span><span class="cov8" title="1">{
                sm.cache.Delete(fmt.Sprintf("article:%d", articleID))
                sm.cache.Delete(fmt.Sprintf("ensemble:%d", articleID))
                sm.cache.Delete(fmt.Sprintf("bias:%d", articleID))
        }</span>

        // Progress: Success
        <span class="cov8" title="1">if sm.progressMgr != nil </span><span class="cov8" title="1">{
                ps := &amp;models.ProgressState{Step: "Complete", Message: "Scoring complete", Percent: 100, Status: "Success", FinalScore: &amp;score, LastUpdated: time.Now().Unix()}
                sm.progressMgr.SetProgress(articleID, ps)
        }</span>

        <span class="cov8" title="1">return score, confidence, nil</span>
}

// InvalidateScoreCache invalidates all score-related caches for an article
func (sm *ScoreManager) InvalidateScoreCache(articleID int64) <span class="cov8" title="1">{
        if sm.cache == nil </span><span class="cov0" title="0">{
                return
        }</span>
        // Invalidate all relevant cache keys (matching API cache usage)
        <span class="cov8" title="1">keys := []string{
                fmt.Sprintf("article:%d", articleID),
                fmt.Sprintf("ensemble:%d", articleID),
                fmt.Sprintf("bias:%d", articleID),
        }
        for _, key := range keys </span><span class="cov8" title="1">{
                sm.cache.Delete(key)
        }</span>
}

// TrackProgress registers progress tracking for an article
func (sm *ScoreManager) TrackProgress(articleID int64, step, status string) <span class="cov8" title="1">{
        if sm.progressMgr != nil </span><span class="cov8" title="1">{
                // Create an initial progress state with parameters
                initialState := &amp;models.ProgressState{
                        Step:        step,
                        Message:     fmt.Sprintf("Progress update: %s", step),
                        Percent:     0,
                        Status:      status,
                        LastUpdated: time.Now().Unix(),
                }
                sm.progressMgr.SetProgress(articleID, initialState)
        }</span>
}

// SetProgress proxies to ProgressManager
func (sm *ScoreManager) SetProgress(articleID int64, state *models.ProgressState) <span class="cov8" title="1">{
        if sm.progressMgr != nil </span><span class="cov8" title="1">{
                sm.progressMgr.SetProgress(articleID, state)
        }</span>
}

// GetProgress proxies to ProgressManager
func (sm *ScoreManager) GetProgress(articleID int64) *models.ProgressState <span class="cov8" title="1">{
        if sm.progressMgr != nil </span><span class="cov8" title="1">{
                return sm.progressMgr.GetProgress(articleID)
        }</span>
        <span class="cov8" title="1">return nil</span>
}
</pre>
		
		<pre class="file" id="file8" style="display: none">package llm

import (
        "context"
        "fmt"
        "strings"

        "github.com/alexandru-savinov/BalancedNewsGo/internal/db"
        "github.com/go-resty/resty/v2"
)

// LLMService defines the interface for LLM analysis providers
type LLMService interface {
        ScoreContent(ctx context.Context, pv PromptVariant, art *db.Article) (score float64, confidence float64, err error)
}

// HTTPLLMService implements LLMService using HTTP calls
type HTTPLLMService struct {
        client    *resty.Client
        apiKey    string
        backupKey string
        baseURL   string
}

// NewHTTPLLMService creates a new HTTP-based LLM service
func NewHTTPLLMService(c *resty.Client, primaryKey string, backupKey string, baseURL string) *HTTPLLMService <span class="cov8" title="1">{
        if baseURL == "" </span><span class="cov8" title="1">{
                baseURL = "https://openrouter.ai/api/v1"
        }</span>
        // Ensure baseURL ends with /chat/completions
        <span class="cov8" title="1">if !strings.HasSuffix(baseURL, "/chat/completions") </span><span class="cov8" title="1">{
                if strings.HasSuffix(baseURL, "/") </span><span class="cov0" title="0">{
                        baseURL = baseURL + "chat/completions"
                }</span> else<span class="cov8" title="1"> {
                        baseURL = baseURL + "/chat/completions"
                }</span>
        }
        <span class="cov8" title="1">return &amp;HTTPLLMService{
                client:    c,
                apiKey:    primaryKey,
                backupKey: backupKey,
                baseURL:   baseURL,
        }</span>
}

// callLLMAPIWithKey makes a direct API call to the LLM service
func (s *HTTPLLMService) callLLMAPIWithKey(modelName string, prompt string, apiKey string) (*resty.Response, error) <span class="cov8" title="1">{
        return s.client.R().
                SetAuthToken(apiKey).
                SetHeader("Content-Type", "application/json").
                SetHeader("HTTP-Referer", "https://github.com/alexandru-savinov/BalancedNewsGo").
                SetHeader("X-Title", "NewsBalancer").
                SetBody(map[string]interface{}{
                        "model": modelName,
                        "messages": []map[string]string{
                                {"role": "user", "content": prompt},
                        },
                }).
                Post(s.baseURL)
}</span>

// ScoreContent implements LLMService by making HTTP requests to score content
func (s *HTTPLLMService) ScoreContent(ctx context.Context, pv PromptVariant, art *db.Article) (score float64, confidence float64, err error) <span class="cov8" title="1">{
        // Try primary key first
        resp, err := s.callLLMAPIWithKey(pv.Model, pv.FormatPrompt(art.Content), s.apiKey)

        // Handle rate limiting and try backup key if available
        if (err != nil &amp;&amp; strings.Contains(err.Error(), "rate limit")) || (resp != nil &amp;&amp; resp.StatusCode() == 429) </span><span class="cov8" title="1">{
                if s.backupKey != "" </span><span class="cov8" title="1">{
                        // Try backup key if rate limited and backup key exists
                        resp, err = s.callLLMAPIWithKey(pv.Model, pv.FormatPrompt(art.Content), s.backupKey)
                        if (err != nil &amp;&amp; strings.Contains(err.Error(), "rate limit")) || (resp != nil &amp;&amp; resp.StatusCode() == 429) </span><span class="cov8" title="1">{
                                // Both keys are rate limited
                                return 0, 0, fmt.Errorf("rate limit exceeded on both keys: %w", ErrBothLLMKeysRateLimited)
                        }</span>
                } else<span class="cov0" title="0"> {
                        // No backup key, propagate the original error
                        return 0, 0, fmt.Errorf("rate limit exceeded on primary key and no backup key provided")
                }</span>
        }

        <span class="cov8" title="1">if err != nil </span><span class="cov8" title="1">{
                return 0, 0, err
        }</span>

        // Check for non-success status codes
        <span class="cov8" title="1">if resp.StatusCode() &gt;= 400 </span><span class="cov8" title="1">{
                return 0, 0, formatHTTPError(resp)
        }</span>

        // Parse the response
        <span class="cov8" title="1">score, _, confidence, err = parseNestedLLMJSONResponse(resp.String())
        return score, confidence, err</span>
}

// formatHTTPError formats a helpful error message from HTTP responses
func formatHTTPError(resp *resty.Response) error <span class="cov8" title="1">{
        return LLMAPIError{
                Message:      "HTTP error from LLM API",
                StatusCode:   resp.StatusCode(),
                ResponseBody: resp.String(),
        }
}</span>

// LLMAPIError represents an error from the LLM API service
type LLMAPIError struct {
        Message      string
        StatusCode   int
        ResponseBody string
}

// Error implements the error interface for LLMAPIError
func (e LLMAPIError) Error() string <span class="cov8" title="1">{
        return fmt.Sprintf("LLM API Error (status %d): %s", e.StatusCode, e.Message)
}</span>
</pre>
		
		</div>
	</body>
	<script>
	(function() {
		var files = document.getElementById('files');
		var visible;
		files.addEventListener('change', onChange, false);
		function select(part) {
			if (visible)
				visible.style.display = 'none';
			visible = document.getElementById(part);
			if (!visible)
				return;
			files.value = part;
			visible.style.display = 'block';
			location.hash = part;
		}
		function onChange() {
			select(files.value);
			window.scrollTo(0, 0);
		}
		if (location.hash != "") {
			select(location.hash.substr(1));
		}
		if (!visible) {
			select("file0");
		}
	})();
	</script>
</html>
