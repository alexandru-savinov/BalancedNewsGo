=== RUN   TestNewCache
--- PASS: TestNewCache (0.00s)
=== RUN   TestMakeKey
=== RUN   TestMakeKey/Simple_key
=== RUN   TestMakeKey/Empty_hash
=== RUN   TestMakeKey/Empty_model
=== RUN   TestMakeKey/Both_empty
--- PASS: TestMakeKey (0.00s)
    --- PASS: TestMakeKey/Simple_key (0.00s)
    --- PASS: TestMakeKey/Empty_hash (0.00s)
    --- PASS: TestMakeKey/Empty_model (0.00s)
    --- PASS: TestMakeKey/Both_empty (0.00s)
=== RUN   TestCacheGetSet
--- PASS: TestCacheGetSet (0.00s)
=== RUN   TestCacheDelete
--- PASS: TestCacheDelete (0.00s)
=== RUN   TestCacheRemove
--- PASS: TestCacheRemove (0.00s)
=== RUN   TestIsInvalid
=== RUN   TestIsInvalid/NaN_value
=== RUN   TestIsInvalid/positive_infinity
=== RUN   TestIsInvalid/negative_infinity
=== RUN   TestIsInvalid/below_min_score
=== RUN   TestIsInvalid/above_max_score
=== RUN   TestIsInvalid/valid_value
=== RUN   TestIsInvalid/zero_value
=== RUN   TestIsInvalid/negative_valid_value
=== RUN   TestIsInvalid/min_boundary
=== RUN   TestIsInvalid/max_boundary
--- PASS: TestIsInvalid (0.00s)
    --- PASS: TestIsInvalid/NaN_value (0.00s)
    --- PASS: TestIsInvalid/positive_infinity (0.00s)
    --- PASS: TestIsInvalid/negative_infinity (0.00s)
    --- PASS: TestIsInvalid/below_min_score (0.00s)
    --- PASS: TestIsInvalid/above_max_score (0.00s)
    --- PASS: TestIsInvalid/valid_value (0.00s)
    --- PASS: TestIsInvalid/zero_value (0.00s)
    --- PASS: TestIsInvalid/negative_valid_value (0.00s)
    --- PASS: TestIsInvalid/min_boundary (0.00s)
    --- PASS: TestIsInvalid/max_boundary (0.00s)
=== RUN   TestMapModelsToPerspectives
=== RUN   TestMapModelsToPerspectives/map_models_to_perspectives
2025/05/30 14:48:38 Mapping model 'neutral' (score 0.00) to perspective 'center'
2025/05/30 14:48:38 Mapping model 'progressive' (score 0.70) to perspective 'left'
2025/05/30 14:48:38 Mapping model 'conservative' (score -0.50) to perspective 'right'
2025/05/30 14:48:38 Perspective center has 1 models
2025/05/30 14:48:38 Perspective left has 1 models
2025/05/30 14:48:38 Perspective right has 1 models
=== RUN   TestMapModelsToPerspectives/ensemble_model_maps_to_center
2025/05/30 14:48:38 Mapping ensemble model (score 0.30) to perspective 'center'
2025/05/30 14:48:38 Mapping model 'progressive' (score 0.70) to perspective 'left'
2025/05/30 14:48:38 Perspective center has 1 models
2025/05/30 14:48:38 Perspective left has 1 models
=== RUN   TestMapModelsToPerspectives/unknown_model_is_skipped
2025/05/30 14:48:38 Warning: Model 'unknown-model' not found in composite score configuration
2025/05/30 14:48:38 Skipping unknown model: unknown-model
2025/05/30 14:48:38 Mapping model 'progressive' (score 0.70) to perspective 'left'
2025/05/30 14:48:38 Perspective left has 1 models
--- PASS: TestMapModelsToPerspectives (0.00s)
    --- PASS: TestMapModelsToPerspectives/map_models_to_perspectives (0.00s)
    --- PASS: TestMapModelsToPerspectives/ensemble_model_maps_to_center (0.00s)
    --- PASS: TestMapModelsToPerspectives/unknown_model_is_skipped (0.00s)
=== RUN   TestCalculateConfidence
=== RUN   TestCalculateConfidence/count_valid_method_with_all_perspectives
=== RUN   TestCalculateConfidence/count_valid_method_with_missing_perspectives
=== RUN   TestCalculateConfidence/default_method_when_unspecified
=== RUN   TestCalculateConfidence/confidence_respects_min_and_max_bounds
--- PASS: TestCalculateConfidence (0.00s)
    --- PASS: TestCalculateConfidence/count_valid_method_with_all_perspectives (0.00s)
    --- PASS: TestCalculateConfidence/count_valid_method_with_missing_perspectives (0.00s)
    --- PASS: TestCalculateConfidence/default_method_when_unspecified (0.00s)
    --- PASS: TestCalculateConfidence/confidence_respects_min_and_max_bounds (0.00s)
=== RUN   TestComputeCompositeScoreWithAllZeroResponses
=== RUN   TestComputeCompositeScoreWithAllZeroResponses/All_models_return_invalid_scores
2025/05/30 14:48:38 Critical warning: All 3 non-ensemble LLM models returned zero confidence
2025/05/30 14:48:38 [ERROR][CONFIDENCE] All scores are zero, returning error
=== RUN   TestComputeCompositeScoreWithAllZeroResponses/All_models_return_zero_confidence
2025/05/30 14:48:38 Critical warning: All 3 non-ensemble LLM models returned zero confidence
2025/05/30 14:48:38 [ERROR][CONFIDENCE] All scores are zero, returning error
=== RUN   TestComputeCompositeScoreWithAllZeroResponses/Only_ensemble_score_with_valid_confidence
2025/05/30 14:48:38 Mapping ensemble model (score 0.70) to perspective 'center'
2025/05/30 14:48:38 Perspective center has 1 models
2025/05/30 14:48:38 Candidates for center: 
2025/05/30 14:48:38   Model: ensemble, Score: 0.70, Metadata: {"all_sub_results":[{"model":"model1","score":0.1,"confidence":0.8},{"model":"model2","score":-0.1,"confidence":0.7}],"confidence":0.9,"final_aggregation":{"weighted_mean":0,"variance":0.1,"uncertainty_flag":false},"per_model_results":{},"per_model_aggregation":{},"timestamp":"2024-04-28T12:00:00Z"}
2025/05/30 14:48:38 [DEBUG] Pre-Sum: validCount=1, len(validModels)=1
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Score map: map[center:0.7 left:0 right:0]
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Valid models map: map[center:true]
2025/05/30 14:48:38 [DEBUG] Pre-Calc: sum=0.7000, weightedSum=0.7000, weightTotal=1.0000, actualValidCount=1
=== RUN   TestComputeCompositeScoreWithAllZeroResponses/Only_ensemble_score_with_zero_confidence
2025/05/30 14:48:38 Mapping ensemble model (score 0.00) to perspective 'center'
2025/05/30 14:48:38 Perspective center has 1 models
2025/05/30 14:48:38 Candidates for center: 
2025/05/30 14:48:38   Model: ensemble, Score: 0.00, Metadata: {"all_sub_results":[{"model":"model1","score":0.1,"confidence":0.8},{"model":"model2","score":-0.1,"confidence":0.7}],"confidence":0,"final_aggregation":{"weighted_mean":0,"variance":1.0,"uncertainty_flag":true},"per_model_results":{},"per_model_aggregation":{},"timestamp":"2024-04-28T12:00:00Z"}
2025/05/30 14:48:38 [DEBUG] Pre-Sum: validCount=1, len(validModels)=1
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Score map: map[center:0 left:0 right:0]
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Valid models map: map[center:true]
2025/05/30 14:48:38 [DEBUG] Pre-Calc: sum=0.0000, weightedSum=0.0000, weightTotal=1.0000, actualValidCount=1
2025/05/30 14:48:38 [ERROR] Error in calculateCompositeScore: all perspectives returned invalid scores. actualValidCount=1
--- PASS: TestComputeCompositeScoreWithAllZeroResponses (0.00s)
    --- PASS: TestComputeCompositeScoreWithAllZeroResponses/All_models_return_invalid_scores (0.00s)
    --- PASS: TestComputeCompositeScoreWithAllZeroResponses/All_models_return_zero_confidence (0.00s)
    --- PASS: TestComputeCompositeScoreWithAllZeroResponses/Only_ensemble_score_with_valid_confidence (0.00s)
    --- PASS: TestComputeCompositeScoreWithAllZeroResponses/Only_ensemble_score_with_zero_confidence (0.00s)
=== RUN   TestComputeCompositeScoreWithConfidence
2025/05/30 14:48:38 Mapping model 'left' (score -1.00) to perspective 'left'
2025/05/30 14:48:38 Mapping model 'center' (score 0.00) to perspective 'center'
2025/05/30 14:48:38 Mapping model 'right' (score 1.00) to perspective 'right'
2025/05/30 14:48:38 Perspective left has 1 models
2025/05/30 14:48:38 Perspective center has 1 models
2025/05/30 14:48:38 Perspective right has 1 models
2025/05/30 14:48:38 Candidates for left: 
2025/05/30 14:48:38   Model: left, Score: -1.00, Metadata: {"confidence":0.9}
2025/05/30 14:48:38 Candidates for center: 
2025/05/30 14:48:38   Model: center, Score: 0.00, Metadata: {"confidence":0.8}
2025/05/30 14:48:38 Candidates for right: 
2025/05/30 14:48:38   Model: right, Score: 1.00, Metadata: {"confidence":0.85}
2025/05/30 14:48:38 [DEBUG] Pre-Sum: validCount=3, len(validModels)=3
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Score map: map[center:0 left:-1 right:1]
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Valid models map: map[center:true left:true right:true]
2025/05/30 14:48:38 [DEBUG] Pre-Calc: sum=0.0000, weightedSum=0.0000, weightTotal=3.0000, actualValidCount=3
--- PASS: TestComputeCompositeScoreWithConfidence (0.00s)
=== RUN   TestComputeCompositeScoreEdgeCases
2025/05/30 14:48:38 Attempting to load composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
2025/05/30 14:48:38 Successfully loaded and parsed composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
=== RUN   TestComputeCompositeScoreEdgeCases/empty_scores_array
=== PAUSE TestComputeCompositeScoreEdgeCases/empty_scores_array
=== RUN   TestComputeCompositeScoreEdgeCases/values_outside_bounds
=== PAUSE TestComputeCompositeScoreEdgeCases/values_outside_bounds
=== RUN   TestComputeCompositeScoreEdgeCases/all_zero_confidence
=== PAUSE TestComputeCompositeScoreEdgeCases/all_zero_confidence
=== RUN   TestComputeCompositeScoreEdgeCases/extreme_values_within_bounds
=== PAUSE TestComputeCompositeScoreEdgeCases/extreme_values_within_bounds
=== RUN   TestComputeCompositeScoreEdgeCases/non-standard_model_names
=== PAUSE TestComputeCompositeScoreEdgeCases/non-standard_model_names
=== RUN   TestComputeCompositeScoreEdgeCases/case_insensitive_model_names
=== PAUSE TestComputeCompositeScoreEdgeCases/case_insensitive_model_names
=== RUN   TestComputeCompositeScoreEdgeCases/NaN_values_-_ignored
=== PAUSE TestComputeCompositeScoreEdgeCases/NaN_values_-_ignored
=== RUN   TestComputeCompositeScoreEdgeCases/All_Infinity_values_-_ignored
=== PAUSE TestComputeCompositeScoreEdgeCases/All_Infinity_values_-_ignored
=== RUN   TestComputeCompositeScoreEdgeCases/Out-of-range_ignored_with_one_valid_score
=== PAUSE TestComputeCompositeScoreEdgeCases/Out-of-range_ignored_with_one_valid_score
=== RUN   TestComputeCompositeScoreEdgeCases/weighted_formula_with_config_override
=== PAUSE TestComputeCompositeScoreEdgeCases/weighted_formula_with_config_override
=== RUN   TestComputeCompositeScoreEdgeCases/ignore_invalid_with_config_override
=== PAUSE TestComputeCompositeScoreEdgeCases/ignore_invalid_with_config_override
=== RUN   TestComputeCompositeScoreEdgeCases/duplicate_model_scores_-_should_use_last_one
=== PAUSE TestComputeCompositeScoreEdgeCases/duplicate_model_scores_-_should_use_last_one
=== RUN   TestComputeCompositeScoreEdgeCases/custom_min/max_bounds
=== PAUSE TestComputeCompositeScoreEdgeCases/custom_min/max_bounds
=== CONT  TestComputeCompositeScoreEdgeCases/empty_scores_array
=== CONT  TestComputeCompositeScoreEdgeCases/ignore_invalid_with_config_override
=== CONT  TestComputeCompositeScoreEdgeCases/weighted_formula_with_config_override
2025/05/30 14:48:38 Mapping model 'left' (score 0.10) to perspective 'left'
2025/05/30 14:48:38 Mapping model 'center' (score 0.20) to perspective 'center'
2025/05/30 14:48:38 Mapping model 'right' (score 0.30) to perspective 'right'
2025/05/30 14:48:38 Perspective left has 1 models
2025/05/30 14:48:38 Perspective center has 1 models
2025/05/30 14:48:38 Perspective right has 1 models
2025/05/30 14:48:38 Candidates for center: 
2025/05/30 14:48:38   Model: center, Score: 0.20, Metadata: {"confidence":0.80}
2025/05/30 14:48:38 Candidates for right: 
=== CONT  TestComputeCompositeScoreEdgeCases/duplicate_model_scores_-_should_use_last_one
2025/05/30 14:48:38   Model: right, Score: 0.30, Metadata: {"confidence":0.85}
2025/05/30 14:48:38 Mapping model 'left' (score NaN) to perspective 'left'
2025/05/30 14:48:38 Candidates for left: 
2025/05/30 14:48:38   Model: left, Score: 0.10, Metadata: {"confidence":0.90}
2025/05/30 14:48:38 Mapping model 'left' (score 0.10) to perspective 'left'
2025/05/30 14:48:38 [DEBUG] Pre-Sum: validCount=3, len(validModels)=3
2025/05/30 14:48:38 Mapping model 'center' (score 0.20) to perspective 'center'
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Score map: map[center:0.2 left:0.1 right:0.3]
2025/05/30 14:48:38 Mapping model 'right' (score 0.30) to perspective 'right'
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Valid models map: map[center:true left:true right:true]
2025/05/30 14:48:38 Mapping model 'left' (score 0.40) to perspective 'left'
2025/05/30 14:48:38 [DEBUG] Pre-Calc: sum=0.6000, weightedSum=1.4000, weightTotal=6.0000, actualValidCount=3
2025/05/30 14:48:38 Mapping model 'center' (score 0.50) to perspective 'center'
2025/05/30 14:48:38 Mapping model 'right' (score 0.60) to perspective 'right'
2025/05/30 14:48:38 Perspective left has 2 models
2025/05/30 14:48:38 Perspective center has 2 models
2025/05/30 14:48:38 Perspective right has 2 models
2025/05/30 14:48:38 Candidates for left: 
2025/05/30 14:48:38   Model: left, Score: 0.10, Metadata: {"confidence":0.90}
2025/05/30 14:48:38   Model: left, Score: 0.40, Metadata: {"confidence":0.92}
2025/05/30 14:48:38 Candidates for center: 
2025/05/30 14:48:38   Model: center, Score: 0.20, Metadata: {"confidence":0.80}
2025/05/30 14:48:38   Model: center, Score: 0.50, Metadata: {"confidence":0.82}
2025/05/30 14:48:38 Candidates for right: 
2025/05/30 14:48:38   Model: right, Score: 0.30, Metadata: {"confidence":0.85}
2025/05/30 14:48:38   Model: right, Score: 0.60, Metadata: {"confidence":0.88}
2025/05/30 14:48:38 [DEBUG] Pre-Sum: validCount=3, len(validModels)=3
2025/05/30 14:48:38 Mapping model 'center' (score +Inf) to perspective 'center'
2025/05/30 14:48:38 Mapping model 'right' (score -2.00) to perspective 'right'
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Score map: map[center:0.5 left:0.4 right:0.6]
2025/05/30 14:48:38 Perspective left has 1 models
2025/05/30 14:48:38 Perspective center has 1 models
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Valid models map: map[center:true left:true right:true]
2025/05/30 14:48:38 Perspective right has 1 models
2025/05/30 14:48:38 [DEBUG] Pre-Calc: sum=1.5000, weightedSum=1.5000, weightTotal=3.0000, actualValidCount=3
2025/05/30 14:48:38 Candidates for left: 
2025/05/30 14:48:38   Model: left, Score: NaN, Metadata: {"confidence":0.90}
2025/05/30 14:48:38 Candidates for center: 
2025/05/30 14:48:38   Model: center, Score: +Inf, Metadata: {"confidence":0.80}
2025/05/30 14:48:38 Candidates for right: 
2025/05/30 14:48:38   Model: right, Score: -2.00, Metadata: {"confidence":0.85}
2025/05/30 14:48:38 [WARN][CONFIDENCE] No valid model scores found after processing. Returning default score.
=== CONT  TestComputeCompositeScoreEdgeCases/extreme_values_within_bounds
2025/05/30 14:48:38 Mapping model 'left' (score -1.00) to perspective 'left'
=== CONT  TestComputeCompositeScoreEdgeCases/all_zero_confidence
2025/05/30 14:48:38 Mapping model 'center' (score 0.00) to perspective 'center'
2025/05/30 14:48:38 Mapping model 'right' (score 1.00) to perspective 'right'
2025/05/30 14:48:38 Perspective left has 1 models
2025/05/30 14:48:38 Perspective center has 1 models
2025/05/30 14:48:38 Perspective right has 1 models
2025/05/30 14:48:38 Candidates for left: 
2025/05/30 14:48:38   Model: left, Score: -1.00, Metadata: {"confidence":0.90}
2025/05/30 14:48:38 Candidates for center: 
2025/05/30 14:48:38   Model: center, Score: 0.00, Metadata: {"confidence":0.80}
2025/05/30 14:48:38 Candidates for right: 
2025/05/30 14:48:38   Model: right, Score: 1.00, Metadata: {"confidence":0.85}
2025/05/30 14:48:38 [DEBUG] Pre-Sum: validCount=3, len(validModels)=3
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Score map: map[center:0 left:-1 right:1]
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Valid models map: map[center:true left:true right:true]
2025/05/30 14:48:38 [DEBUG] Pre-Calc: sum=0.0000, weightedSum=0.0000, weightTotal=3.0000, actualValidCount=3
=== CONT  TestComputeCompositeScoreEdgeCases/custom_min/max_bounds
2025/05/30 14:48:38 Critical warning: All 3 non-ensemble LLM models returned zero confidence
2025/05/30 14:48:38 [ERROR][CONFIDENCE] All scores are zero, returning error
=== CONT  TestComputeCompositeScoreEdgeCases/case_insensitive_model_names
=== CONT  TestComputeCompositeScoreEdgeCases/Out-of-range_ignored_with_one_valid_score
2025/05/30 14:48:38 Mapping model 'LeFt' (score -0.50) to perspective 'left'
2025/05/30 14:48:38 Mapping model 'cEnTeR' (score 0.00) to perspective 'center'
2025/05/30 14:48:38 Mapping model 'RIGHT' (score 0.50) to perspective 'right'
2025/05/30 14:48:38 Perspective left has 1 models
2025/05/30 14:48:38 Perspective center has 1 models
2025/05/30 14:48:38 Perspective right has 1 models
2025/05/30 14:48:38 Candidates for right: 
2025/05/30 14:48:38   Model: RIGHT, Score: 0.50, Metadata: {"confidence":0.85}
2025/05/30 14:48:38 Candidates for left: 
2025/05/30 14:48:38   Model: LeFt, Score: -0.50, Metadata: {"confidence":0.90}
2025/05/30 14:48:38 Candidates for center: 
2025/05/30 14:48:38   Model: cEnTeR, Score: 0.00, Metadata: {"confidence":0.80}
2025/05/30 14:48:38 [DEBUG] Pre-Sum: validCount=3, len(validModels)=3
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Score map: map[center:0 left:-0.5 right:0.5]
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Valid models map: map[center:true left:true right:true]
2025/05/30 14:48:38 [DEBUG] Pre-Calc: sum=0.0000, weightedSum=0.0000, weightTotal=3.0000, actualValidCount=3
2025/05/30 14:48:38 Mapping model 'left' (score 2.00) to perspective 'left'
2025/05/30 14:48:38 Mapping model 'center' (score -2.00) to perspective 'center'
2025/05/30 14:48:38 Mapping model 'right' (score 0.50) to perspective 'right'
2025/05/30 14:48:38 Perspective left has 1 models
2025/05/30 14:48:38 Perspective center has 1 models
2025/05/30 14:48:38 Perspective right has 1 models
2025/05/30 14:48:38 Candidates for left: 
2025/05/30 14:48:38   Model: left, Score: 2.00, Metadata: {"confidence":0.90}
2025/05/30 14:48:38 Candidates for center: 
2025/05/30 14:48:38   Model: center, Score: -2.00, Metadata: {"confidence":0.80}
2025/05/30 14:48:38 Candidates for right: 
2025/05/30 14:48:38   Model: right, Score: 0.50, Metadata: {"confidence":0.85}
2025/05/30 14:48:38 [DEBUG] Pre-Sum: validCount=1, len(validModels)=1
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Score map: map[center:0 left:0 right:0.5]
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Valid models map: map[right:true]
2025/05/30 14:48:38 [DEBUG] Pre-Calc: sum=0.5000, weightedSum=0.5000, weightTotal=1.0000, actualValidCount=1
=== CONT  TestComputeCompositeScoreEdgeCases/All_Infinity_values_-_ignored
2025/05/30 14:48:38 Mapping model 'left' (score +Inf) to perspective 'left'
2025/05/30 14:48:38 Mapping model 'center' (score -Inf) to perspective 'center'
2025/05/30 14:48:38 Mapping model 'right' (score +Inf) to perspective 'right'
2025/05/30 14:48:38 Perspective left has 1 models
2025/05/30 14:48:38 Perspective center has 1 models
2025/05/30 14:48:38 Perspective right has 1 models
=== CONT  TestComputeCompositeScoreEdgeCases/NaN_values_-_ignored
2025/05/30 14:48:38 Candidates for left: 
2025/05/30 14:48:38   Model: left, Score: +Inf, Metadata: {"confidence":0.90}
2025/05/30 14:48:38 Candidates for center: 
2025/05/30 14:48:38   Model: center, Score: -Inf, Metadata: {"confidence":0.80}
2025/05/30 14:48:38 Candidates for right: 
2025/05/30 14:48:38 Mapping model 'left' (score NaN) to perspective 'left'
2025/05/30 14:48:38 Mapping model 'center' (score 0.20) to perspective 'center'
2025/05/30 14:48:38 Mapping model 'left' (score -3.00) to perspective 'left'
2025/05/30 14:48:38 Mapping model 'right' (score 0.40) to perspective 'right'
2025/05/30 14:48:38 Mapping model 'center' (score 0.00) to perspective 'center'
2025/05/30 14:48:38 Mapping model 'right' (score 3.00) to perspective 'right'
=== CONT  TestComputeCompositeScoreEdgeCases/non-standard_model_names
2025/05/30 14:48:38 Perspective left has 1 models
2025/05/30 14:48:38 Perspective center has 1 models
2025/05/30 14:48:38 Perspective right has 1 models
2025/05/30 14:48:38 Candidates for left: 
2025/05/30 14:48:38   Model: left, Score: -3.00, Metadata: {"confidence":0.90}
2025/05/30 14:48:38 Candidates for center: 
2025/05/30 14:48:38   Model: center, Score: 0.00, Metadata: {"confidence":0.80}
2025/05/30 14:48:38 Candidates for right: 
2025/05/30 14:48:38   Model: right, Score: 3.00, Metadata: {"confidence":0.85}
2025/05/30 14:48:38 [DEBUG] Pre-Sum: validCount=3, len(validModels)=3
=== CONT  TestComputeCompositeScoreEdgeCases/values_outside_bounds
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Score map: map[center:0 left:-3 right:3]
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Valid models map: map[center:true left:true right:true]
2025/05/30 14:48:38 [DEBUG] Pre-Calc: sum=0.0000, weightedSum=0.0000, weightTotal=3.0000, actualValidCount=3
2025/05/30 14:48:38 Mapping model 'custom-left-model' (score -0.50) to perspective 'left'
2025/05/30 14:48:38 Mapping model 'left' (score -1.50) to perspective 'left'
2025/05/30 14:48:38   Model: right, Score: +Inf, Metadata: {"confidence":0.85}
2025/05/30 14:48:38 [WARN][CONFIDENCE] No valid model scores found after processing. Returning default score.
2025/05/30 14:48:38 Mapping model 'center' (score 0.10) to perspective 'center'
2025/05/30 14:48:38 Mapping model 'custom-center-model' (score 0.00) to perspective 'center'
2025/05/30 14:48:38 Mapping model 'custom-right-model' (score 0.50) to perspective 'right'
2025/05/30 14:48:38 Perspective left has 1 models
2025/05/30 14:48:38 Perspective center has 1 models
2025/05/30 14:48:38 Perspective center has 1 models
2025/05/30 14:48:38 Perspective right has 1 models
2025/05/30 14:48:38 Perspective right has 1 models
2025/05/30 14:48:38 Perspective left has 1 models
2025/05/30 14:48:38 Candidates for left: 
2025/05/30 14:48:38 Candidates for right: 
2025/05/30 14:48:38   Model: custom-left-model, Score: -0.50, Metadata: {"confidence":0.90}
2025/05/30 14:48:38   Model: right, Score: 0.40, Metadata: {"confidence":0.85}
2025/05/30 14:48:38 Candidates for center: 
2025/05/30 14:48:38 Candidates for left: 
2025/05/30 14:48:38   Model: custom-center-model, Score: 0.00, Metadata: {"confidence":0.80}
2025/05/30 14:48:38   Model: left, Score: NaN, Metadata: {"confidence":0.90}
2025/05/30 14:48:38 Candidates for right: 
2025/05/30 14:48:38 Candidates for center: 
2025/05/30 14:48:38   Model: custom-right-model, Score: 0.50, Metadata: {"confidence":0.85}
2025/05/30 14:48:38   Model: center, Score: 0.20, Metadata: {"confidence":0.80}
2025/05/30 14:48:38 [DEBUG] Pre-Sum: validCount=3, len(validModels)=3
2025/05/30 14:48:38 [DEBUG] Pre-Sum: validCount=2, len(validModels)=2
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Score map: map[center:0 left:-0.5 right:0.5]
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Valid models map: map[center:true left:true right:true]
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Score map: map[center:0.2 left:0 right:0.4]
2025/05/30 14:48:38 [DEBUG] Pre-Calc: sum=0.0000, weightedSum=0.0000, weightTotal=3.0000, actualValidCount=3
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Valid models map: map[center:true right:true]
2025/05/30 14:48:38 [DEBUG] Pre-Calc: sum=0.6000, weightedSum=0.6000, weightTotal=2.0000, actualValidCount=2
2025/05/30 14:48:38 Mapping model 'right' (score 1.50) to perspective 'right'
2025/05/30 14:48:38 Perspective left has 1 models
2025/05/30 14:48:38 Perspective center has 1 models
2025/05/30 14:48:38 Perspective right has 1 models
2025/05/30 14:48:38 Candidates for left: 
2025/05/30 14:48:38   Model: left, Score: -1.50, Metadata: {"confidence":0.90}
2025/05/30 14:48:38 Candidates for center: 
2025/05/30 14:48:38   Model: center, Score: 0.10, Metadata: {"confidence":0.80}
2025/05/30 14:48:38 Candidates for right: 
2025/05/30 14:48:38   Model: right, Score: 1.50, Metadata: {"confidence":0.85}
2025/05/30 14:48:38 [DEBUG] Pre-Sum: validCount=1, len(validModels)=1
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Score map: map[center:0.1 left:0 right:0]
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Valid models map: map[center:true]
2025/05/30 14:48:38 [DEBUG] Pre-Calc: sum=0.1000, weightedSum=0.1000, weightTotal=1.0000, actualValidCount=1
--- PASS: TestComputeCompositeScoreEdgeCases (0.00s)
    --- PASS: TestComputeCompositeScoreEdgeCases/empty_scores_array (0.00s)
    --- PASS: TestComputeCompositeScoreEdgeCases/weighted_formula_with_config_override (0.00s)
    --- PASS: TestComputeCompositeScoreEdgeCases/duplicate_model_scores_-_should_use_last_one (0.00s)
    --- PASS: TestComputeCompositeScoreEdgeCases/ignore_invalid_with_config_override (0.00s)
    --- PASS: TestComputeCompositeScoreEdgeCases/extreme_values_within_bounds (0.00s)
    --- PASS: TestComputeCompositeScoreEdgeCases/all_zero_confidence (0.00s)
    --- PASS: TestComputeCompositeScoreEdgeCases/case_insensitive_model_names (0.00s)
    --- PASS: TestComputeCompositeScoreEdgeCases/Out-of-range_ignored_with_one_valid_score (0.00s)
    --- PASS: TestComputeCompositeScoreEdgeCases/custom_min/max_bounds (0.00s)
    --- PASS: TestComputeCompositeScoreEdgeCases/All_Infinity_values_-_ignored (0.00s)
    --- PASS: TestComputeCompositeScoreEdgeCases/non-standard_model_names (0.00s)
    --- PASS: TestComputeCompositeScoreEdgeCases/NaN_values_-_ignored (0.00s)
    --- PASS: TestComputeCompositeScoreEdgeCases/values_outside_bounds (0.00s)
=== RUN   TestComputeCompositeScoreWeightedCalculation
=== RUN   TestComputeCompositeScoreWeightedCalculation/Equal_weights
2025/05/30 14:48:38 Mapping model 'left' (score 0.10) to perspective 'left'
2025/05/30 14:48:38 Mapping model 'center' (score 0.10) to perspective 'center'
2025/05/30 14:48:38 Mapping model 'right' (score 0.10) to perspective 'right'
2025/05/30 14:48:38 Perspective left has 1 models
2025/05/30 14:48:38 Perspective center has 1 models
2025/05/30 14:48:38 Perspective right has 1 models
2025/05/30 14:48:38 Candidates for center: 
2025/05/30 14:48:38   Model: center, Score: 0.10, Metadata: {"confidence":0.9}
2025/05/30 14:48:38 Candidates for right: 
2025/05/30 14:48:38   Model: right, Score: 0.10, Metadata: {"confidence":0.9}
2025/05/30 14:48:38 Candidates for left: 
2025/05/30 14:48:38   Model: left, Score: 0.10, Metadata: {"confidence":0.9}
2025/05/30 14:48:38 [DEBUG] Pre-Sum: validCount=3, len(validModels)=3
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Score map: map[center:0.1 left:0.1 right:0.1]
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Valid models map: map[center:true left:true right:true]
2025/05/30 14:48:38 [DEBUG] Pre-Calc: sum=0.3000, weightedSum=0.3000, weightTotal=3.0000, actualValidCount=3
=== RUN   TestComputeCompositeScoreWeightedCalculation/Unequal_weights
2025/05/30 14:48:38 Mapping model 'left' (score 0.10) to perspective 'left'
2025/05/30 14:48:38 Mapping model 'center' (score 0.20) to perspective 'center'
2025/05/30 14:48:38 Mapping model 'right' (score 0.30) to perspective 'right'
2025/05/30 14:48:38 Perspective left has 1 models
2025/05/30 14:48:38 Perspective center has 1 models
2025/05/30 14:48:38 Perspective right has 1 models
2025/05/30 14:48:38 Candidates for left: 
2025/05/30 14:48:38   Model: left, Score: 0.10, Metadata: {"confidence":0.9}
2025/05/30 14:48:38 Candidates for center: 
2025/05/30 14:48:38   Model: center, Score: 0.20, Metadata: {"confidence":0.9}
2025/05/30 14:48:38 Candidates for right: 
2025/05/30 14:48:38   Model: right, Score: 0.30, Metadata: {"confidence":0.9}
2025/05/30 14:48:38 [DEBUG] Pre-Sum: validCount=3, len(validModels)=3
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Score map: map[center:0.2 left:0.1 right:0.3]
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Valid models map: map[center:true left:true right:true]
2025/05/30 14:48:38 [DEBUG] Pre-Calc: sum=0.6000, weightedSum=0.2300, weightTotal=1.0000, actualValidCount=3
=== RUN   TestComputeCompositeScoreWeightedCalculation/Zero_weight
2025/05/30 14:48:38 Mapping model 'left' (score 0.10) to perspective 'left'
2025/05/30 14:48:38 Mapping model 'center' (score 0.20) to perspective 'center'
2025/05/30 14:48:38 Mapping model 'right' (score 0.30) to perspective 'right'
2025/05/30 14:48:38 Perspective left has 1 models
2025/05/30 14:48:38 Perspective center has 1 models
2025/05/30 14:48:38 Perspective right has 1 models
2025/05/30 14:48:38 Candidates for left: 
2025/05/30 14:48:38   Model: left, Score: 0.10, Metadata: {"confidence":0.9}
2025/05/30 14:48:38 Candidates for center: 
2025/05/30 14:48:38   Model: center, Score: 0.20, Metadata: {"confidence":0.9}
2025/05/30 14:48:38 Candidates for right: 
2025/05/30 14:48:38   Model: right, Score: 0.30, Metadata: {"confidence":0.9}
2025/05/30 14:48:38 [DEBUG] Pre-Sum: validCount=3, len(validModels)=3
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Score map: map[center:0.2 left:0.1 right:0.3]
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Valid models map: map[center:true left:true right:true]
2025/05/30 14:48:38 [DEBUG] Pre-Calc: sum=0.6000, weightedSum=0.2500, weightTotal=1.0000, actualValidCount=3
--- PASS: TestComputeCompositeScoreWeightedCalculation (0.00s)
    --- PASS: TestComputeCompositeScoreWeightedCalculation/Equal_weights (0.00s)
    --- PASS: TestComputeCompositeScoreWeightedCalculation/Unequal_weights (0.00s)
    --- PASS: TestComputeCompositeScoreWeightedCalculation/Zero_weight (0.00s)
=== RUN   TestGetConfigDir
--- PASS: TestGetConfigDir (0.00s)
=== RUN   TestMinNonNil
=== RUN   TestMinNonNil/empty_map_returns_default
=== RUN   TestMinNonNil/single_value_returns_that_value
=== RUN   TestMinNonNil/multiple_values_returns_minimum
=== RUN   TestMinNonNil/negative_values_handled_correctly
--- PASS: TestMinNonNil (0.00s)
    --- PASS: TestMinNonNil/empty_map_returns_default (0.00s)
    --- PASS: TestMinNonNil/single_value_returns_that_value (0.00s)
    --- PASS: TestMinNonNil/multiple_values_returns_minimum (0.00s)
    --- PASS: TestMinNonNil/negative_values_handled_correctly (0.00s)
=== RUN   TestMaxNonNil
=== RUN   TestMaxNonNil/empty_map_returns_default
=== RUN   TestMaxNonNil/single_value_returns_that_value
=== RUN   TestMaxNonNil/multiple_values_returns_maximum
=== RUN   TestMaxNonNil/negative_values_handled_correctly
--- PASS: TestMaxNonNil (0.00s)
    --- PASS: TestMaxNonNil/empty_map_returns_default (0.00s)
    --- PASS: TestMaxNonNil/single_value_returns_that_value (0.00s)
    --- PASS: TestMaxNonNil/multiple_values_returns_maximum (0.00s)
    --- PASS: TestMaxNonNil/negative_values_handled_correctly (0.00s)
=== RUN   TestScoreSpread
=== RUN   TestScoreSpread/empty_map_returns_0
=== RUN   TestScoreSpread/single_value_returns_0
=== RUN   TestScoreSpread/multiple_values_returns_max-min
=== RUN   TestScoreSpread/negative_values_handled_correctly
--- PASS: TestScoreSpread (0.00s)
    --- PASS: TestScoreSpread/empty_map_returns_0 (0.00s)
    --- PASS: TestScoreSpread/single_value_returns_0 (0.00s)
    --- PASS: TestScoreSpread/multiple_values_returns_max-min (0.00s)
    --- PASS: TestScoreSpread/negative_values_handled_correctly (0.00s)
=== RUN   TestLoadPromptVariants
--- PASS: TestLoadPromptVariants (0.00s)
=== RUN   TestEnsembleAnalyze
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] ScoreWithModel called for article 1 with model model1
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] Test mode detected (nil db), returning score without storage: 0.5000
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] ScoreWithModel called for article 1 with model model2
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] Test mode detected (nil db), returning score without storage: -0.3000
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] ScoreWithModel called for article 1 with model model3
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] Test mode detected (nil db), returning score without storage: 0.0000
2025/05/30 14:48:38 Mapping model 'model1' (score 0.50) to perspective 'left'
2025/05/30 14:48:38 Mapping model 'model2' (score -0.30) to perspective 'right'
2025/05/30 14:48:38 Mapping model 'model3' (score 0.00) to perspective 'center'
2025/05/30 14:48:38 Perspective right has 1 models
2025/05/30 14:48:38 Perspective center has 1 models
2025/05/30 14:48:38 Perspective left has 1 models
2025/05/30 14:48:38 Candidates for left: 
2025/05/30 14:48:38   Model: model1, Score: 0.50, Metadata: {"confidence": 0.80}
2025/05/30 14:48:38 Candidates for right: 
2025/05/30 14:48:38   Model: model2, Score: -0.30, Metadata: {"confidence": 0.70}
2025/05/30 14:48:38 Candidates for center: 
2025/05/30 14:48:38   Model: model3, Score: 0.00, Metadata: {"confidence": 0.90}
2025/05/30 14:48:38 [DEBUG] Pre-Sum: validCount=3, len(validModels)=3
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Score map: map[center:0 left:0.5 right:-0.3]
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Valid models map: map[center:true left:true right:true]
2025/05/30 14:48:38 [DEBUG] Pre-Calc: sum=0.2000, weightedSum=0.2000, weightTotal=3.0000, actualValidCount=3
--- PASS: TestEnsembleAnalyze (0.00s)
=== RUN   TestScoreWithModel_CacheUsage
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] ScoreWithModel called for article 1 with model model1
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] Test mode detected (nil db), returning score without storage: 0.5000
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] ScoreWithModel called for article 1 with model model1
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] Test mode detected (nil db), returning score without storage: 0.5000
--- PASS: TestScoreWithModel_CacheUsage (0.00s)
=== RUN   TestLLMClient_StreamingErrorDetection
=== RUN   TestLLMClient_StreamingErrorDetection/SSE_Streaming_Error
=== RUN   TestLLMClient_StreamingErrorDetection/Stream_Disconnected_Error
=== RUN   TestLLMClient_StreamingErrorDetection/Processing_Error
=== RUN   TestLLMClient_StreamingErrorDetection/Regular_Error_(Non-streaming)
--- PASS: TestLLMClient_StreamingErrorDetection (0.00s)
    --- PASS: TestLLMClient_StreamingErrorDetection/SSE_Streaming_Error (0.00s)
    --- PASS: TestLLMClient_StreamingErrorDetection/Stream_Disconnected_Error (0.00s)
    --- PASS: TestLLMClient_StreamingErrorDetection/Processing_Error (0.00s)
    --- PASS: TestLLMClient_StreamingErrorDetection/Regular_Error_(Non-streaming) (0.00s)
=== RUN   TestLLMAPIError_ErrorPropagation
=== RUN   TestLLMAPIError_ErrorPropagation/Rate_Limit_Error
=== RUN   TestLLMAPIError_ErrorPropagation/Authentication_Error
=== RUN   TestLLMAPIError_ErrorPropagation/Credits_Exhausted
=== RUN   TestLLMAPIError_ErrorPropagation/Streaming_Error
--- PASS: TestLLMAPIError_ErrorPropagation (0.00s)
    --- PASS: TestLLMAPIError_ErrorPropagation/Rate_Limit_Error (0.00s)
    --- PASS: TestLLMAPIError_ErrorPropagation/Authentication_Error (0.00s)
    --- PASS: TestLLMAPIError_ErrorPropagation/Credits_Exhausted (0.00s)
    --- PASS: TestLLMAPIError_ErrorPropagation/Streaming_Error (0.00s)
=== RUN   TestComputeCompositeScore
2025/05/30 14:48:38 Attempting to load composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
2025/05/30 14:48:38 Successfully loaded and parsed composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
=== RUN   TestComputeCompositeScore/normal_case_-_all_models
2025/05/30 14:48:38 Mapping model 'left' (score -0.80) to perspective 'left'
2025/05/30 14:48:38 Mapping model 'center' (score 0.20) to perspective 'center'
2025/05/30 14:48:38 Mapping model 'right' (score 0.60) to perspective 'right'
2025/05/30 14:48:38 Perspective left has 1 models
2025/05/30 14:48:38 Perspective center has 1 models
2025/05/30 14:48:38 Perspective right has 1 models
2025/05/30 14:48:38 Candidates for left: 
2025/05/30 14:48:38   Model: left, Score: -0.80, Metadata: {"confidence":0.9}
2025/05/30 14:48:38 Candidates for center: 
2025/05/30 14:48:38   Model: center, Score: 0.20, Metadata: {"confidence":0.8}
2025/05/30 14:48:38 Candidates for right: 
2025/05/30 14:48:38   Model: right, Score: 0.60, Metadata: {"confidence":0.85}
2025/05/30 14:48:38 [DEBUG] Pre-Sum: validCount=3, len(validModels)=3
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Score map: map[center:0.2 left:-0.8 right:0.6]
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Valid models map: map[center:true left:true right:true]
2025/05/30 14:48:38 [DEBUG] Pre-Calc: sum=-0.0000, weightedSum=-0.0000, weightTotal=3.0000, actualValidCount=3
=== RUN   TestComputeCompositeScore/some_models_missing
2025/05/30 14:48:38 Mapping model 'left' (score -0.50) to perspective 'left'
2025/05/30 14:48:38 Mapping model 'right' (score 0.50) to perspective 'right'
2025/05/30 14:48:38 Perspective left has 1 models
2025/05/30 14:48:38 Perspective right has 1 models
2025/05/30 14:48:38 Candidates for left: 
2025/05/30 14:48:38   Model: left, Score: -0.50, Metadata: {"confidence":0.85}
2025/05/30 14:48:38 Candidates for right: 
2025/05/30 14:48:38   Model: right, Score: 0.50, Metadata: {"confidence":0.9}
2025/05/30 14:48:38 [DEBUG] Pre-Sum: validCount=2, len(validModels)=2
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Score map: map[center:0 left:-0.5 right:0.5]
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Valid models map: map[left:true right:true]
2025/05/30 14:48:38 [DEBUG] Pre-Calc: sum=0.0000, weightedSum=0.0000, weightTotal=2.0000, actualValidCount=2
=== RUN   TestComputeCompositeScore/single_model
2025/05/30 14:48:38 Mapping model 'left' (score 0.10) to perspective 'left'
2025/05/30 14:48:38 Perspective left has 1 models
2025/05/30 14:48:38 Candidates for left: 
2025/05/30 14:48:38   Model: left, Score: 0.10, Metadata: {"confidence":0.9}
2025/05/30 14:48:38 [DEBUG] Pre-Sum: validCount=1, len(validModels)=1
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Score map: map[center:0 left:0.1 right:0]
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Valid models map: map[left:true]
2025/05/30 14:48:38 [DEBUG] Pre-Calc: sum=0.1000, weightedSum=0.1000, weightTotal=1.0000, actualValidCount=1
=== RUN   TestComputeCompositeScore/empty_scores_array
=== RUN   TestComputeCompositeScore/all_invalid_scores
2025/05/30 14:48:38 Mapping model 'left' (score NaN) to perspective 'left'
2025/05/30 14:48:38 Mapping model 'center' (score +Inf) to perspective 'center'
2025/05/30 14:48:38 Mapping model 'right' (score -2.00) to perspective 'right'
2025/05/30 14:48:38 Perspective left has 1 models
2025/05/30 14:48:38 Perspective center has 1 models
2025/05/30 14:48:38 Perspective right has 1 models
2025/05/30 14:48:38 Candidates for left: 
2025/05/30 14:48:38   Model: left, Score: NaN, Metadata: {"confidence":0.9}
2025/05/30 14:48:38 Candidates for center: 
2025/05/30 14:48:38   Model: center, Score: +Inf, Metadata: {"confidence":0.8}
2025/05/30 14:48:38 Candidates for right: 
2025/05/30 14:48:38   Model: right, Score: -2.00, Metadata: {"confidence":0.85}
2025/05/30 14:48:38 [WARN][CONFIDENCE] No valid model scores found after processing. Returning default score.
=== RUN   TestComputeCompositeScore/all_zero_confidence
2025/05/30 14:48:38 Critical warning: All 3 non-ensemble LLM models returned zero confidence
2025/05/30 14:48:38 [ERROR][CONFIDENCE] All scores are zero, returning error
--- PASS: TestComputeCompositeScore (0.01s)
    --- PASS: TestComputeCompositeScore/normal_case_-_all_models (0.00s)
    --- PASS: TestComputeCompositeScore/some_models_missing (0.00s)
    --- PASS: TestComputeCompositeScore/single_model (0.00s)
    --- PASS: TestComputeCompositeScore/empty_scores_array (0.00s)
    --- PASS: TestComputeCompositeScore/all_invalid_scores (0.01s)
    --- PASS: TestComputeCompositeScore/all_zero_confidence (0.00s)
=== RUN   TestLLMClientInitialization
2025/05/30 14:48:38 Attempting to load composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
2025/05/30 14:48:38 Successfully loaded and parsed composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
--- PASS: TestLLMClientInitialization (0.00s)
=== RUN   TestNewLLMClientMissingPrimaryKey
--- PASS: TestNewLLMClientMissingPrimaryKey (0.00s)
=== RUN   TestModelConfiguration
2025/05/30 14:48:38 Attempting to load composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
2025/05/30 14:48:38 Successfully loaded and parsed composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
    llm_test.go:237: Client object before config check: &{client:0xc0000b2a50 cache:0xc0000b27b0 db:<nil> llmService:0xc000096280 config:0xc0000ca300}
    llm_test.go:238: Client.config value before check: 0xc0000ca300
--- PASS: TestModelConfiguration (0.00s)
=== RUN   TestSetHTTPLLMTimeout
--- PASS: TestSetHTTPLLMTimeout (0.00s)
=== RUN   TestCompositeScoreWithConfig
=== RUN   TestCompositeScoreWithConfig/Basic_average_calculation
2025/05/30 14:48:38 Critical warning: All 3 non-ensemble LLM models returned zero confidence
2025/05/30 14:48:38 [ERROR][CONFIDENCE] All scores are zero, returning error
2025/05/30 14:48:38 [ERROR] Error computing composite score: all perspectives returned invalid scores: all LLMs returned empty or zero-confidence responses
=== RUN   TestCompositeScoreWithConfig/Missing_score_uses_default_value_(0.0)
2025/05/30 14:48:38 Critical warning: All 2 non-ensemble LLM models returned zero confidence
2025/05/30 14:48:38 [ERROR][CONFIDENCE] All scores are zero, returning error
2025/05/30 14:48:38 [ERROR] Error computing composite score: all perspectives returned invalid scores: all LLMs returned empty or zero-confidence responses
=== RUN   TestCompositeScoreWithConfig/Empty_scores_array
2025/05/30 14:48:38 [ERROR] Error computing composite score: no scores provided: all perspectives returned invalid scores
--- PASS: TestCompositeScoreWithConfig (0.00s)
    --- PASS: TestCompositeScoreWithConfig/Basic_average_calculation (0.00s)
    --- PASS: TestCompositeScoreWithConfig/Missing_score_uses_default_value_(0.0) (0.00s)
    --- PASS: TestCompositeScoreWithConfig/Empty_scores_array (0.00s)
=== RUN   TestScoreWithModel
=== RUN   TestScoreWithModel/Successful_scoring_and_storage
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] ScoreWithModel called for article 123 with model test-model
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] Successfully scored and stored: article=123, model=test-model, score=0.7500
=== RUN   TestScoreWithModel/DB_error_case
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] ScoreWithModel called for article 123 with model test-model
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] Successfully scored and stored: article=123, model=test-model, score=-0.5000
2025/05/30 14:48:38 [ERROR] InsertLLMScore failed for article 123 model test-model score -0.500: database error
2025/05/30 14:48:38 [ERROR][CONFIDENCE] Failed to store score in database: failed to insert LLM score: internal
--- PASS: TestScoreWithModel (0.00s)
    --- PASS: TestScoreWithModel/Successful_scoring_and_storage (0.00s)
    --- PASS: TestScoreWithModel/DB_error_case (0.00s)
=== RUN   TestAnalyzeAndStore
2025/05/30 14:48:38 Attempting to load composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
2025/05/30 14:48:38 Successfully loaded and parsed composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
2025/05/30 14:48:38 [DEBUG][AnalyzeAndStore] Article 42 | Perspective: left | ModelName passed: left | URL: 
2025/05/30 14:48:38 [analyzeContent] Entry: articleID=42, model=left
2025/05/30 14:48:38 Attempting to load composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
2025/05/30 14:48:38 Successfully loaded and parsed composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
2025/05/30 14:48:38 Prompt snippet [96b417a16bbbf62f] (attempt 1): Please analyze the political bias of the following article on a scale from -1.0 ...
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {
			"choices": [
				{"message": {"content": "{\"score\": 0.5, \"explanation\": \"ok\", \"confidence\": 0.8}"}}
			]
		}
2025/05/30 14:48:38 [DEBUG][LLM] Extracted content: {"score": 0.5, "explanation": "ok", "confidence": 0.8}
2025/05/30 14:48:38 [DEBUG][LLM] Successfully parsed as JSON: score=0.5000, confidence=0.8000
2025/05/30 14:48:38 [LLM] ArticleID 42 | Model left | PromptHash 96b417a16bbbf62f | Success | Score: 0.500 | Confidence: 0.800
2025/05/30 14:48:38 [DEBUG][AnalyzeAndStore] Article 42 | Perspective: center | ModelName passed: center | URL: 
2025/05/30 14:48:38 [analyzeContent] Entry: articleID=42, model=center
2025/05/30 14:48:38 Attempting to load composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
2025/05/30 14:48:38 Successfully loaded and parsed composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
2025/05/30 14:48:38 Prompt snippet [96b417a16bbbf62f] (attempt 1): Please analyze the political bias of the following article on a scale from -1.0 ...
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {
			"choices": [
				{"message": {"content": "{\"score\": 0.5, \"explanation\": \"ok\", \"confidence\": 0.8}"}}
			]
		}
2025/05/30 14:48:38 [DEBUG][LLM] Extracted content: {"score": 0.5, "explanation": "ok", "confidence": 0.8}
2025/05/30 14:48:38 [DEBUG][LLM] Successfully parsed as JSON: score=0.5000, confidence=0.8000
2025/05/30 14:48:38 [LLM] ArticleID 42 | Model center | PromptHash 96b417a16bbbf62f | Success | Score: 0.500 | Confidence: 0.800
2025/05/30 14:48:38 [DEBUG][AnalyzeAndStore] Article 42 | Perspective: right | ModelName passed: right | URL: 
2025/05/30 14:48:38 [analyzeContent] Entry: articleID=42, model=right
2025/05/30 14:48:38 Attempting to load composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
2025/05/30 14:48:38 Successfully loaded and parsed composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
2025/05/30 14:48:38 Prompt snippet [96b417a16bbbf62f] (attempt 1): Please analyze the political bias of the following article on a scale from -1.0 ...
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {
			"choices": [
				{"message": {"content": "{\"score\": 0.5, \"explanation\": \"ok\", \"confidence\": 0.8}"}}
			]
		}
2025/05/30 14:48:38 [DEBUG][LLM] Extracted content: {"score": 0.5, "explanation": "ok", "confidence": 0.8}
2025/05/30 14:48:38 [DEBUG][LLM] Successfully parsed as JSON: score=0.5000, confidence=0.8000
2025/05/30 14:48:38 [LLM] ArticleID 42 | Model right | PromptHash 96b417a16bbbf62f | Success | Score: 0.500 | Confidence: 0.800
2025/05/30 14:48:38 [DEBUG][AnalyzeAndStore] Article 43 | Perspective: left | ModelName passed: left | URL: 
2025/05/30 14:48:38 [analyzeContent] Entry: articleID=43, model=left
2025/05/30 14:48:38 Attempting to load composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
2025/05/30 14:48:38 Successfully loaded and parsed composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
2025/05/30 14:48:38 Prompt snippet [57e0e0df573c0aca] (attempt 1): Please analyze the political bias of the following article on a scale from -1.0 ...
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {
			"choices": [
				{"message": {"content": "{\"score\": 0.5, \"explanation\": \"ok\", \"confidence\": 0.8}"}}
			]
		}
2025/05/30 14:48:38 [DEBUG][LLM] Extracted content: {"score": 0.5, "explanation": "ok", "confidence": 0.8}
2025/05/30 14:48:38 [DEBUG][LLM] Successfully parsed as JSON: score=0.5000, confidence=0.8000
2025/05/30 14:48:38 [LLM] ArticleID 43 | Model left | PromptHash 57e0e0df573c0aca | Success | Score: 0.500 | Confidence: 0.800
2025/05/30 14:48:38 [ERROR] InsertLLMScore failed for article 43 model left score 0.500: db error
2025/05/30 14:48:38 Error inserting LLM score for article 43 model left: failed to insert LLM score: internal
2025/05/30 14:48:38 [DEBUG][AnalyzeAndStore] Article 43 | Perspective: center | ModelName passed: center | URL: 
2025/05/30 14:48:38 [analyzeContent] Entry: articleID=43, model=center
2025/05/30 14:48:38 Attempting to load composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
2025/05/30 14:48:38 Successfully loaded and parsed composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
2025/05/30 14:48:38 Prompt snippet [57e0e0df573c0aca] (attempt 1): Please analyze the political bias of the following article on a scale from -1.0 ...
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {
			"choices": [
				{"message": {"content": "{\"score\": 0.5, \"explanation\": \"ok\", \"confidence\": 0.8}"}}
			]
		}
2025/05/30 14:48:38 [DEBUG][LLM] Extracted content: {"score": 0.5, "explanation": "ok", "confidence": 0.8}
2025/05/30 14:48:38 [DEBUG][LLM] Successfully parsed as JSON: score=0.5000, confidence=0.8000
2025/05/30 14:48:38 [LLM] ArticleID 43 | Model center | PromptHash 57e0e0df573c0aca | Success | Score: 0.500 | Confidence: 0.800
2025/05/30 14:48:38 [ERROR] InsertLLMScore failed for article 43 model center score 0.500: all expectations were already fulfilled, call to ExecQuery '
        INSERT INTO llm_scores (article_id, model, score, metadata, version, created_at)
        VALUES (?, ?, ?, ?, ?, ?)' with args [{Name: Ordinal:1 Value:43} {Name: Ordinal:2 Value:center} {Name: Ordinal:3 Value:0.5} {Name: Ordinal:4 Value:{"explanation": "ok", "confidence": 0.800, "perspective": "center"}} {Name: Ordinal:5 Value:1} {Name: Ordinal:6 Value:2025-05-30 14:48:38.2442994 +0300 EEST m=+0.056122301}] was not expected
2025/05/30 14:48:38 Error inserting LLM score for article 43 model center: failed to insert LLM score: internal
2025/05/30 14:48:38 [DEBUG][AnalyzeAndStore] Article 43 | Perspective: right | ModelName passed: right | URL: 
2025/05/30 14:48:38 [analyzeContent] Entry: articleID=43, model=right
2025/05/30 14:48:38 Attempting to load composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
2025/05/30 14:48:38 Successfully loaded and parsed composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
2025/05/30 14:48:38 Prompt snippet [57e0e0df573c0aca] (attempt 1): Please analyze the political bias of the following article on a scale from -1.0 ...
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {
			"choices": [
				{"message": {"content": "{\"score\": 0.5, \"explanation\": \"ok\", \"confidence\": 0.8}"}}
			]
		}
2025/05/30 14:48:38 [DEBUG][LLM] Extracted content: {"score": 0.5, "explanation": "ok", "confidence": 0.8}
2025/05/30 14:48:38 [DEBUG][LLM] Successfully parsed as JSON: score=0.5000, confidence=0.8000
2025/05/30 14:48:38 [LLM] ArticleID 43 | Model right | PromptHash 57e0e0df573c0aca | Success | Score: 0.500 | Confidence: 0.800
2025/05/30 14:48:38 [ERROR] InsertLLMScore failed for article 43 model right score 0.500: all expectations were already fulfilled, call to ExecQuery '
        INSERT INTO llm_scores (article_id, model, score, metadata, version, created_at)
        VALUES (?, ?, ?, ?, ?, ?)' with args [{Name: Ordinal:1 Value:43} {Name: Ordinal:2 Value:right} {Name: Ordinal:3 Value:0.5} {Name: Ordinal:4 Value:{"explanation": "ok", "confidence": 0.800, "perspective": "right"}} {Name: Ordinal:5 Value:1} {Name: Ordinal:6 Value:2025-05-30 14:48:38.244808 +0300 EEST m=+0.056631001}] was not expected
2025/05/30 14:48:38 Error inserting LLM score for article 43 model right: failed to insert LLM score: internal
--- PASS: TestAnalyzeAndStore (0.03s)
=== RUN   TestGetHTTPLLMTimeout
=== PAUSE TestGetHTTPLLMTimeout
=== RUN   TestHTTPLLMServiceWithMockServer
=== RUN   TestHTTPLLMServiceWithMockServer/Successful_response
    mock_http_service_test.go:25: Mock server URL: http://127.0.0.1:62855
2025/05/30 14:48:38 [DEBUG][LLM] Raw response for article 1, model meta-llama/llama-4-maverick: {
				"id": "test-id",
				"object": "chat.completion",
				"created": 1714349818,
				"model": "meta-llama/llama-4-maverick",
				"choices": [
					{
						"index": 0,
						"message": {
							"role": "assistant",
							"content": "The article has a left-leaning bias. Score: -0.7\\nConfidence: 0.9\\nReasoning: The article emphasizes progressive viewpoints and criticizes conservative positions."
						},
						"finish_reason": "stop"
					}
				]
			}
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {
				"id": "test-id",
				"object": "chat.completion",
				"created": 1714349818,
				"model": "meta-llama/llama-4-maverick",
				"choices": [
					{
						"index": 0,
						"message": {
							"role": "assistant",
							"content": "The article has a left-leaning bias. Score: -0.7\\nConfidence: 0.9\\nReasoning: The article emphasizes progressive viewpoints and criticizes conservative positions."
						},
						"finish_reason": "stop"
					}
				]
			}
2025/05/30 14:48:38 [DEBUG][LLM] Extracted content: The article has a left-leaning bias. Score: -0.7\nConfidence: 0.9\nReasoning: The article emphasizes progressive viewpoints and criticizes conservative positions.
2025/05/30 14:48:38 [DEBUG][LLM] JSON parsing failed, raw content: The article has a left-leaning bias. Score: -0.7\nConfidence: 0.9\nReasoning: The article emphasizes progressive viewpoints and criticizes conservative positions., error: <nil>
2025/05/30 14:48:38 [DEBUG][LLM] Successfully parsed with regex: score=-0.7000, confidence=0.9000
2025/05/30 14:48:38 [DEBUG][LLM] Parsed response for article 1, model meta-llama/llama-4-maverick: score=-0.7000, confidence=0.9000, err=<nil>
=== RUN   TestHTTPLLMServiceWithMockServer/Response_with_missing_content
    mock_http_service_test.go:25: Mock server URL: http://127.0.0.1:62857
2025/05/30 14:48:38 [DEBUG][LLM] Raw response for article 1, model meta-llama/llama-4-maverick: {
				"id": "test-id",
				"object": "chat.completion",
				"created": 1714349818,
				"model": "meta-llama/llama-4-maverick",
				"choices": [
					{
						"index": 0,
						"message": {
							"role": "assistant"
						},
						"finish_reason": "stop"
					}
				]
			}
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {
				"id": "test-id",
				"object": "chat.completion",
				"created": 1714349818,
				"model": "meta-llama/llama-4-maverick",
				"choices": [
					{
						"index": 0,
						"message": {
							"role": "assistant"
						},
						"finish_reason": "stop"
					}
				]
			}
2025/05/30 14:48:38 [DEBUG][LLM] Extracted content: 
2025/05/30 14:48:38 [DEBUG][LLM] JSON parsing failed, raw content: , error: <nil>
2025/05/30 14:48:38 [DEBUG][LLM] No score found in text
2025/05/30 14:48:38 [DEBUG][LLM] Parsed response for article 1, model meta-llama/llama-4-maverick: score=0.0000, confidence=0.0000, err=error parsing inner content JSON: %!w(<nil>)
=== RUN   TestHTTPLLMServiceWithMockServer/Response_with_no_score
    mock_http_service_test.go:25: Mock server URL: http://127.0.0.1:62859
2025/05/30 14:48:38 [DEBUG][LLM] Raw response for article 1, model meta-llama/llama-4-maverick: {
				"id": "test-id",
				"object": "chat.completion",
				"created": 1714349818,
				"model": "meta-llama/llama-4-maverick",
				"choices": [
					{
						"index": 0,
						"message": {
							"role": "assistant",
							"content": "I cannot determine a bias score for this content."
						},
						"finish_reason": "stop"
					}
				]
			}
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {
				"id": "test-id",
				"object": "chat.completion",
				"created": 1714349818,
				"model": "meta-llama/llama-4-maverick",
				"choices": [
					{
						"index": 0,
						"message": {
							"role": "assistant",
							"content": "I cannot determine a bias score for this content."
						},
						"finish_reason": "stop"
					}
				]
			}
2025/05/30 14:48:38 [DEBUG][LLM] Extracted content: I cannot determine a bias score for this content.
2025/05/30 14:48:38 [DEBUG][LLM] JSON parsing failed, raw content: I cannot determine a bias score for this content., error: <nil>
2025/05/30 14:48:38 [DEBUG][LLM] No score found in text
2025/05/30 14:48:38 [DEBUG][LLM] Parsed response for article 1, model meta-llama/llama-4-maverick: score=0.0000, confidence=0.0000, err=error parsing inner content JSON: %!w(<nil>)
=== RUN   TestHTTPLLMServiceWithMockServer/Rate_limit_error
    mock_http_service_test.go:25: Mock server URL: http://127.0.0.1:62861
2025/05/30 14:48:38 Attempting to load composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
2025/05/30 14:48:38 Successfully loaded and parsed composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
2025/05/30 14:48:38 [INFO] Rate limited on model meta-llama/llama-4-maverick, trying alternative model google/gemini-2.0-flash-001
2025/05/30 14:48:38 [INFO] Rate limited on model meta-llama/llama-4-maverick, trying alternative model openai/gpt-4.1-nano
=== RUN   TestHTTPLLMServiceWithMockServer/Server_error
    mock_http_service_test.go:25: Mock server URL: http://127.0.0.1:62863
=== RUN   TestHTTPLLMServiceWithMockServer/Malformed_JSON_response
    mock_http_service_test.go:25: Mock server URL: http://127.0.0.1:62865
2025/05/30 14:48:38 [DEBUG][LLM] Raw response for article 1, model meta-llama/llama-4-maverick: {"this is not valid json
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {"this is not valid json
2025/05/30 14:48:38 [DEBUG][LLM] Outer response is not valid JSON: {"this is not valid json
2025/05/30 14:48:38 [DEBUG][LLM] Error parsing outer JSON: unexpected end of JSON input
2025/05/30 14:48:38 [DEBUG][LLM] Parsed response for article 1, model meta-llama/llama-4-maverick: score=0.0000, confidence=0.0000, err=error parsing outer LLM API response JSON: unexpected end of JSON input
=== RUN   TestHTTPLLMServiceWithMockServer/Response_with_invalid_score_format
    mock_http_service_test.go:25: Mock server URL: http://127.0.0.1:62867
2025/05/30 14:48:38 [DEBUG][LLM] Raw response for article 1, model meta-llama/llama-4-maverick: {
				"id": "test-id",
				"object": "chat.completion",
				"created": 1714349818,
				"model": "meta-llama/llama-4-maverick",
				"choices": [
					{
						"index": 0,
						"message": {
							"role": "assistant",
							"content": "The article has a left-leaning bias. Score: not-a-number\\nConfidence: 0.9\\nReasoning: Invalid score format."
						},
						"finish_reason": "stop"
					}
				]
			}
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {
				"id": "test-id",
				"object": "chat.completion",
				"created": 1714349818,
				"model": "meta-llama/llama-4-maverick",
				"choices": [
					{
						"index": 0,
						"message": {
							"role": "assistant",
							"content": "The article has a left-leaning bias. Score: not-a-number\\nConfidence: 0.9\\nReasoning: Invalid score format."
						},
						"finish_reason": "stop"
					}
				]
			}
2025/05/30 14:48:38 [DEBUG][LLM] Extracted content: The article has a left-leaning bias. Score: not-a-number\nConfidence: 0.9\nReasoning: Invalid score format.
2025/05/30 14:48:38 [DEBUG][LLM] JSON parsing failed, raw content: The article has a left-leaning bias. Score: not-a-number\nConfidence: 0.9\nReasoning: Invalid score format., error: <nil>
2025/05/30 14:48:38 [DEBUG][LLM] No score found in text
2025/05/30 14:48:38 [DEBUG][LLM] Parsed response for article 1, model meta-llama/llama-4-maverick: score=0.0000, confidence=0.0000, err=error parsing inner content JSON: %!w(<nil>)
--- PASS: TestHTTPLLMServiceWithMockServer (0.15s)
    --- PASS: TestHTTPLLMServiceWithMockServer/Successful_response (0.13s)
    --- PASS: TestHTTPLLMServiceWithMockServer/Response_with_missing_content (0.00s)
    --- PASS: TestHTTPLLMServiceWithMockServer/Response_with_no_score (0.00s)
    --- PASS: TestHTTPLLMServiceWithMockServer/Rate_limit_error (0.01s)
    --- PASS: TestHTTPLLMServiceWithMockServer/Server_error (0.00s)
    --- PASS: TestHTTPLLMServiceWithMockServer/Malformed_JSON_response (0.00s)
    --- PASS: TestHTTPLLMServiceWithMockServer/Response_with_invalid_score_format (0.00s)
=== RUN   TestHTTPLLMServiceWithBackupKey
    mock_http_service_test.go:25: Mock server URL: http://127.0.0.1:62869
2025/05/30 14:48:38 [DEBUG][LLM] Raw response for article 1, model meta-llama/llama-4-maverick: {
				"id": "backup-response",
				"object": "chat.completion",
				"choices": [
					{
						"index": 0,
						"message": {
							"role": "assistant",
							"content": "The article has a right-leaning bias. Score: 0.6\\nConfidence: 0.85\\nReasoning: Backup key analysis."
						}
					}
				]
			}
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {
				"id": "backup-response",
				"object": "chat.completion",
				"choices": [
					{
						"index": 0,
						"message": {
							"role": "assistant",
							"content": "The article has a right-leaning bias. Score: 0.6\\nConfidence: 0.85\\nReasoning: Backup key analysis."
						}
					}
				]
			}
2025/05/30 14:48:38 [DEBUG][LLM] Extracted content: The article has a right-leaning bias. Score: 0.6\nConfidence: 0.85\nReasoning: Backup key analysis.
2025/05/30 14:48:38 [DEBUG][LLM] JSON parsing failed, raw content: The article has a right-leaning bias. Score: 0.6\nConfidence: 0.85\nReasoning: Backup key analysis., error: <nil>
2025/05/30 14:48:38 [DEBUG][LLM] Successfully parsed with regex: score=0.6000, confidence=0.8500
2025/05/30 14:48:38 [DEBUG][LLM] Parsed response for article 1, model meta-llama/llama-4-maverick: score=0.6000, confidence=0.8500, err=<nil>
--- PASS: TestHTTPLLMServiceWithBackupKey (0.00s)
=== RUN   TestHTTPLLMServiceWithServerErrors
=== RUN   TestHTTPLLMServiceWithServerErrors/Connection_reset_by_peer
    mock_http_service_test.go:25: Mock server URL: http://127.0.0.1:62871
=== RUN   TestHTTPLLMServiceWithServerErrors/Timeout
    mock_http_service_test.go:25: Mock server URL: http://127.0.0.1:62873
=== RUN   TestHTTPLLMServiceWithServerErrors/Empty_response
    mock_http_service_test.go:25: Mock server URL: http://127.0.0.1:62875
2025/05/30 14:48:38 [DEBUG][LLM] Raw response for article 1, model meta-llama/llama-4-maverick: 
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: 
2025/05/30 14:48:38 [DEBUG][LLM] Outer response is not valid JSON: 
2025/05/30 14:48:38 [DEBUG][LLM] Error parsing outer JSON: unexpected end of JSON input
2025/05/30 14:48:38 [DEBUG][LLM] Parsed response for article 1, model meta-llama/llama-4-maverick: score=0.0000, confidence=0.0000, err=error parsing outer LLM API response JSON: unexpected end of JSON input
=== RUN   TestHTTPLLMServiceWithServerErrors/Non-JSON_response
    mock_http_service_test.go:25: Mock server URL: http://127.0.0.1:62877
2025/05/30 14:48:38 [DEBUG][LLM] Raw response for article 1, model meta-llama/llama-4-maverick: This is not a JSON response
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: This is not a JSON response
2025/05/30 14:48:38 [DEBUG][LLM] Outer response is not valid JSON: This is not a JSON response
2025/05/30 14:48:38 [DEBUG][LLM] Error parsing outer JSON: invalid character 'T' looking for beginning of value
2025/05/30 14:48:38 [DEBUG][LLM] Parsed response for article 1, model meta-llama/llama-4-maverick: score=0.0000, confidence=0.0000, err=error parsing outer LLM API response JSON: invalid character 'T' looking for beginning of value
=== RUN   TestHTTPLLMServiceWithServerErrors/Missing_required_fields
    mock_http_service_test.go:25: Mock server URL: http://127.0.0.1:62879
2025/05/30 14:48:38 [DEBUG][LLM] Raw response for article 1, model meta-llama/llama-4-maverick: {"id": "missing-fields-response", "object": "chat.completion"}
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {"id": "missing-fields-response", "object": "chat.completion"}
2025/05/30 14:48:38 [DEBUG][LLM] No choices in outer response
2025/05/30 14:48:38 [DEBUG][LLM] Parsed response for article 1, model meta-llama/llama-4-maverick: score=0.0000, confidence=0.0000, err=no choices in outer LLM API response
--- PASS: TestHTTPLLMServiceWithServerErrors (0.01s)
    --- PASS: TestHTTPLLMServiceWithServerErrors/Connection_reset_by_peer (0.00s)
    --- PASS: TestHTTPLLMServiceWithServerErrors/Timeout (0.00s)
    --- PASS: TestHTTPLLMServiceWithServerErrors/Empty_response (0.00s)
    --- PASS: TestHTTPLLMServiceWithServerErrors/Non-JSON_response (0.00s)
    --- PASS: TestHTTPLLMServiceWithServerErrors/Missing_required_fields (0.00s)
=== RUN   TestMapModelToPerspective
=== RUN   TestMapModelToPerspective/meta-llama/llama-3-maverick:123
=== RUN   TestMapModelToPerspective/meta-llama/llama-3-maverick:latest
=== RUN   TestMapModelToPerspective/meta-llama/llama-3-maverick
=== RUN   TestMapModelToPerspective/google/gemini-pro
=== RUN   TestMapModelToPerspective/openai/gpt-4-turbo
=== RUN   TestMapModelToPerspective/Vendor/Legacy-Left
=== RUN   TestMapModelToPerspective/_Vendor_/_Mixed-Case-Center_
=== RUN   TestMapModelToPerspective/unknown/model
2025/05/30 14:48:38 Warning: Model 'unknown/model' not found in composite score configuration
=== RUN   TestMapModelToPerspective/:123
2025/05/30 14:48:38 Warning: Model ':123' not found in composite score configuration
=== RUN   TestMapModelToPerspective/#00
=== RUN   TestMapModelToPerspective/left
=== RUN   TestMapModelToPerspective/CENTER
=== RUN   TestMapModelToPerspective/meta-llama/llama-3-maverick:123:456
=== RUN   TestMapModelToPerspective/meta-llama/llama-3-maverick/extra
=== RUN   TestMapModelToPerspective/meta-llama/llama-3-maverick_
=== RUN   TestMapModelToPerspective/_meta-llama/llama-3-maverick
=== RUN   TestMapModelToPerspective/Nil_Config
2025/05/30 14:48:38 Error: CompositeScoreConfig is nil in MapModelToPerspective
=== RUN   TestMapModelToPerspective/Empty_Config
2025/05/30 14:48:38 Warning: Model 'google/gemini-pro' not found in composite score configuration
--- PASS: TestMapModelToPerspective (0.00s)
    --- PASS: TestMapModelToPerspective/meta-llama/llama-3-maverick:123 (0.00s)
    --- PASS: TestMapModelToPerspective/meta-llama/llama-3-maverick:latest (0.00s)
    --- PASS: TestMapModelToPerspective/meta-llama/llama-3-maverick (0.00s)
    --- PASS: TestMapModelToPerspective/google/gemini-pro (0.00s)
    --- PASS: TestMapModelToPerspective/openai/gpt-4-turbo (0.00s)
    --- PASS: TestMapModelToPerspective/Vendor/Legacy-Left (0.00s)
    --- PASS: TestMapModelToPerspective/_Vendor_/_Mixed-Case-Center_ (0.00s)
    --- PASS: TestMapModelToPerspective/unknown/model (0.00s)
    --- PASS: TestMapModelToPerspective/:123 (0.00s)
    --- PASS: TestMapModelToPerspective/#00 (0.00s)
    --- PASS: TestMapModelToPerspective/left (0.00s)
    --- PASS: TestMapModelToPerspective/CENTER (0.00s)
    --- PASS: TestMapModelToPerspective/meta-llama/llama-3-maverick:123:456 (0.00s)
    --- PASS: TestMapModelToPerspective/meta-llama/llama-3-maverick/extra (0.00s)
    --- PASS: TestMapModelToPerspective/meta-llama/llama-3-maverick_ (0.00s)
    --- PASS: TestMapModelToPerspective/_meta-llama/llama-3-maverick (0.00s)
    --- PASS: TestMapModelToPerspective/Nil_Config (0.00s)
    --- PASS: TestMapModelToPerspective/Empty_Config (0.00s)
=== RUN   TestComputeWithMixedModelNames
2025/05/30 14:48:38 Mapping model 'meta-llama/llama-3-maverick:123' (score -0.80) to perspective 'left'
2025/05/30 14:48:38 Mapping model 'google/gemini-pro' (score 0.10) to perspective 'center'
2025/05/30 14:48:38 Mapping model ' Vendor / Mixed-Case-Center ' (score 0.30) to perspective 'center'
2025/05/30 14:48:38 Mapping model 'openai/gpt-4-turbo' (score 0.90) to perspective 'right'
2025/05/30 14:48:38 Warning: Model 'unknown/model' not found in composite score configuration
2025/05/30 14:48:38 Skipping unknown model: unknown/model
2025/05/30 14:48:38 Perspective right has 1 models
2025/05/30 14:48:38 Perspective left has 1 models
2025/05/30 14:48:38 Perspective center has 2 models
2025/05/30 14:48:38 Candidates for left: 
2025/05/30 14:48:38   Model: meta-llama/llama-3-maverick:123, Score: -0.80, Metadata: {"confidence": 0.90}
2025/05/30 14:48:38 Candidates for center: 
2025/05/30 14:48:38   Model: google/gemini-pro, Score: 0.10, Metadata: {"confidence": 0.80}
2025/05/30 14:48:38   Model:  Vendor / Mixed-Case-Center , Score: 0.30, Metadata: {"confidence": 0.70}
2025/05/30 14:48:38 Candidates for right: 
2025/05/30 14:48:38   Model: openai/gpt-4-turbo, Score: 0.90, Metadata: {"confidence": 0.95}
2025/05/30 14:48:38 [DEBUG] Pre-Sum: validCount=3, len(validModels)=3
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Score map: map[center:0.1 left:-0.8 right:0.9]
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Valid models map: map[center:true left:true right:true]
2025/05/30 14:48:38 [DEBUG] Pre-Calc: sum=0.2000, weightedSum=0.2000, weightTotal=3.0000, actualValidCount=3
--- PASS: TestComputeWithMixedModelNames (0.00s)
=== RUN   TestModelNameNormalization
=== RUN   TestModelNameNormalization/MapModelToPerspective_Normalization
=== RUN   TestModelNameNormalization/CompositeScore_Calculation_Normalization
2025/05/30 14:48:38 Mapping model 'meta-llama/llama-3-maverick:latest' (score -0.50) to perspective 'left'
2025/05/30 14:48:38 Mapping model ' google/gemini-pro ' (score 0.00) to perspective 'center'
2025/05/30 14:48:38 Mapping model 'OPENAI/GPT-4-TURBO' (score 0.50) to perspective 'right'
2025/05/30 14:48:38 Perspective left has 1 models
2025/05/30 14:48:38 Perspective center has 1 models
2025/05/30 14:48:38 Perspective right has 1 models
2025/05/30 14:48:38 Candidates for left: 
2025/05/30 14:48:38   Model: meta-llama/llama-3-maverick:latest, Score: -0.50, Metadata: {"confidence": 0.90}
2025/05/30 14:48:38 Candidates for center: 
2025/05/30 14:48:38   Model:  google/gemini-pro , Score: 0.00, Metadata: {"confidence": 0.80}
2025/05/30 14:48:38 Candidates for right: 
2025/05/30 14:48:38   Model: OPENAI/GPT-4-TURBO, Score: 0.50, Metadata: {"confidence": 0.85}
2025/05/30 14:48:38 [DEBUG] Pre-Sum: validCount=3, len(validModels)=3
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Score map: map[center:0 left:-0.5 right:0.5]
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Valid models map: map[center:true left:true right:true]
2025/05/30 14:48:38 [DEBUG] Pre-Calc: sum=0.0000, weightedSum=0.0000, weightTotal=3.0000, actualValidCount=3
--- PASS: TestModelNameNormalization (0.00s)
    --- PASS: TestModelNameNormalization/MapModelToPerspective_Normalization (0.00s)
    --- PASS: TestModelNameNormalization/CompositeScore_Calculation_Normalization (0.00s)
=== RUN   TestModelNameFallbackLogic
2025/05/30 14:48:38 Mapping model 'left' (score -0.50) to perspective 'left'
2025/05/30 14:48:38 Mapping model 'center' (score 0.10) to perspective 'center'
2025/05/30 14:48:38 Mapping model 'right' (score 0.40) to perspective 'right'
2025/05/30 14:48:38 Perspective left has 1 models
2025/05/30 14:48:38 Perspective center has 1 models
2025/05/30 14:48:38 Perspective right has 1 models
2025/05/30 14:48:38 Candidates for left: 
2025/05/30 14:48:38   Model: left, Score: -0.50, Metadata: {"confidence":0.9}
2025/05/30 14:48:38 Candidates for center: 
2025/05/30 14:48:38   Model: center, Score: 0.10, Metadata: {"confidence":0.8}
2025/05/30 14:48:38 Candidates for right: 
2025/05/30 14:48:38   Model: right, Score: 0.40, Metadata: {"confidence":0.85}
2025/05/30 14:48:38 [DEBUG] Pre-Sum: validCount=3, len(validModels)=3
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Score map: map[center:0.1 left:-0.5 right:0.4]
2025/05/30 14:48:38 [DEBUG] Pre-Sum: Valid models map: map[center:true left:true right:true]
2025/05/30 14:48:38 [DEBUG] Pre-Calc: sum=0.0000, weightedSum=0.0000, weightTotal=3.0000, actualValidCount=3
--- PASS: TestModelNameFallbackLogic (0.00s)
=== RUN   TestModelMappingWithInvalidConfiguration
2025/05/30 14:48:38 Error: CompositeScoreConfig is nil in MapModelToPerspective
2025/05/30 14:48:38 Warning: Model 'any-model' not found in composite score configuration
--- PASS: TestModelMappingWithInvalidConfiguration (0.00s)
=== RUN   TestModelNameEdgeCases
--- PASS: TestModelNameEdgeCases (0.00s)
=== RUN   TestOpenRouterErrorTypes
=== RUN   TestOpenRouterErrorTypes/Rate_Limit_Error
=== RUN   TestOpenRouterErrorTypes/Authentication_Error
=== RUN   TestOpenRouterErrorTypes/Credits_Exhausted
=== RUN   TestOpenRouterErrorTypes/Streaming_Error
=== RUN   TestOpenRouterErrorTypes/Server_Error
=== RUN   TestOpenRouterErrorTypes/Malformed_JSON
--- PASS: TestOpenRouterErrorTypes (0.01s)
    --- PASS: TestOpenRouterErrorTypes/Rate_Limit_Error (0.00s)
    --- PASS: TestOpenRouterErrorTypes/Authentication_Error (0.00s)
    --- PASS: TestOpenRouterErrorTypes/Credits_Exhausted (0.00s)
    --- PASS: TestOpenRouterErrorTypes/Streaming_Error (0.00s)
    --- PASS: TestOpenRouterErrorTypes/Server_Error (0.00s)
    --- PASS: TestOpenRouterErrorTypes/Malformed_JSON (0.00s)
=== RUN   TestSanitizeResponseFunction
=== RUN   TestSanitizeResponseFunction/OpenRouter_API_Key
=== RUN   TestSanitizeResponseFunction/OpenAI_API_Key
=== RUN   TestSanitizeResponseFunction/Multiple_API_Keys
=== RUN   TestSanitizeResponseFunction/No_API_Keys
=== RUN   TestSanitizeResponseFunction/Error_with_API_Key_in_Message
--- PASS: TestSanitizeResponseFunction (0.00s)
    --- PASS: TestSanitizeResponseFunction/OpenRouter_API_Key (0.00s)
    --- PASS: TestSanitizeResponseFunction/OpenAI_API_Key (0.00s)
    --- PASS: TestSanitizeResponseFunction/Multiple_API_Keys (0.00s)
    --- PASS: TestSanitizeResponseFunction/No_API_Keys (0.00s)
    --- PASS: TestSanitizeResponseFunction/Error_with_API_Key_in_Message (0.00s)
=== RUN   TestErrorTypes
--- PASS: TestErrorTypes (0.00s)
=== RUN   TestErrorPropagation
=== RUN   TestErrorPropagation/Rate_Limit_Error
=== RUN   TestErrorPropagation/Authentication_Error
--- PASS: TestErrorPropagation (0.00s)
    --- PASS: TestErrorPropagation/Rate_Limit_Error (0.00s)
    --- PASS: TestErrorPropagation/Authentication_Error (0.00s)
=== RUN   TestParseNestedLLMJSONResponse_Valid
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {"choices":[{"message":{"content":"{\"score\":0.5,\"explanation\":\"ok\",\"confidence\":0.8}"}}]}
2025/05/30 14:48:38 [DEBUG][LLM] Extracted content: {"score":0.5,"explanation":"ok","confidence":0.8}
2025/05/30 14:48:38 [DEBUG][LLM] Successfully parsed as JSON: score=0.5000, confidence=0.8000
--- PASS: TestParseNestedLLMJSONResponse_Valid (0.00s)
=== RUN   TestParseNestedLLMJSONResponse_Backticks
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {"choices":[{"message":{"content":"```json {\"score\":1.0,\"explanation\":\"text\",\"confidence\":0.9} ```"}}]}
2025/05/30 14:48:38 [DEBUG][LLM] Extracted content: ```json {"score":1.0,"explanation":"text","confidence":0.9} ```
2025/05/30 14:48:38 [DEBUG][LLM] After backtick stripping: {"score":1.0,"explanation":"text","confidence":0.9}
2025/05/30 14:48:38 [DEBUG][LLM] Successfully parsed as JSON: score=1.0000, confidence=0.9000
--- PASS: TestParseNestedLLMJSONResponse_Backticks (0.00s)
=== RUN   TestParseNestedLLMJSONResponse_NoChoices
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {"choices":[]}
2025/05/30 14:48:38 [DEBUG][LLM] No choices in outer response
--- PASS: TestParseNestedLLMJSONResponse_NoChoices (0.00s)
=== RUN   TestParseNestedLLMJSONResponse_InvalidOuterJSON
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: not json
2025/05/30 14:48:38 [DEBUG][LLM] Outer response is not valid JSON: not json
2025/05/30 14:48:38 [DEBUG][LLM] Error parsing outer JSON: invalid character 'o' in literal null (expecting 'u')
--- PASS: TestParseNestedLLMJSONResponse_InvalidOuterJSON (0.00s)
=== RUN   TestNewProgressManager
--- PASS: TestNewProgressManager (0.00s)
=== RUN   TestProgressManagerSetGet
--- PASS: TestProgressManagerSetGet (0.00s)
=== RUN   TestManualCleanup
--- PASS: TestManualCleanup (0.00s)
=== RUN   TestProgressManager_UpdateProgressWithLLMError
=== RUN   TestProgressManager_UpdateProgressWithLLMError/LLM_Rate_Limit_Error
=== RUN   TestProgressManager_UpdateProgressWithLLMError/LLM_Authentication_Error
=== RUN   TestProgressManager_UpdateProgressWithLLMError/LLM_Credits_Error
=== RUN   TestProgressManager_UpdateProgressWithLLMError/LLM_Streaming_Error
=== RUN   TestProgressManager_UpdateProgressWithLLMError/LLM_Unknown_Error
=== RUN   TestProgressManager_UpdateProgressWithLLMError/Normal_Error
=== RUN   TestProgressManager_UpdateProgressWithLLMError/No_Error
--- PASS: TestProgressManager_UpdateProgressWithLLMError (0.00s)
    --- PASS: TestProgressManager_UpdateProgressWithLLMError/LLM_Rate_Limit_Error (0.00s)
    --- PASS: TestProgressManager_UpdateProgressWithLLMError/LLM_Authentication_Error (0.00s)
    --- PASS: TestProgressManager_UpdateProgressWithLLMError/LLM_Credits_Error (0.00s)
    --- PASS: TestProgressManager_UpdateProgressWithLLMError/LLM_Streaming_Error (0.00s)
    --- PASS: TestProgressManager_UpdateProgressWithLLMError/LLM_Unknown_Error (0.00s)
    --- PASS: TestProgressManager_UpdateProgressWithLLMError/Normal_Error (0.00s)
    --- PASS: TestProgressManager_UpdateProgressWithLLMError/No_Error (0.00s)
=== RUN   TestProgressManager_ClearProgress
--- PASS: TestProgressManager_ClearProgress (0.00s)
=== RUN   TestProgressManager_MultipleArticles
--- PASS: TestProgressManager_MultipleArticles (0.00s)
=== RUN   TestProgressManager_ExportStateWithErrors
--- PASS: TestProgressManager_ExportStateWithErrors (0.00s)
=== RUN   TestRateLimitFallback
--- PASS: TestRateLimitFallback (0.00s)
=== RUN   TestBothKeysRateLimited
--- PASS: TestBothKeysRateLimited (0.00s)
=== RUN   TestPrimaryKeyWorking
--- PASS: TestPrimaryKeyWorking (0.00s)
=== RUN   TestNoBackupKey
--- PASS: TestNoBackupKey (0.00s)
=== RUN   TestHTTPLLMServiceRateLimiting
2025/05/30 14:48:38 [DEBUG][LLM] Raw response for article 1, model test-model: {
				"choices": [
					{
						"message": {
							"content": "{\"score\": 0.5, \"explanation\": \"Test\", \"confidence\": 0.8}"
						}
					}
				]
			}
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {
				"choices": [
					{
						"message": {
							"content": "{\"score\": 0.5, \"explanation\": \"Test\", \"confidence\": 0.8}"
						}
					}
				]
			}
2025/05/30 14:48:38 [DEBUG][LLM] Extracted content: {"score": 0.5, "explanation": "Test", "confidence": 0.8}
2025/05/30 14:48:38 [DEBUG][LLM] Successfully parsed as JSON: score=0.5000, confidence=0.8000
2025/05/30 14:48:38 [DEBUG][LLM] Parsed response for article 1, model test-model: score=0.5000, confidence=0.8000, err=<nil>
--- PASS: TestHTTPLLMServiceRateLimiting (0.00s)
=== RUN   TestBothHTTPKeysRateLimited
2025/05/30 14:48:38 Attempting to load composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
2025/05/30 14:48:38 Successfully loaded and parsed composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
2025/05/30 14:48:38 [INFO] Rate limited on model test-model, trying alternative model meta-llama/llama-4-maverick
2025/05/30 14:48:38 [INFO] Rate limited on model test-model, trying alternative model google/gemini-2.0-flash-001
2025/05/30 14:48:38 [INFO] Rate limited on model test-model, trying alternative model openai/gpt-4.1-nano
--- PASS: TestBothHTTPKeysRateLimited (0.00s)
=== RUN   TestLLMClientRateLimitFallback
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] ScoreWithModel called for article 5 with model test-model
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] Test mode detected (nil db), returning score without storage: 0.5000
--- PASS: TestLLMClientRateLimitFallback (0.00s)
=== RUN   TestLLMClientBothKeysRateLimited
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] ScoreWithModel called for article 6 with model test-model
--- PASS: TestLLMClientBothKeysRateLimited (0.00s)
=== RUN   TestInvalidContentJSON
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {"choices":[{"message":{"content":"This is not valid JSON"}}]}
2025/05/30 14:48:38 [DEBUG][LLM] Extracted content: This is not valid JSON
2025/05/30 14:48:38 [DEBUG][LLM] JSON parsing failed, raw content: This is not valid JSON, error: <nil>
2025/05/30 14:48:38 [DEBUG][LLM] No score found in text
--- PASS: TestInvalidContentJSON (0.00s)
=== RUN   TestMalformedJSONWithBackticks
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {"choices":[{"message":{"content":"```json {\\\"score\\\":1.0,\\\"explanation\\\":\\\"test\\\",\\\"confidence\\\":INVALID} ```"}}]}
2025/05/30 14:48:38 [DEBUG][LLM] Extracted content: ```json {\"score\":1.0,\"explanation\":\"test\",\"confidence\":INVALID} ```
2025/05/30 14:48:38 [DEBUG][LLM] After backtick stripping: {\"score\":1.0,\"explanation\":\"test\",\"confidence\":INVALID}
2025/05/30 14:48:38 [DEBUG][LLM] JSON parsing failed, raw content: {\"score\":1.0,\"explanation\":\"test\",\"confidence\":INVALID}, error: <nil>
2025/05/30 14:48:38 [DEBUG][LLM] No score found in text
--- PASS: TestMalformedJSONWithBackticks (0.00s)
=== RUN   TestMissingRequiredFields
=== RUN   TestMissingRequiredFields/missing_score
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {"choices":[{"message":{"content":"{\"explanation\":\"test\",\"confidence\":0.9}"}}]}
2025/05/30 14:48:38 [DEBUG][LLM] Extracted content: {"explanation":"test","confidence":0.9}
2025/05/30 14:48:38 [DEBUG][LLM] Successfully parsed as JSON: score=0.0000, confidence=0.9000
=== RUN   TestMissingRequiredFields/missing_explanation
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {"choices":[{"message":{"content":"{\"score\":0.5,\"confidence\":0.9}"}}]}
2025/05/30 14:48:38 [DEBUG][LLM] Extracted content: {"score":0.5,"confidence":0.9}
2025/05/30 14:48:38 [DEBUG][LLM] Successfully parsed as JSON: score=0.5000, confidence=0.9000
=== RUN   TestMissingRequiredFields/missing_confidence
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {"choices":[{"message":{"content":"{\"score\":0.5,\"explanation\":\"test\"}"}}]}
2025/05/30 14:48:38 [DEBUG][LLM] Extracted content: {"score":0.5,"explanation":"test"}
2025/05/30 14:48:38 [DEBUG][LLM] Successfully parsed as JSON: score=0.5000, confidence=0.0000
--- PASS: TestMissingRequiredFields (0.00s)
    --- PASS: TestMissingRequiredFields/missing_score (0.00s)
    --- PASS: TestMissingRequiredFields/missing_explanation (0.00s)
    --- PASS: TestMissingRequiredFields/missing_confidence (0.00s)
=== RUN   TestZeroConfidenceHandling
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {"choices":[{"message":{"content":"{\"score\":0.5,\"explanation\":\"test\",\"confidence\":0.0}"}}]}
2025/05/30 14:48:38 [DEBUG][LLM] Extracted content: {"score":0.5,"explanation":"test","confidence":0.0}
2025/05/30 14:48:38 [DEBUG][LLM] Successfully parsed as JSON: score=0.5000, confidence=0.0000
--- PASS: TestZeroConfidenceHandling (0.00s)
=== RUN   TestInvalidScoreValues
=== RUN   TestInvalidScoreValues/out_of_range_negative
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {"choices":[{"message":{"content":"{\"score\":-5.0,\"explanation\":\"test\",\"confidence\":0.9}"}}]}
2025/05/30 14:48:38 [DEBUG][LLM] Extracted content: {"score":-5.0,"explanation":"test","confidence":0.9}
2025/05/30 14:48:38 [DEBUG][LLM] Successfully parsed as JSON: score=-5.0000, confidence=0.9000
=== RUN   TestInvalidScoreValues/out_of_range_positive
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {"choices":[{"message":{"content":"{\"score\":5.0,\"explanation\":\"test\",\"confidence\":0.9}"}}]}
2025/05/30 14:48:38 [DEBUG][LLM] Extracted content: {"score":5.0,"explanation":"test","confidence":0.9}
2025/05/30 14:48:38 [DEBUG][LLM] Successfully parsed as JSON: score=5.0000, confidence=0.9000
--- PASS: TestInvalidScoreValues (0.00s)
    --- PASS: TestInvalidScoreValues/out_of_range_negative (0.00s)
    --- PASS: TestInvalidScoreValues/out_of_range_positive (0.00s)
=== RUN   TestParseWithExtraFields
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {"choices":[{"message":{"content":"{\"score\":0.5,\"explanation\":\"test\",\"confidence\":0.9,\"extraField\":\"value\"}"}}]}
2025/05/30 14:48:38 [DEBUG][LLM] Extracted content: {"score":0.5,"explanation":"test","confidence":0.9,"extraField":"value"}
2025/05/30 14:48:38 [DEBUG][LLM] Successfully parsed as JSON: score=0.5000, confidence=0.9000
--- PASS: TestParseWithExtraFields (0.00s)
=== RUN   TestEmptyContentResponse
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {"choices":[{"message":{"content":""}}]}
2025/05/30 14:48:38 [DEBUG][LLM] Extracted content: 
2025/05/30 14:48:38 [DEBUG][LLM] JSON parsing failed, raw content: , error: <nil>
2025/05/30 14:48:38 [DEBUG][LLM] No score found in text
--- PASS: TestEmptyContentResponse (0.00s)
=== RUN   TestPartialResponse
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {"choices":[{"message":{"content":"{\"score\":0.5,\"explanation\":
2025/05/30 14:48:38 [DEBUG][LLM] Outer response is not valid JSON: {"choices":[{"message":{"content":"{\"score\":0.5,\"explanation\":
2025/05/30 14:48:38 [DEBUG][LLM] Error parsing outer JSON: unexpected end of JSON input
--- PASS: TestPartialResponse (0.00s)
=== RUN   TestHTTPAPIErrorHandling
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] ScoreWithModel called for article 1 with model test-model
--- PASS: TestHTTPAPIErrorHandling (0.00s)
=== RUN   TestAPIErrorResponseHandling
--- PASS: TestAPIErrorResponseHandling (0.00s)
=== RUN   TestErrorRetryLogic
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] ScoreWithModel called for article 1 with model test-model
--- PASS: TestErrorRetryLogic (0.00s)
=== RUN   TestCompletelyEmptyResponse
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {"choices":[{"message":{"content":"{}"}}]}
2025/05/30 14:48:38 [DEBUG][LLM] Extracted content: {}
2025/05/30 14:48:38 [DEBUG][LLM] Successfully parsed as JSON: score=0.0000, confidence=0.0000
2025/05/30 14:48:38 [DEBUG][LLM] Starting to parse response: {"choices":[{"message":{"content":""}}]}
2025/05/30 14:48:38 [DEBUG][LLM] Extracted content: 
2025/05/30 14:48:38 [DEBUG][LLM] JSON parsing failed, raw content: , error: <nil>
2025/05/30 14:48:38 [DEBUG][LLM] No score found in text
--- PASS: TestCompletelyEmptyResponse (0.00s)
=== RUN   TestDefaultScoreCalculator_CalculateScore
=== RUN   TestDefaultScoreCalculator_CalculateScore/all_perspectives_present
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] Calculated composite score: -0.0667 with confidence 0.8000 from 3 valid scores
=== RUN   TestDefaultScoreCalculator_CalculateScore/missing_center_perspective
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] Calculated composite score: -0.1000 with confidence 0.8000 from 2 valid scores
=== RUN   TestDefaultScoreCalculator_CalculateScore/score_out_of_range
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] Ignoring invalid score -2.00 for model left
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] Ignoring invalid score 1.50 for model right
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] Calculated composite score: 0.0000 with confidence 0.8000 from 1 valid scores
=== RUN   TestDefaultScoreCalculator_CalculateScore/no_scores
=== RUN   TestDefaultScoreCalculator_CalculateScore/all_invalid_scores
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] Ignoring invalid score NaN for model left
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] Ignoring invalid score +Inf for model center
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] Ignoring invalid score -2.00 for model right
=== RUN   TestDefaultScoreCalculator_CalculateScore/all_zero_confidence
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] No confidence field in metadata, defaulting to 0.0
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] No confidence field in metadata, defaulting to 0.0
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] No confidence field in metadata, defaulting to 0.0
=== RUN   TestDefaultScoreCalculator_CalculateScore/invalid_model_names
2025/05/30 14:48:38 Warning: Model 'unknown' not found in composite score configuration
2025/05/30 14:48:38 Warning: Model 'unknown' not found in composite score configuration
=== RUN   TestDefaultScoreCalculator_CalculateScore/partial_invalid_metadata
2025/05/30 14:48:38 [ERROR][CONFIDENCE] Failed to parse metadata JSON: invalid character 'i' looking for beginning of value
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] No confidence field in metadata, defaulting to 0.0
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] Calculated composite score: 0.2500 with confidence 0.7500 from 2 valid scores
=== RUN   TestDefaultScoreCalculator_CalculateScore/nil_config
=== RUN   TestDefaultScoreCalculator_CalculateScore/all_perspectives_with_equal_scores
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] Calculated composite score: 0.5000 with confidence 0.9000 from 3 valid scores
=== RUN   TestDefaultScoreCalculator_CalculateScore/extreme_opposing_scores
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] Calculated composite score: 0.0000 with confidence 1.0000 from 3 valid scores
=== RUN   TestDefaultScoreCalculator_CalculateScore/all_zeroes
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] Calculated composite score: 0.0000 with confidence 0.5000 from 3 valid scores
=== RUN   TestDefaultScoreCalculator_CalculateScore/missing_confidence_fields
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] No confidence field in metadata, defaulting to 0.0
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] No confidence field in metadata, defaulting to 0.0
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] No confidence field in metadata, defaulting to 0.0
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] No confidence field in metadata, defaulting to 0.0
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] No confidence field in metadata, defaulting to 0.0
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] No confidence field in metadata, defaulting to 0.0
=== RUN   TestDefaultScoreCalculator_CalculateScore/malformed_metadata_mixed_with_valid
2025/05/30 14:48:38 [ERROR][CONFIDENCE] Failed to parse metadata JSON: unexpected end of JSON input
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] No confidence field in metadata, defaulting to 0.0
2025/05/30 14:48:38 [ERROR][CONFIDENCE] Failed to parse metadata JSON: invalid character ']' looking for beginning of object key string
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] No confidence field in metadata, defaulting to 0.0
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] Calculated composite score: 0.0000 with confidence 0.8000 from 1 valid scores
=== RUN   TestDefaultScoreCalculator_CalculateScore/case_insensitive_model_names
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] Calculated composite score: 0.0000 with confidence 0.8000 from 3 valid scores
=== RUN   TestDefaultScoreCalculator_CalculateScore/duplicate_perspectives_(should_use_last)
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] Calculated composite score: -0.2000 with confidence 0.7750 from 4 valid scores
--- PASS: TestDefaultScoreCalculator_CalculateScore (0.00s)
    --- PASS: TestDefaultScoreCalculator_CalculateScore/all_perspectives_present (0.00s)
    --- PASS: TestDefaultScoreCalculator_CalculateScore/missing_center_perspective (0.00s)
    --- PASS: TestDefaultScoreCalculator_CalculateScore/score_out_of_range (0.00s)
    --- PASS: TestDefaultScoreCalculator_CalculateScore/no_scores (0.00s)
    --- PASS: TestDefaultScoreCalculator_CalculateScore/all_invalid_scores (0.00s)
    --- PASS: TestDefaultScoreCalculator_CalculateScore/all_zero_confidence (0.00s)
    --- PASS: TestDefaultScoreCalculator_CalculateScore/invalid_model_names (0.00s)
    --- PASS: TestDefaultScoreCalculator_CalculateScore/partial_invalid_metadata (0.00s)
    --- PASS: TestDefaultScoreCalculator_CalculateScore/nil_config (0.00s)
    --- PASS: TestDefaultScoreCalculator_CalculateScore/all_perspectives_with_equal_scores (0.00s)
    --- PASS: TestDefaultScoreCalculator_CalculateScore/extreme_opposing_scores (0.00s)
    --- PASS: TestDefaultScoreCalculator_CalculateScore/all_zeroes (0.00s)
    --- PASS: TestDefaultScoreCalculator_CalculateScore/missing_confidence_fields (0.00s)
    --- PASS: TestDefaultScoreCalculator_CalculateScore/malformed_metadata_mixed_with_valid (0.00s)
    --- PASS: TestDefaultScoreCalculator_CalculateScore/case_insensitive_model_names (0.00s)
    --- PASS: TestDefaultScoreCalculator_CalculateScore/duplicate_perspectives_(should_use_last) (0.00s)
=== RUN   TestDefaultScoreCalculator_GetPerspective
=== RUN   TestDefaultScoreCalculator_GetPerspective/left_lowercase
=== RUN   TestDefaultScoreCalculator_GetPerspective/right_lowercase
=== RUN   TestDefaultScoreCalculator_GetPerspective/center_lowercase
=== RUN   TestDefaultScoreCalculator_GetPerspective/left_uppercase
=== RUN   TestDefaultScoreCalculator_GetPerspective/right_uppercase
=== RUN   TestDefaultScoreCalculator_GetPerspective/center_mixed_case
=== RUN   TestDefaultScoreCalculator_GetPerspective/unknown_model
2025/05/30 14:48:38 Warning: Model 'unknown' not found in composite score configuration
=== RUN   TestDefaultScoreCalculator_GetPerspective/empty_string
=== RUN   TestDefaultScoreCalculator_GetPerspective/label_constant_left
=== RUN   TestDefaultScoreCalculator_GetPerspective/label_constant_right
--- PASS: TestDefaultScoreCalculator_GetPerspective (0.00s)
    --- PASS: TestDefaultScoreCalculator_GetPerspective/left_lowercase (0.00s)
    --- PASS: TestDefaultScoreCalculator_GetPerspective/right_lowercase (0.00s)
    --- PASS: TestDefaultScoreCalculator_GetPerspective/center_lowercase (0.00s)
    --- PASS: TestDefaultScoreCalculator_GetPerspective/left_uppercase (0.00s)
    --- PASS: TestDefaultScoreCalculator_GetPerspective/right_uppercase (0.00s)
    --- PASS: TestDefaultScoreCalculator_GetPerspective/center_mixed_case (0.00s)
    --- PASS: TestDefaultScoreCalculator_GetPerspective/unknown_model (0.00s)
    --- PASS: TestDefaultScoreCalculator_GetPerspective/empty_string (0.00s)
    --- PASS: TestDefaultScoreCalculator_GetPerspective/label_constant_left (0.00s)
    --- PASS: TestDefaultScoreCalculator_GetPerspective/label_constant_right (0.00s)
=== RUN   TestDefaultScoreCalculator_ExtractConfidence
=== RUN   TestDefaultScoreCalculator_ExtractConfidence/valid_confidence
=== RUN   TestDefaultScoreCalculator_ExtractConfidence/no_confidence_field
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] No confidence field in metadata, defaulting to 0.0
=== RUN   TestDefaultScoreCalculator_ExtractConfidence/invalid_json
2025/05/30 14:48:38 [ERROR][CONFIDENCE] Failed to parse metadata JSON: invalid character 'i' looking for beginning of object key string
=== RUN   TestDefaultScoreCalculator_ExtractConfidence/empty_string
2025/05/30 14:48:38 [ERROR][CONFIDENCE] Failed to parse metadata JSON: unexpected end of JSON input
=== RUN   TestDefaultScoreCalculator_ExtractConfidence/null_confidence
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] Null confidence value, defaulting to 0.0
=== RUN   TestDefaultScoreCalculator_ExtractConfidence/string_confidence
2025/05/30 14:48:38 [WARN][CONFIDENCE] String confidence value '0.9', defaulting to 0.0
=== RUN   TestDefaultScoreCalculator_ExtractConfidence/integer_confidence
=== RUN   TestDefaultScoreCalculator_ExtractConfidence/nested_confidence
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] No confidence field in metadata, defaulting to 0.0
--- PASS: TestDefaultScoreCalculator_ExtractConfidence (0.00s)
    --- PASS: TestDefaultScoreCalculator_ExtractConfidence/valid_confidence (0.00s)
    --- PASS: TestDefaultScoreCalculator_ExtractConfidence/no_confidence_field (0.00s)
    --- PASS: TestDefaultScoreCalculator_ExtractConfidence/invalid_json (0.00s)
    --- PASS: TestDefaultScoreCalculator_ExtractConfidence/empty_string (0.00s)
    --- PASS: TestDefaultScoreCalculator_ExtractConfidence/null_confidence (0.00s)
    --- PASS: TestDefaultScoreCalculator_ExtractConfidence/string_confidence (0.00s)
    --- PASS: TestDefaultScoreCalculator_ExtractConfidence/integer_confidence (0.00s)
    --- PASS: TestDefaultScoreCalculator_ExtractConfidence/nested_confidence (0.00s)
=== RUN   TestDefaultScoreCalculator_InitializeMaps
--- PASS: TestDefaultScoreCalculator_InitializeMaps (0.00s)
=== RUN   TestIntegrationUpdateArticleScore
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] UpdateArticleScoreLLM called with articleID=123, score=0.1000, confidence=0.8000
2025/05/30 14:48:38 [DEBUG][CONFIDENCE] UpdateArticleScoreLLM affected 1 rows for articleID=123
2025/05/30 14:48:38 [INFO] Updated status for article ID 123 to 'scored'
2025/05/30 14:48:38 [INFO] ScoreManager: ArticleID 123: Score updated successfully, status set to scored.
--- PASS: TestIntegrationUpdateArticleScore (0.00s)
=== RUN   TestIntegrationUpdateArticleScore_ErrAllPerspectivesInvalid
2025/05/30 14:48:38 [ERROR] ScoreManager: ArticleID 456: all perspectives returned invalid scores. Score will not be updated.
2025/05/30 14:48:38 [INFO] Updated status for article ID 456 to 'failed_all_invalid'
2025/05/30 14:48:38 [DEBUG] ScoreManager: ArticleID 456: Successfully updated status to failed_all_invalid.
2025/05/30 14:48:38 [DEBUG] ScoreManager: ArticleID 456: Returning ErrAllPerspectivesInvalid error now.
--- PASS: TestIntegrationUpdateArticleScore_ErrAllPerspectivesInvalid (0.00s)
=== RUN   TestIntegrationTrackProgress
--- PASS: TestIntegrationTrackProgress (0.00s)
=== RUN   TestScoreManagerIntegrationUpdateArticleScoreZeroConfidenceError
2025/05/30 14:48:38 Critical warning: All 2 non-ensemble LLM models returned zero confidence
2025/05/30 14:48:38 [ERROR] ArticleID 1: All LLMs returned zero confidence - this is a serious error: all LLMs returned empty or zero-confidence responses
2025/05/30 14:48:38 [ERROR] Failed to update article status for ID 1 to 'failed_zero_confidence': all expectations were already fulfilled, call to ExecQuery 'UPDATE articles SET status = ? WHERE id = ?' with args [{Name: Ordinal:1 Value:failed_zero_confidence} {Name: Ordinal:2 Value:1}] was not expected
2025/05/30 14:48:38 [ERROR] ScoreManager: ArticleID 1: Failed to update article status to failed_zero_confidence after zero confidence error: failed to update article status for ID 1: internal
--- PASS: TestScoreManagerIntegrationUpdateArticleScoreZeroConfidenceError (0.00s)
=== RUN   TestScoreManagerIntegrationCalculateScoreError
2025/05/30 14:48:38 [ERROR] ScoreManager: ArticleID 1: Unexpected error calculating score: some generic calculation error. Score will not be updated.
2025/05/30 14:48:38 [INFO] Updated status for article ID 1 to 'failed_error'
--- PASS: TestScoreManagerIntegrationCalculateScoreError (0.00s)
=== RUN   TestNewScoreManager
--- PASS: TestNewScoreManager (0.00s)
=== RUN   TestInvalidateScoreCache
--- PASS: TestInvalidateScoreCache (0.00s)
=== RUN   TestSetGetProgress
--- PASS: TestSetGetProgress (0.00s)
=== RUN   TestGetProgressWithNilManager
--- PASS: TestGetProgressWithNilManager (0.00s)
=== RUN   TestUpdateArticleScoreSuccess
--- PASS: TestUpdateArticleScoreSuccess (0.00s)
=== RUN   TestUpdateArticleScoreCalculationError
--- PASS: TestUpdateArticleScoreCalculationError (0.00s)
=== RUN   TestUpdateArticleScoreDBError
--- PASS: TestUpdateArticleScoreDBError (0.00s)
=== RUN   TestUpdateArticleScoreCommitError
--- PASS: TestUpdateArticleScoreCommitError (0.00s)
=== RUN   TestUpdateArticleScoreCacheInvalidation
--- PASS: TestUpdateArticleScoreCacheInvalidation (0.00s)
=== RUN   TestScoreManagerWithAllZeroConfidenceScores
2025/05/30 14:48:38 Critical warning: All 4 non-ensemble LLM models returned zero confidence
--- PASS: TestScoreManagerWithAllZeroConfidenceScores (0.00s)
=== RUN   TestCheckForAllZeroResponses
=== RUN   TestCheckForAllZeroResponses/All_zero_confidence
2025/05/30 14:48:38 Critical warning: All 4 non-ensemble LLM models returned zero confidence
=== RUN   TestCheckForAllZeroResponses/One_non-zero_confidence
--- PASS: TestCheckForAllZeroResponses (0.00s)
    --- PASS: TestCheckForAllZeroResponses/All_zero_confidence (0.00s)
    --- PASS: TestCheckForAllZeroResponses/One_non-zero_confidence (0.00s)
=== RUN   TestHTTPLLMServiceUsesOpenRouterURL
--- PASS: TestHTTPLLMServiceUsesOpenRouterURL (0.00s)
=== RUN   TestLLMAPIError_Error
=== RUN   TestLLMAPIError_Error/Standard_API_error
=== RUN   TestLLMAPIError_Error/Rate_limit_error
=== RUN   TestLLMAPIError_Error/Server_error
=== RUN   TestLLMAPIError_Error/Empty_message
--- PASS: TestLLMAPIError_Error (0.00s)
    --- PASS: TestLLMAPIError_Error/Standard_API_error (0.00s)
    --- PASS: TestLLMAPIError_Error/Rate_limit_error (0.00s)
    --- PASS: TestLLMAPIError_Error/Server_error (0.00s)
    --- PASS: TestLLMAPIError_Error/Empty_message (0.00s)
=== RUN   TestOpenRouterErrorHandling
=== RUN   TestOpenRouterErrorHandling/Rate_Limit_Error
2025/05/30 14:48:38 Attempting to load composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
2025/05/30 14:48:38 Successfully loaded and parsed composite score config from: D:\Dev\newbalancer_go\configs\composite_score_config.json
2025/05/30 14:48:38 [INFO] Rate limited on model test-model, trying alternative model meta-llama/llama-4-maverick
2025/05/30 14:48:38 [INFO] Rate limited on model test-model, trying alternative model google/gemini-2.0-flash-001
2025/05/30 14:48:38 [INFO] Rate limited on model test-model, trying alternative model openai/gpt-4.1-nano
=== RUN   TestOpenRouterErrorHandling/Authentication_Error
=== RUN   TestOpenRouterErrorHandling/Credits_Exhausted
=== RUN   TestOpenRouterErrorHandling/Server_Error
=== RUN   TestOpenRouterErrorHandling/Malformed_JSON
--- PASS: TestOpenRouterErrorHandling (0.01s)
    --- PASS: TestOpenRouterErrorHandling/Rate_Limit_Error (0.00s)
    --- PASS: TestOpenRouterErrorHandling/Authentication_Error (0.00s)
    --- PASS: TestOpenRouterErrorHandling/Credits_Exhausted (0.00s)
    --- PASS: TestOpenRouterErrorHandling/Server_Error (0.00s)
    --- PASS: TestOpenRouterErrorHandling/Malformed_JSON (0.00s)
=== RUN   TestSanitizeResponse
=== RUN   TestSanitizeResponse/OpenRouter_API_Key
=== RUN   TestSanitizeResponse/OpenAI_API_Key
=== RUN   TestSanitizeResponse/Multiple_API_Keys
=== RUN   TestSanitizeResponse/No_API_Keys
--- PASS: TestSanitizeResponse (0.00s)
    --- PASS: TestSanitizeResponse/OpenRouter_API_Key (0.00s)
    --- PASS: TestSanitizeResponse/OpenAI_API_Key (0.00s)
    --- PASS: TestSanitizeResponse/Multiple_API_Keys (0.00s)
    --- PASS: TestSanitizeResponse/No_API_Keys (0.00s)
=== RUN   TestFormatHTTPError
=== RUN   TestFormatHTTPError/Rate_Limit_With_Details
=== RUN   TestFormatHTTPError/Empty_Response
=== RUN   TestFormatHTTPError/Complex_Nested_Error
--- PASS: TestFormatHTTPError (0.00s)
    --- PASS: TestFormatHTTPError/Rate_Limit_With_Details (0.00s)
    --- PASS: TestFormatHTTPError/Empty_Response (0.00s)
    --- PASS: TestFormatHTTPError/Complex_Nested_Error (0.00s)
=== RUN   TestMin
=== RUN   TestMin/First_value_smaller
=== RUN   TestMin/Second_value_smaller
=== RUN   TestMin/Equal_values
=== RUN   TestMin/Negative_values
=== RUN   TestMin/Zero_and_positive
=== RUN   TestMin/Negative_and_zero
--- PASS: TestMin (0.00s)
    --- PASS: TestMin/First_value_smaller (0.00s)
    --- PASS: TestMin/Second_value_smaller (0.00s)
    --- PASS: TestMin/Equal_values (0.00s)
    --- PASS: TestMin/Negative_values (0.00s)
    --- PASS: TestMin/Zero_and_positive (0.00s)
    --- PASS: TestMin/Negative_and_zero (0.00s)
=== RUN   TestHashContent
=== RUN   TestHashContent/Empty_string
=== RUN   TestHashContent/Simple_string
=== RUN   TestHashContent/Special_characters
=== RUN   TestHashContent/Long_text
--- PASS: TestHashContent (0.00s)
    --- PASS: TestHashContent/Empty_string (0.00s)
    --- PASS: TestHashContent/Simple_string (0.00s)
    --- PASS: TestHashContent/Special_characters (0.00s)
    --- PASS: TestHashContent/Long_text (0.00s)
=== CONT  TestGetHTTPLLMTimeout
2025/05/30 14:48:38 [GetHTTPLLMTimeout] Warning: Could not retrieve specific timeout from HTTPLLMService, returning default: 30s
--- PASS: TestGetHTTPLLMTimeout (0.00s)
PASS
ok  	github.com/alexandru-savinov/BalancedNewsGo/internal/llm	2.791s
